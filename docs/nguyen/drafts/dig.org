#+TITLE: DIG: Dynamic Invariant Generation
#+date: <2021-02-01 03:39>
#+description: My dynamic invariant work
#+filetags: research blog writing invariant-generation dig dynamic-analysis

#+MACRO: tool   DIG;

* Abstract
Automatically inferring invariant specifications has proven valuable in enabling a wide range of software verification and validation approaches over the past two decades. Recent approaches have shifted from using observation of concrete program states to exploiting symbolic encodings of sets of concrete program states in order to improve the quality of inferred invariants.

In this work, we demonstrate that working directly with symbolic states generated by symbolic execution approaches can improve invariant inference further.
Our technique uses a counterexample-based algorithm that iteratively creates concrete states from symbolic states, infers candidate invariants from both concrete and symbolic states, and then validates or refutes candidate invariants using symbolic states. The refutation process serves both to eliminate spurious invariants and to drive the inference process to produce more precise invariants.  This framework can be employed to infer complex invariants that capture nonlinear polynomial relations among program variables.

The open-source {{{tool}}} tool implements these ideas to automatically generate invariants at arbitrary locations in Java or C programs.
Our results show that across a collection of four benchmarks {{{tool}}} improves on the state-of-the-art by efficiently inferring more informative invariants than prior work.

* Keywords
Program Invariants, Numerical Invariants, Dynamic Analysis, Symbolic Execution, CounterExample Guided Refinement, Program Testing and Verification

* Introduction

The expressive power of programs lies in their ability to concisely represent
repeated sub-computations that arise due to iteration or recursion.
Developing software that correctly orchestrates those sub-computations is challenging as
programmers learn when they study even basic sorting algorithms. 
Classic approaches for defining, and understanding, the correctness of such algorithms rely on 
specification of \emph{program invariants} which define relationships
that must hold between program variables at a given location in the
program~\cite{hoare1969axiomatic,cormen2009introduction,hoare1971proof}.

While invariants play a role in educating programmers about complex algorithms,
they also offer the potential to improve programming practice.
Research has demonstrated how invariant specifications can be leveraged for
fault-detection and verification~\cite{hoare1971proof,perkins2009automatically}, 
detecting security vulnerabilities~\cite{bodik2000abcd},
automating the repair of faults~\cite{cashin2019understanding}, and
synthesizing low-level implementations~\cite{srivastava2013template}.
A number of industrial-strength tools provide support for reasoning about invariant 
specifications~\cite{coverityscan,fbinfer}.

Despite the fact that programmers are exposed to the concept of invariants early in their
education, writing specifications is viewed as a burden and developers don't generally 
add them to their code base~\cite{das2002esp}. 
One approach to addressing this is to define implicit specifications, e.g., that a null pointer
should never be dereferenced or an array should never be indexed out of bounds.  While valuable, this
does not address the potential benefit from \emph{program specific invariants}.

The seminal work by Ernst et al. on Daikon~\cite{ernst2007daikon,ernst2000dynamically} addressed this problem by observing
that invariants can be thought of as latent properties of program behavior that can be inferred
by observing sets of program runs.
Techniques like Daikon can only determine \emph{candidate invariants} -- since there may be executions
that are not observed which falsify the candidate.
Nevertheless, these techniques have proven valuable in overcoming the specification burden and
generating candidate invariants that can be subsequently verified or falsified by other 
techniques~\cite{csallner2008dysy}.
Moreover, the ability to reveal these latent properties serves as a rich source of information
for understanding undocumented code~\cite{ernst2001dynamically}, generating more formal
documentation~\cite{ernst2007daikon},
localizing bugs~\cite{b2016learning}, and even proving program termination and non-termination properties~\cite{le2020dynamite}.

Daikon~\cite{ernst2000dynamically,ernst2001dynamically} works by observing \emph{concrete program states} 
that capture the values of variables at designated \emph{locations of interest} in the
program when a program is run on a given input.  By sampling large numbers of inputs, Daikon can efficiently
determine relationships that may hold among variables across those samples.
Confirming that those relationships constitute a true invariant has been a focus of follow-on work to Daikon.
Several invariant generation approaches (e.g., iDiscovery~\cite{zhang2014feedback}, PIE~\cite{padhi2016data}, ICE~\cite{garg2016learning}, NumInv~\cite{nguyen2017counterexample}, G-CLN~\cite{yao2020learning}) use a hybrid approach that dynamically infers candidate invariants and then attempts to verify that they hold for all inputs.
When verification fails, counterexamples are generated which help to refine the invariant inference process 
to obtain more accurate results -- reporting only true invariants.
This \emph{CounterExample Guided Invariant Generation} (CEGIR) approach iterates the inference and verification processes until achieving stable results.

An important class of invariants capture numerical relations among program variables.
Such \emph{numerical invariants} can take on different mathematical forms.
Daikon can infer conjunctive numerical invariant candidates, but its template matching engine makes it
inefficient to infer disjunctive invariants. 
Disjunctive invariants are required to encode properties of programs, but fortunately rich forms of
disjunction can be captured by more complex numerical relations.
\emph{Nonlinear polynomial} relations, e.g., \(x^2 \le y^2\), arise in many scientific, engineering, and safety- and security-critical software~\cite{cousot2005astree}, and 
can encode disjunctive information, e.g., \(x^2 \le y^2\) implies \(x\le -y \vee x\le y\).
\emph{Max/min-plus} relations encode properties that can be expressed in ``tropical'' algebra~\cite{maclagan2015introduction,allamigeon2008inferring} and are able to encode a complementary form of disjunctive information, e.g., the max inequality $\max(x,y) \ge 2$ is equivalent to \((x \ge y \land x \ge 2) \lor (x < y \land y \ge 2)\).
As we demonstrate, when used together nonlinear and max/min invariants can express complex program properties, e.g., permutation and sortedness (\S\ref{sec:rq3}), that cannot be expressed with purely conjunctive formulae.

In this work, we present {{{tool}}}, a CEGIR technique that targets the inference of rich forms of
numerical invariants using \emph{symbolic program states} as a basis.
Our key insight is that symbolic states generated by a symbolic execution engine are
(1) compact encodings of large (potentially infinite) sets of concrete states,
(2) naturally diverse since they arise along different execution paths,
(3) explicit in encoding relationships between program variables,
(4) amenable to direct manipulation and optimization, and 
(5) reusable across many different reasoning tasks within CEGIR algorithms.

We define algorithms for symbolic CEGIR that can be instantiated
using different symbolic execution engines, and the {{{tool}}} implementation uses symbolic states generated from Symbolic PathFinder~\cite{anand2007jpf} (SPF)---a symbolic executor for Java---and CIVL~\cite{siegel2015civl}---a symbolic executor for C.
{{{tool}}} uses symbolic states for both invariant inference and verification.
For inference, {{{tool}}} uses symbolic states to obtain concrete states to bootstrap a set of candidate 
invariants using DIG~\cite{nguyen2012using,nguyen2014using,nguyen2014dig}---a dynamic analysis framework for inferring expressive numerical invariants.
For verification, {{{tool}}} formulates verification conditions from symbolic states to confirm or refute an invariant, solves those using an SMT solver, and produces counterexamples to refine the inference process.

We evaluated {{{tool}}} over 4 distinct benchmarks, consisting of 108 programs, and compared its performance
to state-of-the-art numerical invariant approaches.
We find that the use of symbolic states allows {{{tool}}} to overcome several limitations of 
existing CEGIR approaches.
iDiscovery, which uses Daikon for inference, does not support nonlinear properties, and both ICE and PIE timeout frequently when nonlinear arithmetic is involved.
NumInv also uses DIG to infer invariants, but it invokes KLEE~\cite{cadar2008klee} as a black box verifier for candidate invariants and which causes it to underperform relative to {{{tool}}} for nonlinear and disjunctive invariant inference.
G-CLN can infer nonlinear invariants for loops, but it requires manual problem-specific configuration to generate and prove invariants, and even then {{{tool}}} infers more relevant invariants.
Our evaluation demonstrates that {{{tool}}} establishes the state-of-the-art for inference of
complex nonlinear invariants.
Across the benchmarks it is able to infer the ground truth specifications for 106 of 108 programs;
the next best tool can infer only 89.%\tvn{Is this too strong to explicitly say this number?  Technically we were not able to get other tools, e.g., G-CLN, on other benchmarks like DISJ or COMPLEXITY}.

Our prior work~\cite{nguyen2017syminfer} made an initial step in exploiting
symbolic states for invariant inference.  This paper
significantly extends those results to include:
(1) a novel efficient algorithm to generate inequalities;
(2) support for \emph{max}- and \emph{min-plus} formulae to represent disjunctive invariants;
(3) proofs of correctness and termination for the presented algorithms;
(4) support for Java and C programs; and
(5) a broader experimental evaluation that demonstrates the cost-effectiveness of the approach and its superiority to existing invariant inference techniques.
The implementation of {{{tool}}} and  all experimental data reported in this paper are available at \url{https://github.com/unsat/dig/}.
These results strongly suggest that symbolic states form a powerful basis for computing program invariants.
They permit an approach that blends the best features of dynamic inference techniques and purely symbolic techniques to enable a new state-of-the-art.


# \documentclass[10pt,journal,compsoc]{IEEEtran}

# \ifCLASSOPTIONcompsoc
# % IEEE Computer Society needs nocompress option
# % requires cite.sty v4.0 or later (November 2003)
# \usepackage[nocompress]{cite}
# \else
# % normal IEEE
# \usepackage{cite}
# \fi

# \newif\ifdebug
# \debugtrue
# %\debugfalse
# \newcommand{\debug}[1]{{\ifdebug #1\fi}}
# \newcommand{\notdebug}[1]{{\ifdebug \else #1 \fi}}
# \newcommand{\debugite}[2]{{\ifdebug #1 \else #2\fi}}

# \usepackage{amsthm}
# \usepackage{amssymb}
# \usepackage{amsmath}
# \usepackage{amsfonts}
# \usepackage{caption}
# \usepackage{booktabs}
# \usepackage{wrapfig}
# \usepackage[vlined,figure,linesnumbered]{algorithm2e}
# \usepackage{tcolorbox}
# \usepackage{array}
# \usepackage{url}
# \usepackage{color}
# \usepackage{tikz}
# \usepackage{pgfplots}
# \usepackage{listings}
# \usepackage[basic,classfont=typewriter,langfont=typewriter,funcfont=typewriter]{complexity}
# \usepackage{wrapfig}
# \usepackage[]{hyperref}
# \usepackage{cleveref}
# \usetikzlibrary{plotmarks}

# \lstset{basicstyle=\ttfamily\small,
#   language=C,
#   morekeywords={assert},
#   keywordstyle=\color{blue},
#   commentstyle=\color{red},
#   numbers=none,
#   mathescape,
#   stepnumber=1,
#   numbersep=8pt,
#   backgroundcolor=\color{white},
#   tabsize=4,
#   showspaces=false,
#   showstringspaces=false,
#   emph={},
#   emphstyle=\color{red}\bfseries
# }
# \newcommand{\lt}[1]{{\lstinline+#1+}}
# \newtheorem{mtheorem}{Theorem}[section]
# \newtheorem{mlemma}[mtheorem]{Lemma}
# \newtheorem{mcorollary}[mtheorem]{Corollary}
# \newtheorem{mdefinition}[mtheorem]{Definition}
# \theoremstyle{definition}
# \newtheorem{mexample1}[mtheorem]{Example}
# \newtheorem{mtheorem1}{Theorem}
# \newtheorem{mthesis}[mtheorem1]{Thesis}
# \newtheorem{definition}{Definition}

# \newcommand{\tool}{SymInfer}
# \newcommand{\naive}{na\"\i ve}
# \renewcommand{\implies}{\Rightarrow}
# \newcommand{\Astree}{Astr\'ee}
# \newcommand{\code}[1]{\texttt{#1}}



# \newcommand{\mycomment}[3][\color{red}]{{#1{[{#2}: {#3}]}}}
# \newcommand{\tvn}[1]{\mycomment[\color{red}]{TVN}{#1}}
# \newcommand{\mbd}[1]{\mycomment[\color{blue}]{MBD}{#1}}
# \newcommand{\matt}[1]{\mycomment[\color{blue}]{MBD}{#1}}
# \newcommand{\khn}[1]{\mycomment[\color{brown}]{KHN}{#1}}

# \def\Section{\S}

# \begin{document}
# \input{code}
# \input{figs}
# \input{algs}

# \newcommand{\ignore}[1]{}

# \title{Using Symbolic States to Infer \\Numerical Invariants}


# \author{ThanhVu~Nguyen,
#         KimHao Nguyen, 
#         and~Matthew~B.~Dwyer% <-this % stops a space
#         \IEEEcompsocitemizethanks{
#           \IEEEcompsocthanksitem ThanhVu Nguyen is with the Department of Computer Science, George Mason University, USA.\protect\\Email: \href{mailto:tvn@gmu.edu}{tvn@gmu.edu}.
#           \IEEEcompsocthanksitem KimHao Nguyen is with the University of Nebraska-Lincoln, USA.\protect\\Email:\href{kdnguyen@cse.unl.edu}{kdnguyen@cse.unl.edu}.
# % note need leading \protect in front of \\ to get a newline within \thanks as
# % \\ is fragile and will error, could use \hfil\break instead.
# %E-mail: see http://www.michaelshell.org/contact.html
#           \IEEEcompsocthanksitem Matthew Dwyer is with the Department of Computer Science, University of Virginia, USA.\protect\\Email:\href{matthewbdwyer@virginia.edu}{matthewbdwyer@virginia.edu}.
#         }% <-this % stops an unwanted space
# }

# \maketitle
# \IEEEdisplaynontitleabstractindextext
# \IEEEpeerreviewmaketitle



% \IEEEPARstart{T}{he} automated discovery of \emph{program invariants}---relations among variables that are guaranteed to hold at certain locations of a program---is an important research area in program analysis, verification, and synthesis.
% Generated invariants can help understand undocumented programs, prove correctness assertions, reason about program termination, check resource usage, establish security properties, provide formal documentation, repair programs, and more~\cite{le2020dynamite,slam,henzinger2002lazy,das2002esp,demoura:tacas08:z3,leroy:popl06:,ernst2000dynamically,nguyen2020using}.

% A particularly useful class of invariants is \emph{numerical invariants}, which involve relations among numerical program variables.
% Within numerical invariants, \emph{nonlinear polynomial} relations, e.g., \(x \le y^2\), \(x = qy + r\), arise in many scientific, engineering, and safety- and security-critical software, e.g., to prove the absence of errors in Airbus avionic systems~\cite{cousot2005astree}.
% Nonlinear relations can also encode certain form of disjunctive information, e.g., \(x\le y^2\) is \(x\le -y \vee x\le y\).
% \emph{Min/max-plus} relations are another class of numerical informations that can represent a different form of disjunction, e.g., the max inequality $\max(x,y) \ge 2$ represents \((x > y \land x \ge 2) \lor (x < y \land y \ge 2)\).
% When used together nonlinear and min/max invariants can be complimentary and help reason about complex but useful program properties, e.g., permutation and sortedness (\S\ref{sec:rq3}).

% %Another complemtary class of numerical invariants that can represent disjuctions is \emph{min/max-plus} relations .


% %Disjunctive invariants, which represent the semantics of branching, are more difficult to analyze but crucial to many programs. For example, after if (p) {a=1;} else {a=2;} neither a = 1 nor a = 2 is an invariant, but (p ∧ a = 1) ∨ (¬p∧a = 2) is a disjunctive invariant. Disjunctive invariants thus capture path-sensitive reasoning, such as those found in most sorting and searching tasks, as well as functions like strncpy in the C standard library.

% Daikon~\cite{ernst2000dynamically,ernst2001dynamically} demonstrated that dynamic analysis is a practical approach to infer numerical and other invariants from \emph{concrete program states} that program execution traces observed when running the program on sample inputs.
% Dynamic inference is typically efficient and supports expressive invariants, but can often produce spurious invariants that do not hold for all possible inputs.
% Several numerical invariant generation approaches (e.g., iDiscovery~\cite{zhang2014feedback}, PIE~\cite{padhi2016data}, ICE~\cite{garg2016learning}, NumInv~\cite{nguyen2017counterexample}, G-CLN~\cite{yao2020learning}) use a hybrid approach that dynamically infers candidate invariants and then statically checks that they hold for all inputs.
% For a spurious invariant, the checker produces counterexamples, which help the inference process avoid this invariant and obtain more accurate results.
% This CEGIR \emph{(CounterExample Guided Invariant Generation)} approach iterates the inference and checking processes until achieving stable results.

% In this work, we present {{{tool}}}, a CEGIR technique and tool that uses \emph{symbolic program states}.
% Our key insight is that symbolic states generated by a symbolic execution engine are
% (1) compact encodings of large (potentially infinite) sets of concrete states,
% (2) naturally diverse since they arise along different execution paths,
% (3) explicit in encoding relationships between program variables,
% (4) amenable to direct manipulation and optimization, and %such as combining sets of states into a single joint encoding, and
% (5) reusable across many different reasoning tasks within CEGIR algorithms.

% We define algorithms for symbolic CEGIR that can be instantiated
% using different symbolic execution engines, and the {{{tool}}} implementation uses symbolic states generated from Symbolic PathFinder~\cite{anand:tacas07:spf} (SPF)---a symbolic executor for Java---and CIVL~\cite{siegel2015civl}---a symbolic executor for C.
% {{{tool}}} uses symbolic states for both invariant inference and
% validation.
% For inference, {{{tool}}} uses symbolic states to obtain concrete states to bootstrap a set of candidate invariants using
% DIG~\cite{nguyen:icse12:dig,nguyen2014using,nguyen:tosem14:dig}---a dynamic analysis framework for inferring expressive numerical invariants.
% For validation, {{{tool}}} formulates verification conditions from symbolic states to confirm or refute an invariant, solves those using an SMT solver, and produces counterexamples to refine the inference process.

% %TODO: min/max invariants
% Symbolic states allow {{{tool}}} to overcome several limitations of existing CEGIR approaches.
% iDiscovery, ICE, and PIE are limited to computing relatively simple numerical invariants and  often do not consider programs with complex nonlinear arithmetic and properties. % such as $x=qy+r, x^2+y^2=z^2$.
% As our evaluation of {{{tool}}} demonstrates in Section~\ref{sec:eval},
% iDiscovery, which uses Daikon for inference, does not support nonlinear properties, and both ICE and PIE timeout frequently when nonlinear arithmetic is involved.
% The tool NumInv~\cite{nguyen2017counterexample} also uses DIG to infer invariants, but it invokes KLEE~\cite{cadar2008klee} as a blackbox
% verifier for candidate invariants.  Since KLEE is unaware of the goals
% of its verification it will attempt to explore the entire program state space
% and must recompute that state space for each candidate invariant.
% In contrast, {{{tool}}} incrementally constructs a fragment of the state space
% that generates a set of symbolic states that is sufficiently diverse
% for invariant verification and it reuses symbolic states for all invariants.
% The recent work G-CLN~\cite{yao2020learning} learns nonlinear invariants for loops using neural networks, but it requires the user to provide strong postconditions and manual tweaks to generate and prove invariants, whereas {{{tool}}} only requires the implementation.
% % and generate counterexamples

% We evaluated {{{tool}}} over 4 distinct benchmarks which consist of 108 programs.
% The study shows that {{{tool}}}:
% (1) can generate the complex nonlinear invariants expected in 28/28 of the NLA benchmarks;
% (2) can generate nonlinear relations and min/max invariants to represent interesting disjunctive properties (e.g., permutation);
% (3) is effective in finding nontrivial complexity bounds for 18/19 programs, with 4 of those improving on the best known bounds from the literature; 
% (4) can  generate invariants whose combination establish asserted properties involving relations not supported directly by {{{tool}}} for 45/46 programs; and
% (5) effectively uses symbolic execution at various depths to incrementally improve results; and
% (6) outperforms existing numerical invariant generation works, especially in generating rich invariants capturing nonlinear and disjunctive information.

% These results strongly suggest that symbolic states form a powerful basis for computing program invariants.
% They permit an approach that blends the best features of dynamic inference techniques and purely symbolic techniques. 
% The key contribution of our work lies in (i) the identification of the value
% of symbolic states in CEGIR, (ii) developing an algorithmic framework for
% adaptively computing a sufficient set of symbolic states for invariant
% inference, (iii) developing and releasing {{{tool}}} as an open-source implementation of this framework,
% and (iv) demonstrating, through our evaluation of
% {{{tool}}}, that it improves on the best known techniques.

% Our prior work~\cite{nguyen2017syminfer} made an initial step in exploiting
% symbolic states for invariant inference.
% This paper significantly extends those results to include:
% (1) a novel efficient algorithm to generate inequalities;
% (2) support for \emph{max}- and \emph{min-plus} formulae to represent disjunctive invariants;
% (3) proofs of correctness and termination for the presented algorithms;
% (4) support for Java, Java bytecode,  and C programs; and
% (5) a broader experimental evaluation that demonstrates the cost-effectiveness of the approach.
% The implementation of {{{tool}}} and  all experimental data reported in this paper are available at \url{https://github.com/unsat/dig/}.
% % \matt{I see that we dropped pre/post discussion so I dropped it here.}


# \section{Overview}\label{sec:overview}
# To illustrate {{{tool}}}, we show how inferred numerical invariants can assist in understanding programs and analyzing program complexity.
# We then describe techniques using symbolic states to help remove spurious invariants to achieve expressive and accurate invariants.

# \subsection{Applications of Numerical Invariants}\label{sec:applications}

# \begin{figure}[t]
#   \begin{minipage}{0.48\linewidth}
#     \usebox{\cohendiv}
#   \end{minipage}
#   \hfill
#   \begin{minipage}{0.48\linewidth}
#     \centering
#     \small
#     ~~\\
#     \begin{tabular}{c  c c  c  c c}
#       \multicolumn{6}{r}{\textbf{Concrete States}}\\
#       \\
#       $x$&$y$&$a$&$b$&$q$&$r$\\
#       \midrule
#       15& 2& 1& 2& 0& 15\\
#       15& 2& 2& 4& 0& 15\\
#       15& 2& 1& 2& 4& 7\\
#       \multicolumn{3}{c}{}&\multicolumn{1}{c}{$\vdots$}&\multicolumn{1}{c}{}\\
#       \midrule
#       4& 1& 1& 1& 0& 4\\
#       4& 1& 2& 2& 0& 4\\
#       \multicolumn{3}{c}{}&\multicolumn{1}{c}{$\vdots$}&\multicolumn{1}{c}{}\\
#     \end{tabular}
#   \end{minipage}
#   \caption{The \texttt{cohendiv} integer division program and concrete states
#     observed at location L1 on inputs $(x=15, y=2)$ and $(x=4,y=1)$.
#     Among the invariants discovered by {{{tool}}} at L1, the key nonlinear equality $x = qy+r$ describes the precise semantics of the program.}\label{fig:cohen}
# \end{figure}


# \textbf{Program Understanding.} {{{tool}}} can help understand program behavior and discover unknown program properties.
# Consider the \texttt{cohendiv} integer division algorithm in Figure~\ref{fig:cohen}; L1 and L2 mark the locations of interest.
# Given this program and the considered locations, {{{tool}}} automatically discovers at L1 the (loop) invariants:
# \begin{gather*}
#   x = qy+r\quad ay = b\\
#   b \le x\quad y \le r\quad 0 \le q\quad 1 \le b\quad 1 \le y
# \end{gather*}
# and at L2 the (postcondition) invariants:
# \begin{gather*}
#   x = qy+r\quad ay = b\\
#   r \le y - 1\quad 0 \le r \quad r \le x
# \end{gather*}

# These relations are sufficiently strong to understand the semantics and verify the correctness of \texttt{cohendiv}.
# The key invariant is the nonlinear equality $x=qy+r$, which
# captures the precise behavior of integer division: the dividend $x$ equals the divisor $y$ times the quotient $q$ plus the remainder $r$.
# The other inequalities also provide useful information.
# For example, the invariants at the program exit reveal several required properties of the remainder $r$, e.g., non-negative ($0 \le r$), at most the dividend ($r \le x$), but strictly less than the divisor ($r \le y - 1$).

# Also, these invariants might help check assertions in the program.
# For example, we can assert and verify the postcondition stating that the returned quotient is non-negative  because the discovered invariants at L2 implies $0 \le q$.\footnote{{{{tool}}} also found $0 \le q$
#   and other invariants at L2, but discarded them because they are implied by other discovered properties and thus are redundant.}%\tvn{proving is such a strong word because we do not provide guarantee on the inferred invariants (only up to certain symbolic depth)}

# \begin{figure}
#   \usebox{\tripple}
#   \caption{A program with three complexity bounds.}\label{fig:complexity}
# \end{figure}

# \textbf{Complexity Analysis.} Another rather surprising use of {{{tool}}}'s nonlinear numerical invariants is to characterize the computational complexity of a program, which is useful for identifying possible security problems~\cite{hoffman17verifying,antonopoulos2017decomposition,nguyen2020using}.
# Figure~\ref{fig:complexity} shows the program \texttt{tripple}, adapted from Figure 2 of~\cite{gulwani2009control}, with nontrivial runtime complexity;
# the program has been modified to include a variable $t$ which serves
# to encode the number of loop iterations that are computed.
# At first, \texttt{tripple} appears to take $O(NMP)$ due to the three nested loops.
# A closer analysis~\cite{gulwani2009control} shows a more precise bound $O(N+NM+P)$ because the innermost loop, which is updated each time the middle loop executes, changes the behavior of the outermost loop.

# When given this program, {{{tool}}} discovers an interesting and complex postcondition at location L about the variable $t$:
# \begin{gather*}
#   P^2Mt + PM^2t - PMNt - M^2Nt - PMt^2 + \\
#   MNt^2 + PMt -PNt - 2MNt + Pt^2 + Mt^2 + \\
#    Nt^2 - t^3 - Nt + t^2 = 0
# \end{gather*}
# This nonlinear equality is valid, but looks incomprehensible and quite different than the expected bound $N+NM+P$ or even $NMP$.
# However, when solving this equation (finding the roots of $t$), we obtain three solutions showing that this program has different time complexities:

# % \tvn{TODO} {{{tool}}} next examines this nonlinear equation to provide more meaningful results.  First, {{{tool}}} solves the equation for $\mathtt{t}$, i.e., finding the roots, and obtains three solutions $\mathtt{t=0, t = P+M+1}$ and $\mathtt{t=N-M(N-P)}$.
# % These results suggest that this program has \emph{three} possible complexity bounds.
# % Then, {{{tool}}} computes likely conditions over input variables that lead to these complexity bounds.
# % %% To obtain even more precise and useful results, {{{tool}}} next computes likely \emph{conditions} of each complexity bound.
# % %% At high level, {{{tool}}} extracts traces that represent program paths leading to different complexity bounds, and then finds predicates over the program inputs for these path to represent conditions for these bounds.
# % At the end, {{{tool}}} obtains three distinct bounds of this program:

# \[
#   t =
#   \begin{cases} 
#     0           & \text{when } \quad N = 0 \\
#     P + M + 1   & \text{when } \quad N \le P\\
#     N - M(P - N)& \text{when } \quad N > P
#   \end{cases}
# \]

# Manual analysis shows these results represent the \emph{exact} bound of \texttt{tripple} and are more precise than the bound $O(N+MN+P)$ given in~\cite{gulwani2009control}.
# Note that $O(N+MN+P)$ is still a correct \emph{upper bound} of \texttt{tripple}, e.g., when $N>P$ then $O(N+NM+P) = O(N+NM)$, which is equivalent to $O(N-M(P-N)) = O(N+MN)$.

# %Complexity analysis can help detect side-channel information leakage, e.g., if some of the inputs from $M, N$ and $P$ are high-security and some are public, an attacker can infer valuable information about the high-security inputs by observing the running time of the program~\cite{nguyen2020using}.
# %By knowing the execution times of different high-security dependent branches (e.g., one branch takes linear time while the other takes quadratic time), the developer can mitigate an attack by ``padding'' the computation so that all executions take the same time (e.g., instrumenting the program to add dummy loops, instructions, or delays).

# % These results, which consist of both the complexity bounds and the conditions causing them, are more precise than the bound $\mathtt{N+MN+P}$ given
# % in~\cite{gulwani:pldi09:invsboundanalysis}.
# % The discovered bound conditions can be useful for both understanding and debugging tasks, e.g., developers can generate inputs causing the program to run in different time complexities.


# \subsection{{{tool}}}
# To infer invariants, {{{tool}}} integrates dynamic and symbolic analyses using the \emph{counterexample-guided invariant generation} (CEGIR) approach.
# {{{tool}}}'s CEGIR consists of two phases:
# a \emph{dynamic analysis} that infers candidate (equality and inequality) invariants from program execution traces or concrete states, and a
# \emph{symbolic checker} that checks candidates against the program using symbolic states obtained from a symbolic execution tool.
# If a candidate invariant is spurious, the checker also provides counterexamples.
# Concrete states from these counterexamples are obtained and recycled to repeat the process, and produce more accurate results.

# These steps of inferring and checking repeat until no new counterexamples or (true) invariants are found.
# The CEGIR technique exploits the observation that checking a (cheaply generated) candidate solution is often easier than directly inferring a sound solution~\cite{nguyen2014using}.

# %Inferring a sound solution directly is often harder than checking a (cheaply generated) candidate solution.


# \subsubsection{Concrete States}
# Existing dynamic invariant analyses such as Daikon or DIG instrument the program at considered locations to record values
# of the local variables, and then, given a set of inputs, execute the program to record a set of \textit{concrete states} of the program to generate candidate invariants.
# %% Given a location L, we refer to the concrete states of L as \textit{concrete L-states} and we distinguish those that are \textit{observed} by instrumentation on a program run.
# %% It is these observed concrete states that form the basis for all dynamic invariant inference techniques.
# Figure~\ref{fig:cohen} shows several concrete states obtained at location L1 when running \texttt{cohendiv} using inputs $(x=15, y=2)$ and $(x=4,y=1)$.

# However, inferring invariants on just concrete states often produces undesirable results.
# On several hand-selected sets of inputs that seek to expose diverse concrete states, running Daikon on \texttt{cohendiv} results in very simple invariants, e.g., $4 \le x$ and  $0 \le q$ at location L1.
# These are clearly much weaker than the key nonlinear invariant for this example.
# Moreover, the invariant on $x$ is actually spurious since clearly values smaller than 4 can be passed as the first input which will reach L1.
# The more powerful DIG invariant generator permits the identification of the key equality invariant, but it too will yield the spurious $4 \le x$ invariant.
# Spurious invariants are a consequence of the diversity and representativeness of the inputs used, and the observed concrete states.
# Leveraging symbolic states can help address this weakness.

# \subsubsection{Symbolic States}
# {{{tool}}} symbolically executes the program to compute the \emph{symbolic states} at a considered program location L.
# A symbolic state compactly encodes a large (potentially infinite) set of concrete states.
# Symbolic states consist of path conditions describing execution paths to L and mappings from program variables at L to symbolic values. 
# For example, Figure~\ref{fig:symbolicstates} shows the symbolic states at location L1 of \texttt{cohendiv}.

# \begin{figure}
#   \[ \small
#     \begin{array}{cc} \textbf{\text{Path Conditions}}~(\Pi_{\text{L1}}) & \textbf{\text{Variable Mappings}}~(\sigma_{\text{L1}})\\
#       0 < y \land y \le x         & q\mapsto 0; r\mapsto x; a\mapsto 1; b\mapsto y\\
#       0 < y \land 2y \le x        & q\mapsto 0 ; r \mapsto  x; a \mapsto  2 ; b \mapsto  2y\\
#       0 <y \land 2y +y \le x < 4y & q \mapsto  2 ; r \mapsto  x - 2y ; a \mapsto  1 ; b\mapsto y\\
#       \vdots & \vdots
#     \end{array}
#   \]
#   \caption{Symbolic states at location L1 in the program \texttt{cohendiv} in Figure~\ref{fig:cohen}.}\label{fig:symbolicstates}
# \end{figure}


# To check a candidate invariant $p$, {{{tool}}} asks a constraint solver to determine the validity of $p$ with respect to the symbolic states.
# If the solver finds a counterexample disproving $p$, {{{tool}}} extracts concrete states from the counterexample and saves them for subsequent inference.
# Otherwise, {{{tool}}} accepts and saves $p$ as an invariant.

# {{{tool}}} can return \emph{spurious} invariants because the solver might timeout or return unknown, or because symbolic execution might not be able to explore all program paths to compute precise symbolic states.
# Consequently, {{{tool}}} is designed to explore program states incrementally and adaptively to minimize 
# cost while finding accurate invariants (\S\ref{sec:using_symbolic_states}). 

# \subsubsection{Inferring Numerical Invariants}

# {{{tool}}} uses a CEGIR algorithm to find \emph{polynomial equalities} at program locations of interest.
# For each considered program location, {{{tool}}} creates an equation \emph{template} $c_1t_1 + c_2t_2 + \cdots + c_{n}t_{n} = 0$.
# This template contains $n$ unknown coefficients $c_i$ and $n$ \emph{terms} $t_i$, with one term for each possible multiplicative combination of relevant program variables, up to some degree $d$.

# {{{tool}}} uses symbolic states to obtain many possible concrete states and substitute their concrete values into the template to form an instantiated linear equation.
# After obtaining at least $n$ concrete states, {{{tool}}} solves the resulting set of equations for the $n$ unknown coefficients $c_i$.
# {{{tool}}} then extracts candidate invariants by substituting the solutions back into the template.
# Now, {{{tool}}} enters a CEGIR loop that checks the candidate invariants by using symbolic states.
# We discard any spurious invariants and use the corresponding counterexample concrete states to infer new candidates until no additional true invariants are found.

# {{{tool}}} also generates \emph{linear inequalities} in the forms of (i) \emph{octagons}, which are inequalities over two terms, and (ii) \emph{max/min-plus constraints}, which are a form of disjunctive invariants.
# The early version of {{{tool}}}~\cite{nguyen2017syminfer} does not support max/min relations and can only infer octagonal inequalities using a CEGIR divide-and-conquer algorithm, e.g., repeatedly invokes symbolic states to guess and tighten lower and upper bounds of candidate inequalities.

# The current version of {{{tool}}} takes advantage of advances in constraint solvers and uses linear optimization to obtain directly from symbolic state the bounds for both octagonal and max/min inequalities.
# This optimization approach is much more efficient and allows {{{tool}}} to discover more challenging and expressive invariants than the previous CEGIR approach (e.g., the user can configure {{{tool}}} to generate max/min invariants as well as nonlinear inequalities over an arbitrary number of variables instead of the default octagonal linear inequalities).

# % {{{tool}}} applies linear optimization directly on symbolic states to obtain the bounds on the candidate invariants.
# % From an initial set of conrete states, {{{tool}}} enumerates all possible octagonal inequality forms involving one and two
# % variables and uses symbolic states to check inequalities under these forms are
# % within certain ranges $[\mathtt{minV}, \mathtt{maxV}]$.
# % %% \mbd{Are the dimensions of the octagons configurable?  You mention one and two variables, but could you have a 3 dimensional octagon?  If so we don't want to give the impression that there is a restriction.}
# % %% \tvn{It's fixed to 2 variables because that's the standard definition for octagonal invs. It can be generalize k-dimension but not sure if it is worth doing?}
# % It then narrows this range,
# % iteratively seeking tighter lower and upper bounds.

# Finally, in a post-processing phase, from the obtained invariants, {{{tool}}} uses an SMT solver to check and remove any redundant invariants that are logical implications of other invariants.
# For instance, we suppress $x^2 = y^2$ if $x = y$ is also found because the latter implies the former.
# %{{{tool}}} checks possible implications using an SMT solver (checking whether the negation of the implication is unsatisfiable).


# \section{Symbolic States}\label{sec:symstates}

# The behavior of a program at a location L can be precisely represented by the set of all possible values of the variables in scope at L.
# We refer to such values as \emph{concrete L-states} of the program and define them as:

# \begin{mdefinition}[Concrete State]
#   A concrete L-state is a mapping $\sigma_\text{L}$  from program variables in scope at L to concrete values.
# \end{mdefinition}

# \noindent Figure~\ref{fig:cohen} shows several concrete L-states in the \texttt{cohendiv} program.
# Dynamic analyses such as Daikon and DIG analyze concrete states to infer invariants.
# These techniques instrument the program at location L to take ``snapshots'' of the state of the program at L and then execute the program on a set of inputs to record a set of concrete states, which are values of some/all variables at L.

# In contrast, a \emph{symbolic L-state} is formulated in terms of a set of \textit{input variables} that capture the values of program inputs in order to 
# represent a (potentially infinite) set of concrete states:
# \begin{mdefinition}[Symbolic State]
#   A symbolic L-state is a tuple \(\langle\sigma_{\text{L}}, \Pi_{\text{L}}\rangle\), where $\sigma_L$ is a map from program variables in scope at L to symbolic expressions over input variables, and  $\Pi_\text{L}$ is the path condition, which is a logical formula over the input variables that the inputs must satisfy to reach $L$.   
# \end{mdefinition}

# %\matt{added the feasibility requirement here}
# \noindent The concrete states defined by a symbolic state are \textit{feasible} -- realizable
# at L on some program execution.
# Figure~\ref{fig:symbolicstates} shows several symbolic states at location L1 of the program \texttt{cohendiv}.
# {{{tool}}} uses the technique described in \S\ref{sec:getss} to collect symbolic states.

# Finally, an \emph{invariant} is a logical formula that always holds at a program location.
# For efficiency, invariant generation tools typically infer invariants under a certain template or form.
# \begin{mdefinition}[Template-based Invariant]\label{defn:inv}
#   An invariant under a template $\beta_{\tau}$ is a tuple $\langle L, \beta_{\tau} \rangle$  where $L$ is a program point and $\beta$ is a formula, which has the form $\tau$ and ranges over program variables in scope at $L$, that holds over all concrete or symbolic states of $L$.
# \end{mdefinition}

# \noindent Section~\ref{sec:overview} shows invariants under different forms obtained by {{{tool}}} for the example program, e.g., nonlinear equalities and octagonal linear inequalities.

# Concrete states, symbolic states, and invariants are different representations of properties at a program location.
# Concrete states describe program properties precisely, but there may be (infinitely) many to consider, and analyzing a smaller, finite subset of concrete states may lead to \emph{spurious} invariants that dramatically underapproximate the set of program states.
# Program invariants overapproximate program states at L, but they generally
# have a compact form that can be leveraged to support understanding and
# reasoning about properties at L.
# Symbolic states serve as an intermediate representation between concrete states and invariants that might be inferred from the concrete states.

# \subsection{Obtaining Symbolic States}\label{sec:getss}

# \begin{algorithm}[t]
#   \small
#   \DontPrintSemicolon
  
#   \Input{program $P$, location $L$, maxdepth $k$}
#   \Output{symbolic states \sstates}
#   \BlankLine

#   $\Pi, \sigma \leftarrow \symexe(P,L,k)$ //invoke symbolic execution\;\label{line:symexe}
#   \BlankLine
  
#   $\sstates\leftarrow \Pi$\;\label{line:pathcond}
#   \ForEach{variable $v \in \sigma$}{\label{line:ssloopStart}
#     $\sstates = \sstates \wedge (v \equiv \sigma(v))$\;
#   }\label{line:ssloopEnd}
#   \KwRet \sstates\;
#   \caption{\texttt{getSymbolicstates}: calling a symbolic execution tool to obtain symbolic states}
#   \label{fig:getsymbolicstates}
# \end{algorithm}


# To obtain symbolic states, we modify the search process of a symbolic execution tool. 
# We require that these tools produce underapproximating symbolic encodings of program states, which
# ensures that generated symbolic states only define feasible concrete states.
# First, we introduce a new method \texttt{vtrace} and insert a \texttt{vtrace} call at each location of interest in the program.
# Next, we intercept the search process of the symbolic execution tool whenever it enters a \texttt{vtrace} call.
# Most symbolic execution tools already maintain information such as location, path conditions $\Pi$, and variable mappings $\sigma$, and thus we just need to access and record this information.
# Thus, we obtain a symbolic L-state $\langle \sigma_L , \Pi_L \rangle$ whenever the program enters a \texttt{vtrace} call at some program location L and obtain a set of symbolic L-states if the program hits L multiple times (e.g., in a loop).
# There are potentially an infinite number of symbolic states at L, thus we adapt the symbolic execution tool to just return the symbolic L-states encountered during a search of a given depth $k$.

# % but most existing symbolic execution tools have the ability to perform a depth-limited search.
# % We adapt SPF and CIVL to just return the symbolic L-states encountered during search of a given depth $k$.
# Figure~\ref{fig:getsymbolicstates} summarizes the process of invoking the symbolic execution tool to obtain symbolic states (line~\ref{line:symexe}).
# {{{tool}}} uses logical formulae to represent symbolic states and reuse these formulae for invariant checking and inference.
# For each symbolic L-state, we create the formula (lines~\ref{line:pathcond}\,--\,~\ref{line:ssloopEnd})
# \[
# \Pi_\texttt{L} \land  \bigwedge_{v \in \sigma} (v = \sigma_\texttt{L}(v)).
# \]
# For example, the formulae representing the symbolic state in the first row of Figure~\ref{fig:symbolicstates} is 
# \[
# (0< y \land y \le x) \land (q= 0\land r= x\land a= 1\land b= y)
# \]
# This formula generalizes the first, $x=15, y=2, a=1, b=2, q=0, r=15$, and fourth, $x=4, y=1, a=1, b=1, q=0, r=4$, concrete states
# listed in Figure~\ref{fig:cohen}.
# Since each symbolic state represents a path leading to L, we can obtain a formula capturing multiple paths leading to L by taking the disjunction of formulae representing individual symbolic L-states.

# The user of {{{tool}}} can insert \texttt{vtrace} calls to multiple locations of interests and {{{tool}}} will extract the appropriate set of symbolic states for those locations.
# Moreover, the user can specify subsets of variables in scope at L to \texttt{vtrace}, e.g., \texttt{vtrace(x,y,z)}, to obtain symbolic traces relevant to only those variables.
  
# \subsection{Using Symbolic States}\label{sec:using_symbolic_states}

# Symbolic states can help invariant generation in many ways.
# We describe techniques using symbolic states to check and compute candidate invariants and to generate diverse concrete states.

# As mentioned in~\S\ref{sec:getss}, the number of symbolic states varies with the given symbolic execution depth.
# A low depth means few states. Few states will tend to encode a small set of concrete L-states, which limits verification and refutation power.
# Few states will also tend to solve verification condition faster.
# To address this cost-effectiveness tradeoff, rather than try to choose an optimal depth, our algorithm
# computes the lowest depth that yields symbolic states that change verification outcomes.
# In essence, the algorithm adaptively computes a good cost-effectiveness tradeoff for a given
# program, location of interest, and invariant.

# \subsubsection{Symbolic States as a ``Verifier''}\label{sec:verify}

# \begin{algorithm}[t]
#   \small
#   \DontPrintSemicolon
  
#   \Input{program $P$, location $L$, property $p$, clauses to \block}
#   \Output{proved status \isProved, counterexample \cex}
#   \BlankLine
  
#   $\isProved \leftarrow \unknown$ // is $p$ proved?\;
#   $\result, \result' \leftarrow \unknown, \unknown$\;
#   $\cex \leftarrow \emptyset$\;
#   $\nochanges, \nochanges_{max} \leftarrow 0, \texttt{NOCHANGES\_MAX}$\;
#   $k, k_{max}  \leftarrow d_{\mathit{def}}, d_{max}$ // default and max depth
  
#   \While{$k < k_{max}$}{\label{line:extract}
#     $\sstates \leftarrow \getSymbolicStates(P, L, k)$\; \label{line:getSymbolicStates}
#     % $\vc \leftarrow(\bigvee\limits_{s \in \sstates}(s.\pi \land \bigwedge\limits_{i} var(i)=s.\vec{e}[i])$\; \label{line:first}
#     $\vc \gets (\bigvee \sstates) \land (\lnot \block)$\; \label{line:second} %
#     $\result' \leftarrow \SAT(\lnot(\vc \implies p))$\; \label{line:check}
#     \If{$\result' \equiv \result$}{\label{line:check2}
#       \If {$\nochanges \equiv \nochanges_{max}$}{
#         \mybreak\label{line:break}
#       }
#       $\nochanges \leftarrow \nochanges + 1$\;
#     }
    
#     \If{$\result' \equiv \sat$}{\label{line:sat1}
#       $\isProved \leftarrow \False$\;
#       $\cex \leftarrow \getModel()$\;
#       \mybreak\label{line:sat2}
#     }\ElseIf{$\result' \equiv \unsat$}{
#       $\isProved \leftarrow \True$
#     }\ElseIf{$\result' \equiv \unknown$}{
#       $\isProved \leftarrow \unknown$
#     }
#     $\result \gets \result'$\;
#     $k \leftarrow k + 1$\label{line:endloop}
#   }
#   \BlankLine
#   \KwRet \isProved, \cex\;
#   \caption{\texttt{check}: check a candidate property using symbolic states.}
#   \label{fig:check}
# \end{algorithm}

# Figure~\ref{fig:check} shows how we use symbolic states to \texttt{check}, or refute, a property.
# The algorithm is incremental and obtains symbolic states at a greater depth to increase the accuracy of verification.

# The algorithm iterates with each iteration considering a different depth, $k$.
# The body of each iteration (lines~\ref{line:extract}\, --\, \ref{line:endloop}) works as follows.
# For each iteration we use the function \texttt{getSymbolicStates}, which implements the technique described in~\S\ref{sec:getss}, to generate the set of symbolic L-states reachable at depth less than or equal to $k$ (line~\ref{line:getSymbolicStates}).
# Note that these states can be cached and reused for a given $P$ and $L$.
# %Also, obtaining symbolic states can be done incrementally to avoid re-exploring the program's state space using techniques like~\cite{Yang:2012:MSE:2338965.2336771}.

# We next create a verification condition (\texttt{vc}) by conjoining the symbolic states (i.e., the disjunction of individual symbolic states such as those in Figure~\ref{fig:symbolicstates}) and the states to be blocked.
# We use these \texttt{block} states to avoid generating the same concrete states or counterexample inputs (when \texttt{check} returns a counterexample, we (e.g., the algorithm in Figure~\ref{fig:findeqts}) use it to obtain concrete states, and then block the counterexample so that we do not generate it again).
# If the resulting formula implies a candidate property $p$ then that candidate is consistent with the set of symbolic states.
# We use an SMT solver to check the negation of this implication.

# The solver can return \texttt{sat} indicating that the property is not an invariant (lines~\ref{line:sat1}\, --\, \ref{line:sat2}).
# In this case, we query the solver for a model which represents a concrete state that is inconsistent with the proposed invariant.  This counterexample state is saved so that the inference algorithm can search for
# invariants that are consistent with it.
# The solver can also return \texttt{unsat} indicating the property is a true invariant;
# at least as far as the algorithm can determine given the symbolic states at the
# current depth.
# Finally, the solver can also return \texttt{unknown}, indicating it cannot determine whether the given property is true or false.

# For the latter two cases, we increment the depth and explore a larger set
# of symbolic states generated from a deeper symbolic execution.
# Lines~\ref{line:check}\, --\, \ref{line:break} work to determine when increasing the depth
# does not influence the verification.
# In essence, they check if the same result is computed at several consecutive adjacent depths and if so, they return (line~\ref{line:break}).

# \textbf{Correctness and Termination: }
# This \texttt{check} function guarantees that refuted candidates are \emph{not} invariant 
# because running the program with the generated counterexample inputs would violate these candidates.
# However, results only hold over the symbolic states obtained up to the considered depth and might not hold in general.
# The function terminates because the loop executes for at most $d_{max}-d_{\mathit{def}}$ iterations.

# \subsubsection{Symbolic States as an ``Optimizer''}\label{sec:optimize}

# \begin{algorithm}[t]
#   \small
#   \DontPrintSemicolon
  
#   \Input{program $P$, location $L$, term $t$}
#   \Output{upper bound value of $t$ within a predefined \texttt{v\_max} range}
#   \BlankLine
  
#   $v, \result, \result' \leftarrow \infty, \infty, \infty$\;
#   $\nochanges, \nochanges_{max} \leftarrow 0, \texttt{NOCHANGES\_MAX}$\;
#   $k, k_{max}  \leftarrow d_{\mathit{def}}, d_{max}$ // default and max depth
  
#   \While{$k < k_{max}$}{$\sstates \leftarrow \getSymbolicStates(P, L, k)$\; \label{line:extract2}
#     % $\vc \leftarrow(\bigvee\limits_{s \in \sstates}(s.\pi \land \bigland\limits_{i} var(i)=s.\vec{e}[i])$\; \label{line:first}
#     $\result' \leftarrow \MAX(\bigvee \sstates, t)$\; \label{line:max}
#     \If{$\result' \equiv \result$}{\label{line:break2}
#       \If {$\nochanges \equiv \nochanges_{max}$}{
#         \mybreak\label{line:max2}
#       }
#       $\nochanges \leftarrow \nochanges + 1$\;
#     }
    
#     \If{$\result' \equiv \unknown$}{ 
#       $v \leftarrow \infty$
#     }    
#     \ElseIf{$\result' \le v_{max}$ } { 
#       $v \leftarrow \result'$
#     }
#     \ElseIf{$\result' > v_{max}$}{
#       $v \leftarrow \infty$\;
#       \mybreak
#     }
#     $\result \gets \result'$\;
#     $k \leftarrow k + 1$\label{line:endloop2}
#   }
#   \BlankLine
#   \KwRet $v$
#   \caption{\texttt{optimize}: find the upper bound value of a term from symbolic states.}
#   \label{fig:max}
# \end{algorithm}

# Figure~\ref{fig:max} shows how we use symbolic states to compute an upper bound of a term.
# The algorithm directly computes an inequality of the form $t \le c$, where $t$ is a term (e.g., $x - y$) and $c$ is some integer value.
# The approach leverages the power of modern constraint solvers, which, in addition to finding satisfiability assignments, can find \emph{optimal} assignments with respect to objective constraints using linear optimization techniques~\cite{bjorner2015nuz}.


# This algorithm, similarly to the one in Figure~\ref{fig:check}, incrementally obtains symbolic states at a greater depth to improve accuracy.
# The main difference is that instead of checking satisfiability of a formula, we use the ``optimizer'' component of the solver to find the maximum value of the given term from symbolic states (line~\ref{line:max}).

# The solver returns two possible values: a concrete integer value or \texttt{unknown}.
# If the returned value is less than or equal to a parameterized $v_{max}$ bound, it is saved and the algorithm repeats the process using a higher depth.
# Otherwise, if the solver returns \texttt{unknown} or a value larger than the bound, then we save the result as $\infty$, which produces a trivial invariant \(t \le \infty\) that would be discarded.
# We also break out of the loop in the latter case because a result larger than some bound at some depth $k$ will remain larger than that bound at any depth larger than $k$.

# \textbf{Correctness and Termination: } Similarly to the \texttt{check} function in Figure~\ref{fig:check},  \texttt{optimize} guarantees that discarded results are those with bounds greater than the considered bound (or that cannot be determined).
# However, the results only hold over symbolic states at a given depth and might not hold in general.
# The function terminates because the loop executes for at most $d_{max} - d_{\mathit{def}}$ iterations.

# \subsubsection{Bootstrapping DIG with Concrete States}\label{sec:bootstrap}

# \begin{algorithm}[t]
#   \small
#   \DontPrintSemicolon
#   \Input{program $P$, location $L$, symbolic depth $k$, number of requested concrete states $n$}
#   \Output{set of concrete states \cstates, clauses to \block}
#   \BlankLine
  
#   $\block \gets \text{false}$\;
#   $\cstates \gets \emptyset$\;
#   $\sstates \gets \getSymbolicStates(P, L, k)$\;\label{line:getsymbolicstates}
#   $\inputVars \gets \getInputs(P)$\;
  
#   \ForEach{$s \in \sstates$} {\label{line:loop1Start}
#     \If{$\SAT(s.\Pi)$} {
#       $\model \gets \getModel()$\;\label{line:model}
#       $\cstates \gets \cstates \cup(L, \eval(s.\sigma,\model))$\;\label{line:union}
#       $\block \gets \block \lor (\bigwedge\limits_{v \in \inputVars} v\equiv\model[v])$\; }
#   }
  
#   \While{$|\cstates | < n \wedge |\sstates| > 0$}{\label{line:loop2Start}
#     $s \gets \choose(\sstates)$\;
#     \If{$\SAT(s.\Pi \land \lnot \block)$} {\label{line:satblock}
#       $\model \gets \getModel()$\;
#       $\cstates \gets \cstates \cup(L, \eval(s.\sigma,\; \model))$\;
#       $\block \gets \block \lor (\bigwedge\limits_{v \in \inputVars} v\equiv\model[v])$\;\label{line:blockstates}
#     }
#     \Else{
#       $\sstates \gets \sstates - \{s\}$\label{line:unsatblock}
#     }
#   }
#   \BlankLine
#   \KwRet \cstates, \block
#   \caption{\texttt{getConcreteStates}: generate concrete states from symbolic states.}\label{fig:getConcreteStates}
# \end{algorithm}

# {{{tool}}} generates candidate invariants using existing concrete state-based invariant inference techniques like DIG.
# In this application, we only need a small number of concrete states to bootstrap the algorithms to generate a diverse set of candidate invariants since symbolic states will be used to refute spurious invariants.
# In prior work~\cite{nguyen2012using,nguyen2014dig}, fuzzing was used to generate inputs and that could be used here as well, but we instead exploit symbolic states which allows us to force diversity among
# generated concrete states, e.g., one per symbolic state.

# Figure~\ref{fig:getConcreteStates} shows how we use symbolic states to
# generate a diverse set of concrete states---at least one for each
# symbolic state. The loop on line~\ref{line:loop1Start} considers each such state, checks the satisfiability of the states path condition $\Pi$ and then extracts the model from the solver.
# Next, we bind concrete values to variables in the model to obtain a concrete state, which is then accumulated.
# %% We encode the model as a sequence, $\vec{i}$, indexed by the name of
# %% a free input variables.  The symbolic state is then evaluated by
# %% the binding of concrete values to input variables in the model.
# % This produces a concrete state which is accumulated.
# Then we block the model to avoid generating the same concrete state in the future.
# %% A conjunction of constraints equating the values of the model,
# %% $\vec{i}$, and the names of inputs, $I$, is added to the
# %% blocking clause for future state generation.

# The loop on line~\ref{line:loop2Start} generates additional concrete states up to the requested number, $n$.
# We randomly pick a symbolic state and then call an SMT solver to generate a
# solution that has not already been computed.
# When a solution is found, we use the same processing as in lines~\ref{line:model}--\ref{line:union} to create a new concrete state; otherwise, we block that symbolic state as in line~\ref{line:unsatblock} and continue.
# Note that it is possible that we cannot obtain exactly $n$ concrete states as requested and therefore cannot perform intended tasks (e.g., for equation solving, as discussed in~\S\ref{sec:inferring_equalities}).

# \textbf{Correctness and Termination:} {{{tool}}} generates symbolic states representing all possible paths up to a given bound, so generated invariants will be correct with respect to the paths found in that bound.
# Function \texttt{getConcreteStates} terminates because (1) the depth bounded symbolic
# execution (line~\ref{line:getsymbolicstates}) returns a finite set of states which guarantees
# termination of the loop on line~\ref{line:loop1Start}, and (2) the loop on line~\ref{line:loop2Start} terminates, since at each iteration either the set of concrete
# states computed  increases monotonically (newly added states cannot carry over from prior iterations since
# prior states are explicitly blocked from the SMT call (lines~\ref{line:satblock} and~\ref{line:blockstates})) or the set of symbolic states decreases monotonically (depth bounded symbolic execution produces a finite set of symbolic states).

# \subsection{Benefits of Symbolic States}\label{sec:benefits}
# Symbolic states are useful as a basis for efficient inference of invariants for several reasons:

# \textbf{Symbolic states are expressive}:
# Dynamic analysis has to observe many concrete states to obtain useful results.
# Many of those states may be equivalent from a symbolic perspective because a symbolic state can encode a potentially infinite set of concrete states.
# % A symbolic state, like $l_2$, encodes a potentially infinite set of concrete states, e.g., $X_1 > 0 \land X_2 = 1$.
# {{{tool}}} exploits this expressive power to infer and refute candidate invariants from a huge set of concrete states by processing a single symbolic state.

# \textbf{Symbolic states are relational}:
# Symbolic states encode the values of program variables as expressions
# over free-variables capturing program inputs.
# This permits relationships between variables to be gleaned from the state.
# % For example, state $l_2$ represents the fact that $y3 < x1$ for a large set of inputs.

# \textbf{Symbolic states can be reused}:
# Invariant generation has to infer or refute
# candidate invariants relative to the set of observed concrete states.
# This can grow in cost as the product of the number of candidates and
# the number of concrete states.
# A disjunctive encoding of observed symbolic states can be constructed once and reused for each of
# the candidate invariants, which can lead to performance improvement.

# \textbf{Symbolic states can be used for optimization}:
# % By representing symbolic states as constraints
# We can use a constraint solver to check guessed invariants from symbolic states.
# However, for certain types of invariants, we can eschew guessing and directly use the solver to compute invariants.
# For example, instead of checking if $x + y \le 10$ is an invariant, we just query the solver to find the least upper bound of the term $x + y$ from symbolic states.
# Thus, instead of performing multiple guesses and checks, we can obtain the desired invariant with a single call to the solver.

# \textbf{Symbolic states form a sufficiency test}:
# The diversity of symbolic states found during
# depth-bounded symbolic execution combined with the expressive power of each of those states provides a rich basis for inferring strong invariants.
# We have observed that for many programs a sufficiently
# rich set of observed states for invariant inference will be
# found at relatively shallow depth.
# That is, the invariants generated and consistent with
# symbolic states at depth 10 are the same as those at depth 11.
# %% For example, the invariants generated and not refuted by the
# %% disjunction of L-states at depth 5, $L_{k=5} = \{l_1,l_2,l_3,l_4,l_5,l_7,l_8,l_9,l_{11}\}$, is the
# %% same for those at depth 6, $\bigvee\limits_{i \in [1-13]} l_i$.
# Consequently, we employ an adaptive and
# incremental approach that increases depth only when new states
# lead to changes in candidate invariants.

# \section{The {{{tool}}} Approach}\label{sec:dig}

# {{{tool}}} uses symbolic states to generate equality and inequality forms of numerical invariants.
# First, we present a CEGIR technique that integrates a dynamic inference algorithm from DIG, which generates (nonlinear) \emph{equality} invariants from concrete states, with symbolic states to check and refine invariants.
# Then, we present an optimization technique to compute \emph{inequality} invariants directly from symbolic states.

# %% We introduce techniques that integrate algorithms from DIG, which infers candidate numerical invariants from concrete states, with symbolic states to generate diverse concrete states for invariant inference and checking.


# \subsection{Nonlinear Equalities}
# At a high level, we treat concrete state values as points in Euclidean space and compute geometric shapes enclosing these points.
# For example, the values of the two variables $x, y$ are points in the $(x, y)$-plane.
# We then can determine if these points lie on a line, represented by a linear equation of the form $c_0 + c_1x + c_2y = 0$.  % If such a line does not exist, we can build a bounded convex polygon enclosing these points.

# Thus, we treat equalities as unbounded geometric shapes, e.g., lines and planes, to obtain a set or conjunction of linear equalities over program variables.
# To support nonlinear equalities, we create \emph{terms} to represent nonlinear information from the given variables up to a certain degree.
# For example, the set of 10 terms  $\{1,r,y,a,ry,ra,ya,r^2,y^2,a^2\}$ consists of all monomials up to degree 2 over the variables $\{r,y,a\}$.
# In total, we enumerate  $\binom{|V|+d}{d}$ monomial terms over $n$ variables up to the given degree $d$.

# {{{tool}}} uses the equation solving technique in DIG to find equality invariants over these terms by using concrete states.
# % First, we create \emph{terms} to represent nonlinear information from the given variables  up to degree $d$ (line ~\ref{line:terms}).
# First, we form an equation \emph{template}
# \begin{equation}
#   \label{eq:eqt}
#   c_1t_1 + c_2t_2 \dotsb + c_nt_n = 0
# \end{equation}
# where $t_i$ are the generated terms and $c_i$ are real-valued unknowns to be solved for.
# Next, we instantiate the template with concrete state values to obtain concrete and linear equations.
# For example, instantiating the template with the concrete state $r=3,y=2,a=6$ produces the equation  $c_1 + 3c_2 \dotsb + 36c_n = 0$.
# We then solve these equations for the unknown coefficients using a standard linear equation solving technique such as  Gauss-Jordan elimination~\cite{strang1993introduction}.
# Finally, we combine solutions for the unknowns (if found) with the template to obtain equality relations.

# \subsubsection{Inferring Equalities}\label{sec:inferring_equalities}
# \begin{algorithm}[t]
#   \small
#   \DontPrintSemicolon
#   \Input{program $P$, location $L$, degree $d$, symbolic depth $k$}
#   \Output{nonlinear equalities up to deg $d$ at $L$}
#   \BlankLine

#   $\cstates \leftarrow \emptyset$\;
#   $\invs \leftarrow \emptyset$\;
#   $\vars \leftarrow \getVars(P, L)$\;
#   $\terms \leftarrow \createTerms(\vars, d)$\;\label{line:terms}
#   $\cstates, \block \leftarrow \getConcreteStates(P, L, k, |\terms|)$\;\label{line:genstates}
#   $\candidates \leftarrow \inferEqts(\terms, \cstates)$\;\label{line:infer0}
#   \While{$\candidates \neq \emptyset$}{\label{line:loop0}
#     $\cexs \leftarrow \emptyset$\;
#     \ForEach {$p \in$ \candidates}{\label{line:forloop}
#       $\isProved, \newcexs \leftarrow \check(P, L, p, \block)$\;
#       $\cexs \leftarrow \cexs \cup \newcexs$\;
#       \lIf{$\isProved$}{$\invs \leftarrow \invs \cup \{p\}$}
#     }
    
#     \lIf{\cexs $\equiv \emptyset$}{\mybreak}
#     $\block \leftarrow \block \cup \cexs$\;
#     $\cstates \leftarrow \cstates \cup \cexs$\;\label{line:append}
#     $\newcandidates \leftarrow \inferEqts(\terms, \cstates)$\;
#     \candidates $\leftarrow \newcandidates - \invs$\;\label{line:loop1}
#   }
  
#   \BlankLine
#   \KwRet \invs \;
  
#   \caption{Algorithm for finding (potentially nonlinear) polynomial equalities. The \texttt{check} function described in~\S\ref{sec:verify} is used to validate invariants.}\label{fig:findeqts}
# \end{algorithm}

# Figure~\ref{fig:findeqts} defines our CEGIR algorithm for computing nonlinear equality invariants.
# It consists of two phases: an initial invariant candidate generation
# phase and then an iterative invariant refutation and refinement phase.

# Lines~\ref{line:terms}\,--\,\ref{line:infer0} define the initial generation phase.
# We first create terms to represent nonlinear polynomials.
# Because solving for $n$ unknowns requires at least $n$ unique equations, we use symbolic states as described in~\S\ref{sec:bootstrap} to generate a sufficient set of concrete states.

# The initial candidate set of invariants is iteratively refined on lines~\ref{line:loop0}\,--\,\ref{line:loop1}.
# The algorithm refutes or confirms them using symbolic states as described in~\S\ref{sec:verify}.
# Any property that is proven to hold is recorded in $\texttt{invs}$ and counterexample  states, $\texttt{cexs}$, are accumulated across the set of properties.
# Generated counterexample states are also blocked so that they are not generated again.

# If no property generated counterexample states, then the
# algorithm terminates returning the verified invariants.
# The counterexamples are added to the set of states that are used to infer new candidate
# invariants; this ensures that new invariants will be consistent with the counterexample states (line~\ref{line:append}).
# These new results may include some already proven invariants, so we remove those from the set of candidates considered in the next round of refinement.


# \textbf{Example:} We demonstrate the algorithm by finding the nonlinear equalities $b=ya$ and $x=qy+r$ at location L1 in the \texttt{cohendiv} program in Figure~\ref{fig:cohen}, when using degree $d=2$.

# % iter 1 (6)
# % 19*a*r - b*r - 19*a*x + b*x + 171*q + 9*r - 9*x == 0
# % 190*q*r + 10*r^2 - 10*x^2 - 551*q - 2929*r + 2929*x == 0
# % 10*r*x - 10*x^2 + 1539*q - 2819*r + 2819*x == 0
# % a*y - b == 0
# % q*y + r - x == 0
# % r*y - x*y - 171*q - 28*r + 28*x == 0



# % iter 2 (4):

# % 19*a*q - b*q - 19*q - r + x == 0
# % 171*q^2 + 28*q*r + r^2 - 28*q*x - 2*r*x + x^2 + 2*r*y - 2*x*y - 342*q - 56*r + 56*x == 0
# % a*y - b == 0
# % q*y + r - x == 0


# % iter 3(3):

# % 57*a*q - 22*b*q - b*r + b*x + r*y - x*y - 57*q - 22*r + 22*x == 0
# % a*y - b == 0
# % q*y + r - x == 0


# % iter 4(2):

# % q*y + r - x == 0
# % a*y - b == 0



# For the 6 variables $\{a,b,q,r,x,y\}$ at L1, together with $d=2$, {{{tool}}} generates 28 terms $\{1,a,\dots,y^2\}$.
# {{{tool}}} uses these terms to form the template $c_1 + c_2a + \dots c_{28}y^2=0$ with 28 unknown coefficients $c_i$.
# {{{tool}}} then uses the obtained symbolic states to generate concrete states such as those given in Figure~\ref{fig:cohen} to form (at least) 28 linear equations.
# From this set of initial equations {{{tool}}} extracts 6 equalities.

# Now, {{{tool}}} iteratively refines the inferred invariants.
# In iteration \#1, {{{tool}}} cannot refute 2 of these candidates $x = qy+r, b = ya$ (which are actually true invariants) and thus saves these as invariants.
# {{{tool}}} finds counterexamples for the other 4 equalities\footnote{Spurious results often have many terms and large coefficients, e.g.,  $190qr + 10r^2 - 10x^2 - 551q - 2929r + 2929x = 0$}, and creates new equations from the counterexamples.
# {{{tool}}} next combines the old and new equations and solves them to obtain 4 candidates, 2 of which are the already saved ones.
# In iteration \#2, {{{tool}}} obtains counterexamples for the 2 new candidates.
# With the help of the new counterexamples, {{{tool}}} generates 3 candidates, 2 of which are the saved ones.
# In iteration \#3, {{{tool}}} obtains counterexamples disproving the remaining candidate and again uses the new counterexamples to generate new candidates.
# This time {{{tool}}} only finds the two saved invariants $x = qy+r, b = ya$ and thus stops.

# \textbf{Correctness and Termination:}
# The algorithm returns (potentially nonlinear) equalities that are correct up to the considered symbolic depth (since it uses the \texttt{check} function in \S\ref{sec:verify} to check invariants with respect to the given depth).
# The algorithm terminates because linear equation solving provides a solution space containing all possible coefficients for an equality invariant and each added counterexample input  decreases the dimension of the solution space by one. 
# Thus if we keep finding counterexamples, then the solution space will keep decreasing, and eventually, that will stop when the dimension of the solution space becomes zero (i.e., no equalities found).
# The full termination proof is provided in the supplementary appendix.


# \textbf{Insufficient Traces:} This algorithm might fail to produce equality invariants due to insufficient traces.
# More specifically, if the \texttt{getConcreteStates} call on line~\ref{line:genstates} fails to obtain $|\texttt{terms}|$ concrete states, then we cannot form the necessary equations to solve for $|\texttt{terms}|$ unknowns and thus the \texttt{inferEqts} call on line~\ref{line:infer0} produces no results.

# \begin{wrapfigure}{r}{0.28\linewidth}
#   \vspace{-0.2in}
# \begin{lstlisting}[basicstyle=\ttfamily\footnotesize,emph={L}]
# if(x==1&&y==2){
#   [L]
#   ..
# }
# \end{lstlisting}
#   \vspace{-0.2in}
# \end{wrapfigure}
# The example on the right illustrates the situation.
# The program has exactly 1 concrete L-state (i.e., $\{(x=1 \wedge y = 2)\}$), thus we can form only a single equation and cannot solve for 2 or more unknowns.
# While it is possible to mitigate the problem in specific cases (e.g., equation solving is not even needed here), it is challenging for more general cases (e.g., if we do not have enough traces to solve $n$ unknowns, do we consider only some subsets of those unknowns? and if so, which subsets to consider?).

# Fortunately, this problem, in which every considered variable at a location has a fixed value, rarely happens.  In the above program, if L has another variable $z$ that is not fixed to a value, then we can generate a large number of concrete traces, e.g., $\{(x=1, y=2, z=0), (x=1,y=2,z=1), (x=1,y=2,z=2), \dots\}$, and form equations to obtain the invariants $x=1$ and $y=2$. Thus, as long as some of the considered variables are ``free'', we can generate sufficient traces for equation solving.

# Note that for this example, even if we cannot obtain the equalities \(x=1 \land y=2\) due to insufficient traces, we can still infer inequalities that are exactly equivalent, i.e., \(x \le 1 \land 1\le x \land y \le 2 \land 2\le y\), using the inference technique described next in~\S\ref{sec:ieqs}.


# % Appendix~\ref{sec:eqterminate} provides .

# % adding a counterexample will decrease the 
# % The linear equation solving process gives us a solution space containing all possible coefficients for an equation invariant.
# %We have proved (see Appendix~\ref{sec:eqterminate}) that each counterexample added to the system decreases the dimension of the solution space by one.
# % When the dimension reaches zero, we found no meaningful equation invariant, so we stop the inference algorithm after a finite number of counterexamples.
# %The algorithm terminates because the initial
# %set of candidate invariants is finite (line~\ref{line:infer0}) and each iteration
# %of the loop guarantees that this set is reduced.
# %The outcome of the loop at line~\ref{line:forloop} is that either $\mathsf{cexs}$ or $\mathsf{invs}$ is increased.
# %If some \cexs exist then line 16 computes a new set of candidates by using
# %this increasingly large set of counterexamples.  Since these
# %form a conjunctive set of constraints with which all invariants must
# %be consistent, increasing the set guarantees a reduction in
# %{\newcandidates} which in turn reduces {\candidates} at line 17.

# \subsection{Linear Inequalities}\label{sec:ieqs}

# \begin{figure}[t]
#   \hfill
#   \geoshapes{}{a}{0.20}
#   \hfill
#   \geoshapes{
#     \draw[mygeoshape]
#     (-2,1) -- (5,2) -- (3,-2) -- (1,-3) -- (-1,-1) -- cycle;
#   }{b}{0.20}
#   \hfill
#   \geoshapes{
#     \draw[mygeoshape]
#     (-2,1) -- (-1,2) -- (5,2) -- (5,0) -- (2,-3) -- (1,-3) -- (-2,0) -- cycle;
#   }{c}{0.20}
#   \hfill
#   \geoshapes{
#     \draw[mygeoshape]
#     (-2,1) -- (-1,2) -- (5,2) -- (5,0) -- (2,-3) -- (-2,-3) -- cycle;
#   }{d}{0.20}
#   \hfill
#   \geoshapes{
#     \draw[mygeoshape] (-2,2) -- (5,2) -- (5,-3) -- (-2,-3) -- cycle;
#   }{e}{0.20}
#   \hfill
#   \caption[Geometric Invariant Inference]
#   {(a) A set of 6 points in 2D  and its approximation using the (b) polyhedral, (c) octagonal, (d) zone, and (e) interval shapes, represented by the conjunctions of inequalities of the forms $c_1v_1+c_2v_2\le c$, $\pm v_1 \pm v_2 \le c$, $v_1 - v_2 \le c$, and $\pm v \le c$, respectively.}\label{fig:geoshapes}
# \end{figure}


# %% essentially by finding convex and non-convex polyhedra that contain observed concrete states in a multi-dimensional space.

# % {{{tool}}} uses DIG's algorithms to infer linear inequalities by computing convex hulls over points created from concrete states.
# {{{tool}}} supports several forms of inequality invariants.
# Below we describe inequalities using different geometric shapes and then present a linear programming approach that computes inequalities directly from symbolic states.
# {{{tool}}} supports both linear (default) and nonlinear (configurable by the user) inequalities.
# % a techniques that {{{tool}}} uses to find certain types of  inequalities: (i) a CEGIR approach that builds convex and non-convex polyhedra from concrete states to represent inequalities (and check them against symbolic states) and (i) a 
# % By default, {{{tool}}} generates linear inequalities, but the user can configure it to compute nonlinear relations.

# % because they represent two major ways to exploit symbolic states.
# %% The earlier version of {{{tool}}}~\cite{nguyen2017syminfer} uses a CEGIR technique to infer inequalities over concrete states and check them against symbolic states.
# %% The current version of {{{tool}}} uses a new algorithm to infer inequalities directly over symbolic states.

# % The CEGIR technique in {{{tool}}} uses DIG's algorithms to infer linear inequalities by computing convex hulls over points created from concrete states.
# Figure~\ref{fig:geoshapes} shows several types of convex polygons (polyhedra in 2D) that represent different forms of inequalities over two variables.
# Figure~\ref{fig:geoshapes}a shows a set of points created from concrete states.
# Figures~\ref{fig:geoshapes}b,~\ref{fig:geoshapes}c,~\ref{fig:geoshapes}d, and~\ref{fig:geoshapes}e approximate the region enclosing these points using the polyhedral, octagonal, zone, and interval shapes that are represented by \emph{conjunctions of inequalities} of different forms as shown in Figure~\ref{fig:geoshapes}.
# Note that these forms of relations are sorted in decreasing order of expressive power and computational cost.
# For example, while interval relations are less expressive than zone relations, computing an interval, i.e., the upper and lower bounds of a variable, is much cheaper than computing the convex hull of a zone.

# %% {{{tool}}} computes two types of convex polyhedra to infer two classes of inequality invariants: octagonal relations and max/min-plus relations.

# \subsubsection{Octagonal Relations}
# While inequalities represented by a general polyhedron are expressive, computing a polyhedron is expensive (exponential time in the number of dimensions or variables) and often produces many complex and spurious invariants~\cite{nguyen2014dig}.
# {{{tool}}} thus focuses on \emph{octagonal} invariants, whose shape and inequality form are shown in Figure~\ref{fig:geoshapes}c.
# Octagonal invariants can be computed efficiently from concrete states (linear time complexity~\cite{nguyen2014dig}) and are also relatively expressive (e.g., can represent zone and interval inequalities as shown in Figure~\ref{fig:geoshapes}).
# Thus, the computation of octagonal relations also produces zone and interval relations for free.
# By balancing expressive power with computational cost, octagonal invariants are especially useful in practice for detecting bugs in flight-control software, and performing array bound and memory leak checks~\cite{cousot2005astree,mine2004weakly}.

# The edges of an octagon are represented by a conjunction of eight inequalities of the form
# \begin{equation}
#   \label{eq:oct}
#   c_1v_1 + c_2v_2 \le k,
# \end{equation}
# where $v_1,v_2$ are variables, $c_1,c_2\in \{-1,0,1\}$ are  coefficients, and $k$ is a real-valued constant.
# For example, from the concrete states in Figure~\ref{fig:cohen}, we can build an octagon represented by inequalities such as $4 \le x \le 15$ and $3 \le x - y \le 13$.


# \subsubsection{Max- and Min-plus Relations}
# \begin{figure}[t]
#   \hfill
#   \geoshapesB{ %max lines
#     \draw[myline,<->,draw=red] (0,3) -- (4.5,3) -- (7,5.5);
#     \draw[myline,<->,draw=red] (5,0) -- (5,2) -- (7,4);
#     \draw[myline,<->,draw=red] (0,2) -- (4,2) -- (4,0);
#   }{a}{0.20}
#   \hfill
#   \geoshapes{ %max-plus
#     \draw[mygeoshape]
#     (-2,1) -- (4,1) -- (5,2) -- (3,0) -- (3,-2) -- (2,-3) -- (1,-3) -- (1,-1) -- (-1,-1) -- (-1,1);
#   }{b}{0.20}
#   \hfill
#   \geoshapesB{  %weak max lines
#     \draw[myline,<->,draw=red] (0,3) -- (4.5,3) -- (7,5.5);
#     \draw[myline,<->,draw=red] (5,0) -- (5,2) -- (7,4);
#   }{d}{0.20}
#   \hfill
#   \geoshapes{
#     \draw[mygeoshape]
#     (-2,-3) -- (-2,1) -- (0,1) -- (1,2) -- (5,2) -- (3,0) -- (3,-2) -- (2,-3) -- (1,-3) -- cycle;
#   }{c}{0.20}
#   \hfill
#   \caption{(a) Three possible shapes of a max-plus line segment: $\max(c_1+v_1,c_2) \le v_2$ (top), $\max(c_1+v_2,c_2) \le v_1$ (right), $\max(c_1+v_1,c_2+v_2) \le c_3$ (left); (b) approximation of the points in Figure~\ref{fig:geoshapes}a using a max-plus polygon; (c) two possible line segment of weak max-plus; and (d) approximation of points using a weak max-plus polygon.}\label{fig:maxplus}
# \end{figure}

# The convex polygons shown in Figure~\ref{fig:geoshapes} represent conjunctions, but not disjunctions, of numerical relations.
# \emph{Disjunctive invariants}, which represent the semantics of branching, are more difficult to analyze, but are crucial to many programs.
# For example, after \texttt{if (p) \{a=1;\} else \{a=2;\}}, neither $a=1$ nor $a=2$ is an invariant, but
# \[
#   (p \land a=1) \lor (\lnot p \land a=2)
# \]
# is a disjunctive invariant.

# {{{tool}}} supports a form of disjunctive invariants by using a special type of nonconvex polyhedra in the \emph{max-plus} (max) algebra~\cite{allamigeon2008inferring,kapur2013geometric,maclagan2015introduction}.
# Briefly, max inequalities are analogous to polyhedra inequalities, but use $(\max,+)$ instead of the $(+, \times)$ of standard arithmetic.
# These operators allow max relations to form shapes that are nonconvex in the classical sense.
# For example, the max relation $\max(x,y) \le 5$, which is equivalent to 
# $(x \ge y) \implies x \le 5 ~\land (x < y)~ \implies y \le 5$, 
# can be simplified to $x\le 5 ~\lor~ y \le 5$---a nonconvex area.
# Figure~\ref{fig:maxplus}a shows the three possible shapes of a max line segment in 2D.
# Figure~\ref{fig:maxplus}b depicts a max polygon represented by a set of four lines connecting the points shown in Figure~\ref{fig:geoshapes}a.

# Similarly to polyhedra, computing a general max convex hull is expensive and can result in many spurious inequalities.
# We thus focus on a ``weaker'' form of max invariants introduced in~\cite{nguyen2014using} that retains much of the general forms expressive power, but avoids the high computational cost.

# Weak max invariants have the form:
# \begin{equation}\label{eq:max}
#   \begin{split}
#     \max(c_0,c_1+v_1,\dots,c_k+v_k) \le v_j+d\\
#     v_j + d \le \max(c_0,c_1+v_1,\dots,c_k+v_k),
#   \end{split}
# \end{equation}
# where $v_i$ are program variables, $c_i \in \{0,-\infty\}$, $d$ is a real numbers or $-\infty$, and $k$ is a constant, e.g., $k=2$ in 2D.

# Weak max relations are thus a strict subset of general max relations.
# For example, the weak max form cannot represent  general max relations like $\max(x+7,y)\le z$ or $\max(x,y) \le \max(z,w)$, but it does support zone relations like $x -y \le 10$ and $x = y$ and disjunctive relations like $\max(x,y)\le z$ and $\max(x,0) \le y+7$.
# Geometrically, weak max polyhedra are a restricted kind of max polyhedra in which a right angle cannot occur because their formula, $\max(x,y)\le 0$, is inexpressible using the weak max form.
# Figure~\ref{fig:maxplus} compares general and weak max polyhedra in 2D.

# The advantage of these restrictions is that they admit a straightforward and efficient algorithm to compute the bounded weak max polyhedron over a set of finite points representing concrete states in $k$ dimensions.
# The algorithm first enumerates all possible weak relations over $k$ variables and then finds the unknown parameter $d$ in each relation
# from the given points.  The resulting set of relations represent the weak max polyhedron enclosing the
# points.

# Dually, {{{tool}}} also computes weak min invariants, whose form is similar to the one in Eq~\ref{eq:max} but with \texttt{min} instead of \texttt{max}:
# \begin{equation}
#   \label{eq:min}
#   \begin{split}
#     \min(c_0,c_1+v_1,\dots,c_k+v_k) \le v_j+d,\\
#     v_j + d \le \min(c_0,c_1+v_1,\dots,c_k+v_k).
#   \end{split}
# \end{equation}
# The algorithm for computing these invariants over concrete states is similar to the one for weak max invariants described above.

# % Dually, we also consider weak min-plus polyhedra and combined max- and min-plus relations capturing if-and-only-if information.
# % \tvn{For example ...}
# % \mbd{I think this would be interesting to develop.}
# % \tvn{For a full max-plus constraint we can get if-and-only-if, not sure about the weaker max-plus constraints that this paper considers}

# %% Although a max-plus polyhedron is not convex in the classical sense, it is convex in the max-plus sense using max-plus algebra.  That is, it contains
# %% any max-plus line segment between any pair of its points.
# %% This allows for computing max-plus polyhedra over a fite set of concrete states, as shown in Figure~\ref{fig_maxpus}b.




# \subsubsection{Inferring Inequalities}
# \begin{algorithm}[t]
#   \small
#   \DontPrintSemicolon
  
#   \Input{program $P$, location $L$, symbolic depth $k$}
#   \Output{octagonal or max/min inequalities at $L$}
#   \BlankLine
  

#   $\invs \leftarrow \emptyset$\;
#   $\vars \leftarrow \getVars(P, L)$\;
#   $\terms_{oct} \leftarrow \createTerms_{ieq}(\vars, d=1, \coefs=(-1,0, 1), \subsetSize=2)$\;\label{line:termsoct}
#   $\terms_{minmax} \leftarrow \createTerms_{minmax}(\vars, d)$\;\label{line:termsminmax}
#   \ForEach {$t \in \terms_{oct} \cup \terms_{minmax}$}{\label{line:forLoop2start}
#     $k \leftarrow \optimize(P, L, t)$\;
#     \If{$k \ne \infty$}{
#       $\invs \leftarrow \invs \cup (t \le k)$\;\label{line:forLoop2end}
#     }
#   }

#   \BlankLine
#   \KwRet \invs \;
  
#   \caption{Algorithm for finding octagonal and max/min-plus inequalities. The \texttt{optimize} function described in~\S\ref{sec:optimize} is used to find upper bound values of terms.}\label{fig:findieqs}
# \end{algorithm}

# {{{tool}}} uses a single algorithm, shown in Figure~\ref{fig:findieqs}, to infer octagonal and (weak) max and min invariants.
# The key idea is to compute the invariant $t \le k$ where the term $t$ represents different forms of octagonal and max/min inequalities and the constant $k$ is the upper bound of $t$.

# The algorithm consists of two phases.
# First, we invoke \texttt{createTerms} to enumerate terms over program variables (lines~\ref{line:termsoct}--~\ref{line:termsminmax}).
# For octagonal inequalities, each term $t$ involves two variables so that $t \le k$ is an octagonal constraint of the form in Eq~\ref{eq:oct}. %, e.g., $t$ could be $x$, $y$, $x+y$, or $x-y$.
# Given $n$ variables, we create $\binom{n}{2}$ variable pairs, and for each pair $x,y$, obtain $8$ terms of the octagonal form in Eq~\ref{eq:oct} in which $x$ and $y$ are associated with one of the 3 coefficients $\{-1,0,1\}$, e.g., $-1 \times x + 1\times y$ gives the term $-x+y$ (and our goal is to find the upperbound $k$ to form the octagonal inequality $-x+y \le k$).
# In total, we generate $\binom{n}{2} \times (2^3 - 1)$ octagonal terms (we exclude the term $0\times x + 0 \times y$, which simplifies to 0).

# For max inequalities, $t$ involves combinations of variables of the weak form in Eq~\ref{eq:max}. 
# Given $n$ variables, we create $\binom{n}{n-1}$ tuples of variables, and for each tuple of $k$ variables, obtain $2^{k-1}$ terms of the form in Eq~\ref{eq:max} in which each of the $k-1$ variables is associated with one of the 2 coefficients $\{0, -\infty\}$. 
# For example, the invariant
# $\max(0, -\infty + x, 0 + y) \le z + d$ generates the term $\max(0,y) - z$ that we optimize to
# compute an upperbound $k$ that forms the max inequality  $\max(0,y) - z \le k$.
# In total, we generate $\binom{n}{n-1} \times 2^{k-1}$ max terms.
# We also do the same to obtain min terms of the form in Eq~\ref{eq:min}.

# Then, we use the \texttt{optimize} function described in \S\ref{sec:optimize} to compute the smallest upper bound $k$ of each term $t$ from symbolic states (lines~\ref{line:forLoop2start}--~\ref{line:forLoop2end}).
# If $k$ is found, we obtain the candidate invariant $t\le k$; otherwise, we discard the relation $t\le k $.

# We also use the same approach to find the lower bound of a term
# $t$ (i.e., $k \le t$, which is also an octagonal inequality) by computing the upper bound of the term $-t$.
# % This is possible because the semantics and results of all computations are reversed when we consider $\mathtt{-t}$. 
# For example, if $t \in \{-2,3,7\}$ then we have $t \le 7$ and $-t \le 2$, which is equivalent to $-2 \le t$.
# The function \texttt{createTerms} generates both terms $t$ and $-t$.

# \textbf{Correctness and Termination: }
# This algorithm returns inequalities that are correct up to the considered symbolic depth (since it uses \texttt{optimize} to obtain the upper bounds with respect to the given depth). The algorithm terminates because the enumerated terms are finite and each \texttt{optimize} call terminates.

# Note that the previous version of {{{tool}}}~\cite{nguyen2017syminfer} computes the upper bound for a term using a CEGIR approach, which repeatedly invokes symbolic states to guess and tighten the candidate upper bounds.
# This approach, which invokes the solver multiple times to check guessed results, is slower than the described optimization-based algorithm, which invokes the solver once to find the upper bound.
# However, the CEGIR algorithm, provided in the supplementary appendix, might be useful when computing the upper bound directly from symbolic states is not possible (e.g., when symbolic states or terms involve complex arithmetic not supported by the constraint solver's optimizer).

# % \subsection{Inferring Pre- and Post-Conditions}
# % As shown in section~\ref{sec:applications}, {{{tool}}}'s invariants reveal that the \texttt{tripple} program in Figure~\ref{fig:complexity} has different run-time complexities depending on the inputs $M,N,P$, e.g., the loop does not run when $N=0$, runs in linear time when $N \le P$, and quadratic time when $N > P$.  Surprisingly, these information, reprepresented by invariants involving both pre and post-conditions, are extracted from a single nonlinear invariant.

# % {{{tool}}} infers invariants involving multiple pre and postconditions by first generating nonlinear invariants representing postconditions.
# % {{{tool}}} next solves these nonlinear postconditions to obtain multiple solutions.
# % Then for each solution, the tool extract traces consistent to that solution, and infers relations over input variables on those traces.
# % The inferred relations represent preconditions associated with the postconditions.

# % \textbf{Example:} We use the \texttt{tripple} program to demonstrate the technique.
# % First, the tool infers the postcondition $P^2Mt + PM^2t - PMNt - \dots= 0$, where the variable $t$ represents the number of loop iterations.
# % {{{tool}}} then solves this equality for $t$ and obtains three solutions $t=0,t=P+M+1, t=N-M(N-P)$, which suggest that the program has three possible complexity bounds.
# % For each equality, {{{tool}}} extracts from the observed traces those that are consistent with the equality.
# % Next, the tool computes relations over input variables from these traces to represent to conditions leading to the considered complexity bound.
# % For examples, we infer $N \le P$ from the traces satisfying $t=P+M+1$ and thus obtain the invariant $N \le P \implies t=P+M+1$.
# % Note that if multiple preconditions, e.g., $p_1,\dots,p_k$, are obtained from the traces satisfying the postcondition $q$, {{{tool}}} combines them to obtain the invariant $ p_1 \lor p_k \implies q$.
# % Finally, {{{tool}}} uses the algorithm described in Figure~\ref{fig:check} to check and return the inferred invariants.

# \subsubsection{Nonlinear Inequalities}\label{sec:nlieqs}

# % \matt{I found this section confusing.  There is a discussion of using configurations to get these capabilities, but there is no presentation of the ``language of configuration''.  If we want to include these capabilities we need to define what can be done precisely and to describe how the config setting parameterize the preceding algorithms.}
# By default, {{{tool}}} generates the described octagonal and max/min-plus relations.
# However, the user can easily configure {{{tool}}} to generate more expressive inequalities by either specifying a command line argument and setting environment variables of {{{tool}}}.

# For example, by default function \texttt{createTerms}$_{ieqs}$ in Figure~\ref{fig:findieqs} generates octagonal terms such as $x, x-y$. These terms are linear (degree $d$ = 1), have coefficients -1, 0, 1, and involve at most two variables (\texttt{subsetSize=2}).
# {{{tool}}} allows the user to change these parameters to generate more expressive invariants involving more variables (e.g., using \texttt{subsetSize=3} would generate invariants such as  $x + y -z \le s$), having larger coefficients (changing the coefficients range to (-5,5) would produce invariants such as $2x - 5y \le z$), and with higher degrees (setting $d=2$ would generate  nonlinear inequalities such as $x^2 + z + w \le 7$).  The user also can introduce new terms to represent desired information (e.g., we can obtain relations involving exponential properties such as  $2^x \le y$ by representing $2^x$ with a new term).
# The trade-off is that {{{tool}}} would take longer to generate and analyze these richer invariants.

# % \subsection{Generated Terms}\label{sec:terms}

# % The performance of {{{tool}}} for inferring equalities and inequalities strongly depends on the generated terms over variables. For equalities, the number of generated terms specifies the number of unknowns to be solved for, and for inequalities, we compute octagonal, max/min, and nonlienar invariants over each term as shown in Figure~\ref{fig:findieqs}.
# % %The number of terms also suggest the number of potentially generated invariants, especially for inequalities where each term can result in a candidate invariant.
# % Below we describe how terms are generated over a set of $n$ variables for each considered form of invariants.

# % For equality invariants of the form in Eq~\ref{eq:eqt}, we create $\binom{|V|+d}{d}$ terms representing monomials over variables up to the given degree $d$.
# % For octagonal invariants of the form in Eq~\ref{eq:oct}, we enumerate pairs of variables and create 8 terms for each pair, thus the number of terms is $8\times \binom{|V|}{2}$.
# % More generally, if we consider $m$ variables and each variable associated with one of $k$ coeficients, then we would have $m^k\times \binom{|V|}{m}$ terms (the octagonal form in Eq~\ref{eq:oct} has $m=2$ variables and coefficients range over $k=3$ values $\{-1,0,1\}$). 
# % For max/min invariants of the form in Eq~\ref{eq:min} and Eq~\ref{eq:max}, we generate terms over all subsets of variables.\tvn{tofinish}


# \subsection{Post-Processing}
# Depending on the number of variables and form of invariants, {{{tool}}} could generate many invariants (e.g., each octagonal and max/min term can produce an invariant candidate).
# Reporting too many invariants, even if they are all valid, would be a burden to the user and reduce the general effectiveness of the tool.
# Thus, {{{tool}}} uses a post-processing step to reduce the number of reported invariants.

# The post-processing consists of two parts; both aim to reduce the number of reported invariants.
# The first part simply checks  generated invariants against all cached concrete states and removes violated ones.
# This part is efficient (we simply instantiate and check candidate relations with concrete values), but removes few results (because most generated invariants are already valid).

# The second part attempts to remove redundant invariants, i.e., from a set of candidate invariants, we extract a subset of \emph{independent} relations such that every member of the set is not implied by other relations in that set.
# This part is time-consuming because we essentially invoke the solver to check every candidate invariant, but is effective in reducing invariants because many invariants can be obtained by the combination of others.
# Our experiences show that this part can effectively reduce many inequalities to just a few strongest and relevant ones--making it much easier for the user to analyze and use the reported results.

# %\subsection{Parallel Processing}

# % \section{Implementation}
# % \begin{figure}
# %   \includegraphics[width=1\linewidth]{arch}
# %   \caption{{{{tool}}} Implementation}
# %   \label{fig:arch}
# % \end{figure}

# % Figure~\ref{fig:arch} shows the implementation of {{{tool}}}.


# % \paragraph*{Parallel Processing} \tvn{many things in tool are highly independent,}.

# % \matt{I thought the following was tool specific and not directly related to symbolic
# % states so we can drop it or push the discussion to the web-site.}
# % \tvn{sure, comment out}
# % \textbf{Pure Dynamic Analysis:} When symbolic execution is not feasible (e.g., the source code is unavailable or difficult to analyze), {{{tool}}} can perform pure dynamic invariant generation over program traces, i.e., concrete states generated by fuzzing or given by the user.
# % The generate invariants are correct with respect the given concrete states, but might not be in general because checking is not involved. 
# % However, depending on the type of invariants, e.g., equality relations, {{{tool}}} can generate very few spurious invariants~\cite{nguyen:icse12:dig}, even without symbolic checking.

# % \textbf{Configurability:} {{{tool}}} currently supports equality and inequality relations over nonlinear numerical variables as described in \S\ref{sec:dig}.
# % For equalities, {{{tool}}} uses techniques from DIG~\cite{nguyen2014dig} to limit the number
# % of generated terms.
# % This allows us, for example, to infer equalities up to degree 5 for a program with 4 variables and up to degree 2 for program with 12 variables.
# % For octagonal and max/min inequalities, we consider upper and lower bounds within the range $[-10,10]$; we rarely observe inequalities with large bounds.
# % {{{tool}}} can either choose random values in a range, $[-300,300]$ by default, for bootstrapping, or use the algorithm in Figure~\ref{fig:getConcreteStates}.
# % Our experience shows that we do not need very large input values to generate precise invariants.
# % We start SPF with depth 8 and CIVL with depth 20; these seem to be good default search depths for almost all our test programs.
# % % Cav09\_fig1a.c and Cav09\_1d.c requires a depth greater than or equal to 100 and 200 respectively for CIVL to produce symbolic states.
# % All these parameters can be changed by {{{tool}}}'s user; we chose these default values based on our experience.

# % The modular design of {{{tool}}} makes it easy to add new algorithms to analyze  additional forms of invariants.
# % The user just implements an \texttt{infer} function in Python to compute invariants over concrete states and adds that function into {{{tool}}}'s framework to take advantage of CEGIR, post-processing, and parallel computing, etc.
# % {{{tool}}} has been used this way in other projects to support new classes invariants, e.g., Dynamite~\cite{le2020dynamite}  is built on top of {{{tool}}} to generate ranking functions (for termination) and recurrence set (for non-termination) and the work in~\cite{nguyen2020using} extends {{{tool}}} to generate recurrence relations to determine program complexity.  We can also use other checking techniques instead of symbolic execution, e.g., the mentioned Dynamite project uses the Ultimate Automizer~\cite{} and CPAChecker~\cite{} tools to verify results and obtain counterexamples.
# % \matt{need citations here}


# \section{Implementation and Evaluation}\label{sec:eval}

# {{{tool}}} is implemented in Python/SAGE~\cite{sagemath}.
# The tool takes as input a program with marked target locations, i.e., using the \texttt{vtrace} method discussed in~\S\ref{sec:getss}, and generates invariants at those locations.

# Currently, {{{tool}}} supports programs written in Java, (Java) bytecode, and C.
# {{{tool}}} uses the Z3 SMT solver~\cite{demoura2008z3} to check and produce models representing counterexamples. We also use Z3 to identify and remove redundant invariants in post-processing.
# We use the same backend algorithms to infer and analyze invariants, but call different symbolic execution frontend tools to obtain symbolic states (Symbolic PathFinder (SPF)~\cite{anand2007jpf} for Java and bytecode programs and CIVL~\cite{siegel2015civl} for C programs).
# Our experiments focus on evaluating {\tool} on Java programs, and we discuss {\tool}'s performance on C programs in~\S\ref{sec:rq6}.

# {{{tool}}} leverages the increasingly popular and affordable multicore architecture.
# The tool performs many independent tasks in parallel, e.g., running symbolic execution at different depths, generating invariants at different locations, computing upper bounds for terms, and checking candidate invariants.
# Parallel processing is crucial to the performance of {{{tool}}} as it allows the tool to process and analyze thousands of candidate invariants at multiple program locations simultaneously.


# % \tvn{IGNORE para}
# % Ideally we would perform a comparative evaluation of {{{tool}}} and prior implementations of dynamic invariant inference.
# % However, as described in \S\ref{sec:related}
# % limitations in prior work mean that it is not directly comparable to {{{tool}}}.
# % \ignore{
# % Techniques like Daikon and iDiscover are unable to infer nonlinear or disjunctive invariants, thus it would be unfair to compare them to {{{tool}}}.
# % {{{tool}}} builds extends DIG and, thus, is more capable (by construction), thus it would be unfair to compare their results.
# % Techniques like PIE, ICE, and \cite{yao2020learning} require that programs come equipped with auxiliary specifications in order for them to infer invariants, thus they cannot be applied to specification-free programs.

# % % \matt{Where does NumInv, PIE, and HOLA fit in here? What is the rationale
# % % for not comparing?}
# % }
# % Consequently the focus of our evaluation is to explore the ability of {{{tool}}}
# % to infer nonlinear and disjunctive invariants that are documented in source programs.
# % While we report on the performance of {{{tool}}} to demonstrate that it is practical,
# % our primary metric is the degree to which documented invariants are recovered.
# % In addition, we consider several applications of invariant inference and explore
# % how {{{tool}}}'s invariants support those applications.
# % These considerations lead to the following research questions:

# To evaluate {{{tool}}} we consider 6 research questions:
# \begin{itemize}
#   \item\textbf{RQ1:} How well does {{{tool}}} infer nonlinear invariants describing complex program semantics and correctness conditions?
#   \item\textbf{RQ2:} How well does {{{tool}}} generate expressive invariants to capture program runtime complexity?
#   \item\textbf{RQ3:} How well does {{{tool}}} infer min and max-plus invariants to prove disjunctive invariants?
#   \item\textbf{RQ4:} How does {{{tool}}} perform on programs involving non-trivial properties not directly supported by {{{tool}}}?
#   \item\textbf{RQ5:} How does the depth of symbolic states influence the ability of {{{tool}}} to infer invariants?
#   \item\textbf{RQ6:} How does {{{tool}}} compare to other invariant generation tools?
# \end{itemize}

# To investigate these questions, we used 4 benchmark suites consist of 108 Java programs described in detail in the following subsections.
# These programs come with known or documented invariants.
# To compare the invariants inferred by {{{tool}}}, we manually checked consistency with 
# the documented invariants and we encoded documented invariants and used Z3 to determine that the inferred results imply them.

# For our experiments, we use {\tool}'s default settings to infer nonlinear equalities, octagonal and max/min relations.
# For equalities, {{{tool}}} uses the default setting DIG~\cite{nguyen2014dig} that limits the number of generated terms up to 200.
# This allows us, for example, to infer equalities up to degree 5 for a program with 4 variables and up to degree 2 for a program with 12 variables).
# For octagonal and max/min inequalities, we consider upper and lower bounds (the \texttt{v\_max} value in Figure~\ref{fig:max}) within the range $[-20,20]$; we rarely observe inequalities with large bounds.
# {{{tool}}} can either choose random values in a range, $[-300,300]$ by default, for bootstrapping, or use the algorithm in Figure~\ref{fig:getConcreteStates}.
# Our experience shows that we do not need very large input values to generate precise invariants.
# We start SPF with depth 7 and CIVL with depth 20; these seem to be good default search depths for almost all our test programs.
# All these parameters can be changed by {{{tool}}}'s user; we chose these default values based on our experience.

# % \textbf{Configurability:} {{{tool}}} currently supports equality and inequality relations over nonlinear numerical variables as described in \S\ref{sec:dig}.


# % We generate numerical invariants of two forms: nonlinear poly- nomial equations and octagonal inequalities. For octagonal invari- ants, NumInv by default considers the bounds within the range [−10, 10]. For equalities, NumInv by default sets a single param- eter α = 200 so that it can generate invariants without a priori knowledge of specific degrees. NumInv automatically adjusts the maximum degree so that the number of generated terms does not exceed α . For example, NumInv considers equalities up to degree 5 for a program with four variables and equalities up to degree 2 for a program with twelve variables. We acknowledge that inferring these parameter constants robustly and automatically is important future work. These constants can be chosen by the NumInv user; we chose values based on our experience. Note that the divide and conquer approach to inferring inequalities in Figure 4 is quite useful if the user decides to increase the bounds; for range [−10, 10] the number of iterations is log(20) ≈ 5 rather than 20 (if we use a brute force algorithm) but for range [−100, 100] it is log(200) ≈ 8, not 200 (using brute force).

# % SymInfer currently supports equality and inequality re- lations over numerical variables. For (nonlinear) equalities, SymInfer uses techniques from DIG to limit the number of generated terms. This allows us, for example, to infer equalities up to degree 5 for a program with 4 variables and up to degree 2 for program with 12 variables. For octagonal invariants, we consider upper and lower bounds within the range [−10,10]; we rarely observe inequalities with large bounds. SymInfer can either choose random values in a range, [−300,300] by default, for bootstrapping, or use the algorithm in Figure 5. All these parameters can be changed by SymInfer’s user; we chose these values based on our experience.

# {{{tool}}} has several sources of randomness, e.g., the generation of concrete states from the Z3 and the collection of symbolic states symbolic execution tools.
# In our experiments, we ran {{{tool}}} 5 times on each program and report the median results (e.g., the median results of the runtimes and number of invariants collected over 5 runs).

# The experiments reported were run on a 64-core AMD CPU 4 GHZ Linux system with 64 GB of RAM.
# {{{tool}}} and  all experimental benchmarks and results are available at \url{https://github.com/unsat/dig/}.

# \subsection{RQ1: Programs With Nonlinear Invariants}\label{sec:rq1}

# In this experiment, we use programs from the NLA testsuite~\cite{nguyen2014dig} in the SV-COMP benchmark~\cite{beyer:tacas17:svcomp}.
# This testsuite consists of 28 programs implementing mathematical functions such as \texttt{intdiv}, \texttt{gcd}, \texttt{lcm}, and \texttt{power sum}.
# Although these programs are relatively small (under 50 LoCs), they contain nontrivial structures such as nested loops and nonlinear invariant properties.
# To the best of our knowledge, NLA is the largest benchmark of programs containing nonlinear arithmetic.
# Many of these programs have also been used to evaluate other numerical invariant systems~\cite{rodriguez2007generating,sharma2013data,nguyen2017syminfer,le2020dynamite}.

# These programs come with known program invariants at various program locations (e.g., mostly nonlinear equalities for loop invariants).
# For this experiment, we evaluate {{{tool}}} by finding invariants at these locations and comparing them with known invariants.

# \begin{table}
#   \centering
#   \caption{Experimental results for NLA programs. \checkmark: produce results that match or imply documented invariants. $^*$: require minor modifications to work.}\label{tab:nla}
#   \begin{tabular}{@{}l@{ }c@{ \;}r@{ }|rcc@{ }|rr|@{}c@{}}
#     &&&\multicolumn{3}{c|}{\textbf{Invs}}  & \multicolumn{2}{c|}{\textbf{Time(s)}}\\
#     \textbf{Prog} & \textbf{L} & \textbf{V} & \textbf{T} &
#     \textbf{E, I, M} & \textbf{NL(d)} & \textbf{T} & \textbf{Exp}& \textbf{Correct}\\
#     \midrule    
#     Bresenham   & 1 &  5 &  5 &  1,2,2  & 1(2) &  78.5 &  64.5 S & \checkmark\\
#     CohenCu     & 1 &  5 &  6 &  3,2,1  & 2(2) &  16.0 &  12.5 E & \checkmark\\
#     CohenDiv    & 2 &  6 & 20 & 4,15,1  & 4(2) &  84.7 &  61.9 E & \checkmark\\
#     Dijkstra    & 2 &  5 & 19 & 7,10,2  & 4(3) &  98.2 &  76.4 E & \checkmark\\
#     DivBin      & 2 &  5 & 15 &  3,8,4  & 1(2) &  47.3 &  32.9 S & \checkmark\\
#     Egcd        & 1 &  8 & 11 &  3,8,0  & 3(2) & 104.7 &  67.6 S & \checkmark\\
#     Egcd2       & 2 & 10 & 60 & 5,25,30 & 5(2) & 165.2 &  71.7 R & \checkmark\\
#     Egcd3       & 3 & 12 & 92 & 9,42,41 & 9(2) & 406.0 & 220.4 R & \checkmark\\
#     Fermat1     & 3 &  5 & 27 & 3,8,16  & 3(2) & 241.8 & 110.5 M & \checkmark\\
#     Fermat2     & 1 &  5 &  9 &  1,2,6  & 1(2) & 310.3 & 190.2 M & \checkmark\\
#     Freire1$^*$ & 1 &  3 &  4 &  1,1,2  & 1(2) &   3.8 &   1.8 E & \checkmark\\
#     Freire2$^*$ & 2 &  4 &  4 &  4,0,0  & 4(2) &   6.4 &   3.0 E & \checkmark\\
#     Geo1        & 1 &  4 &  7 &  1,6,0  & 1(2) &  11.1 &   4.3 M & \checkmark\\
#     Geo2        & 1 &  4 &  8 &  1,6,1  & 1(2) &  15.4 &   4.3 M & \checkmark\\
#     Geo3        & 1 &  5 & 10 &  1,7,2  & 1(3) &  53.2 &  34.4 E & \checkmark\\
#     Hard        & 2 &  6 & 21 & 5,11,5  & 3(2) &  63.8 &  39.5 S & \checkmark\\
#     Knuth$^*$   & 1 &  8 & 13 &  4,5,4  & 4(3) & 197.7 &  82.6 M & \checkmark\\
#     Lcm1        & 3 &  6 & 32 & 4,21,7  & 4(2) &  96.8 &  44.2 S & \checkmark\\
#     Lcm2        & 1 &  6 &  9 &  1,6,2  & 1(2) &  96.5 &  73.3 S & \checkmark\\
#     MannaDiv    & 1 &  5 &  7 &  1,6,0  & 1(2) & 202.9 & 195.5 E & \checkmark\\
#     Prod4br     & 1 &  6 &  9 &  1,6,2  & 1(3) &  69.6 &  31.4 E & \checkmark\\
#     ProdBin     & 1 &  5 &  7 &  1,6,0  & 1(2) &  75.3 &  35.4 S & \checkmark\\
#     Ps2         & 1 &  3 &  4 &  1,3,0  & 1(2) &   3.4 &   1.5 E & \checkmark\\
#     Ps3         & 1 &  3 &  4 &  1,3,0  & 1(3) &   3.4 &   1.5 E & \checkmark\\
#     Ps4         & 1 &  3 &  4 &  1,3,0  & 1(4) &   3.3 &   1.4 E & \checkmark\\
#     Ps5         & 1 &  3 &  4 &  1,3,0  & 1(5) &   3.7 &   1.7 E & \checkmark\\
#     Ps6         & 1 &  3 &  4 &  1,3,0  & 1(6) &   3.9 &   1.8 E & \checkmark\\
#     Sqrt1       & 1 &  4 &  6 &  2,4,0  & 1(2) &   4.6 &   1.8 E & \checkmark\\
#     \bottomrule
#   \end{tabular}
# \end{table}

# \subsubsection*{Results}

# Table~\ref{tab:nla} presents the results of {{{tool}}} for the 28 NLA programs.
# Columns \textbf{L} and \textbf{V} show the number of locations where we obtain invariants and the number of variables at the location that has the largest number of variables, respectively.
# The \textbf{Invs} group shows the total number of discovered invariants (\textbf{T}) and from those the number of equalities (\textbf{E}), octagonal inequalities (\textbf{I}), min and max-plus inequalities (\textbf{M}), and nonlinear \textbf{NL} (equality) invariants and the highest degree (\textbf{D}) among those.
# The \textbf{Time} group shows the total time (\textbf{T}) in seconds.
# This time includes subtasks such as 
# symbolic execution (\textit{S}), equation solving (\textit{E}), upper bound computation (\textit{M}), and removing redundant results (\textit{R}).
# The \textbf{Exp} column lists the time for the most expensive sub-task and indicates
# that task.
# Column \textbf{Correct} shows if the obtained results match or imply the known invariants.
# We also modified three programs (indicated with $^*$) as they contain external calls and floating-point values that {{{tool}}} currently does not support (details given below).

# For all 28 programs, {{{tool}}} generated correct invariants that match or imply the known results.
# In most cases, the discovered invariants matched the known ones exactly.
# Occasionally, we obtained results that are equivalent or imply the known results.
# For example, for some runs of \texttt{Sqrt1} we found the documented equalities $t=2a+1$ and $s=(a+1)^2$, and for other runs we obtained $t=2a+1$ and $t^2 - 4s + 2t = -1$, which are equivalent to $s=(a+1)^2$ by replacing $t$ with $2a+1$.

# {{{tool}}} also discovered undocumented invariants.
# For example, for \texttt{Egcd1}, which implements an extended GCD algorithm, DIG identifies three equalities for loop invariants: $x=ai+bj,y=ak+bm$, and $1=im - jk$. The first two are documented invariants that assert the computation and preservation of the  B\'{e}zout identity in the loop\footnote{An extended GCD algorithm takes as input a pair of integers $(a,b)$ and, in addition to computing the $\gcd$ of $a,b$, also produces two integers $i,j$ satisfying the B\'{e}zout identity $x=ai+bj$}. The third relation is a valid, but undocumented invariant, revealing a potentially useful implementation detail: the product $im$ is exactly 1 more than the product $jk$ whenever the program reaches location $L$.
# Also, as shown in~\S\ref{sec:applications}, for \texttt{CohenDiv} {{{tool}}} generated undocumented but useful inequalities such as $r \ge 0$, $r \le x$, and $r\le y -1$ which state that the remain $r$ is non-negative, is at most the dividend $x$, but is strictly less than the divisor $y$.
# Our experience shows that {{{tool}}} is capable of generating many invariants that are unexpected yet correct and useful.

# We had to modify three programs \texttt{Freire1}, \texttt{Freire2}, \texttt{Knuth} to work with {{{tool}}}.
# We changed the floating point values used in \texttt{Freire1, Freire2} to integers because 
# {{{tool}}} currently does not support floating point arithmetic.
# We also remove the external library \texttt{sqrt} call from \texttt{Knuth} (by replacing \texttt{x == Math.sqrt(y)} with \texttt{x*x == y}) because SPF cannot obtain symbolic states from unknown functions.


# From Table~\ref{tab:nla}, we see that the number of invariants, especially inequalities, obtained at a location is largely dependent on the number of variables (i.e., the generated terms over these variables).
# Programs such as \texttt{Egcd2}, \texttt{Egcd3}, \texttt{LCM1} have more invariants because multiple locations are considered and also each location contains many variables.
# Nonetheless, we consider the final number of invariants reasonable, especially the stronger nonlinear equalities, and thus can be directly presented and analyzed by the user.

# The runtime of {{{tool}}} is largely dependent on the number of variables.  While generally taking less than 2 minutes, for some programs
# the inference process can take a 5-7 minutes,
# e.g., \texttt{Fermat1} is the slowest because it infers invariants at 3 locations and \texttt{Egcd3} searches for invariant relations over 12 variables.
# Also, the most expensive subtasks vary across the programs: symbolic execution dominate for some programs (e.g., \texttt{Bresenham, Egcd}) while computing upper bounds is expensive for others (e.g., \texttt{Fermat1, Knuth}) because these programs involve many variables and thus generate many inequality candidate relations. 

# Note that our experiments use the typical \emph{wall-clock} time to measure the time elapsed between the start and end of the program (by simply storing the start and end time values and obtaining their difference).
# If we instead use \emph{user-cpu} time, which measures the CPU usage of a program, the time would be larger because {\tool} exploits parallelism.
# For example, for a run of \texttt{CohenDiv}, the user time is 370.51s but the wall-clock time is 68.48s.
# Multiprocessing can help program analysis scale, and {\tool} leverages this ability on increasingly available and affordable multicore computers.



# \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
#   \textbf{RQ1}: {{{tool}}} was able to discover complex and precise nonlinear invariants to describe the semantics and correctness properties of 28/28 programs from the SV-COMP NLA benchmark. In many cases, {{{tool}}} found undocumented but useful invariants revealing additional facts about program semantics.
# \end{tcolorbox}

# \subsection{RQ2: Analyzing Computational Complexity}\label{sec:rq2}

# \begin{table}
#   \caption{{{{tool}}}'s results for computing programs' complexities. \checkmark: generates the expected bounds.
#     \checkmark \checkmark: obtains more precise bounds than reported results.
#     $^*$: require minor modifications.}\label{tab:complexity}
#   \footnotesize
#   \centering
#   \begin{tabular}{lc|ccc|r|c}
#     \textbf{Prog}& \textbf{V} &\textbf{T} & \textbf{E,I,M} & \textbf{NL(D)} & \textbf{Time(s)} & \textbf{Correct}\\
#     \midrule
#     cav09\_fig1a    & 2 & 1  & 1,0,0  & 1(2) &  5.7 &      \checkmark      \\
#     cav09\_fig1d    & 2 & 1  & 1,0,0  & 1(2) &  5.8 &      \checkmark      \\
#     cav09\_fig2d    & 3 & 4  & 1,3,0  & 1(2) & 23.8 &      \checkmark      \\
#     cav09\_fig3a    & 2 & 3  & 1,1,1  & 1(2) &  3.5 &      \checkmark      \\
#     cav09\_fig5b    & 5 & 7  & 2,4,1  & 1(2) & 10.0 &      \checkmark      \\
#     pldi09\_ex6     & 4 & 9  & 5,1,3  & 4(3) &  9.1 &      \checkmark      \\
#     pldi09\_fig2    & 4 & 6  & 2,4,0  & 2(4) & 43.3 & \checkmark\checkmark \\
#     pldi09\_fig4\_1 & 3 & 7  & 1,2,4  & 0(1) & 13.5 &      \checkmark      \\
#     pldi09\_fig4\_2 & 5 & 13 & 2,4,7  & 1(2) & 14.4 &      \checkmark      \\
#     pldi09\_fig4\_3 & 3 & 3  & 1,2,0  & 1(2) & 35.1 &      \checkmark      \\
#     pldi09\_fig4\_4$^*$ & 4 & 6  & 1,3,2  & 1(2) & 18.1 &   $\circ$      \\
#     pldi09\_fig4\_5 & 3 & 3  & 1,2,0  & 1(2) & 26.5 &      \checkmark    \\
#     popl09\_fig2\_1 & 5 & 2  & 1,1,0  & 1(3) & 50.6 &      \checkmark$^*$      \\
#     popl09\_fig2\_2 & 4 & 2  & 1,1,0  & 1(3) & 70.7 & \checkmark\checkmark \\
#     popl09\_fig3\_4 & 3 & 5  & 3,1,1  & 3(4) & 44.0 &      \checkmark      \\
#     popl09\_fig4\_1 & 3 & 3  & 1,2,0  & 1(3) & 134.1 & \checkmark  \\
#     popl09\_fig4\_2 & 5 & 2  & 1,1,0  & 1(3) & 51.1 &      \checkmark$^*$      \\
#     popl09\_fig4\_3 & 5 & 19 & 3,1,15 & 1(2) & 16.7 &      \checkmark      \\
#     popl09\_fig4\_4 & 3 & 4  & 1,3,0  & 1(2) & 10.0 &      \checkmark      \\
#     \bottomrule
#   \end{tabular}  
# \end{table}


# % results for C programs
# % \begin{table}
# %   \caption{Experimental results for computing programs' complexities. \checkmark: {{{tool}}} generates the expected bounds.
# %   \checkmark$^*$: program was slightly modified to assist the analysis.
# %   \checkmark \checkmark: {{{tool}}} obtains more precise bounds than reported results.}
# %   \footnotesize
# %   \centering
# %   \begin{tabular}{l|ccr|c}
#       %       \textbf{Prog}  & \textbf{V,T,D}     & \textbf{Invs} & \textbf{Time(s)} &\textbf{Bound}\\
#       %       \midrule
#       %       cav09\_fig1a	   & 2,5,2  &   3 & 13.94  & \checkmark            \\
#       %       cav09\_fig1d     & 2,5,2  &   3 & 11.96  & \checkmark            \\
#       %       cav09\_fig2d     & 3,2,2  &   3 & 40.89  & \checkmark            \\
#       %       cav09\_fig3a     & 2,2,2  &   3 & 9.95   & \checkmark            \\
#       %       cav09\_fig5b     & 5,4,2  &   6 & 93.67  & \checkmark            \\
#       %       pldi09\_ex6      & 4,9,3  &   7 & 65.18  & \checkmark            \\
#       %       pldi09\_fig2     & 4,15,4 &   6 & 67.77  & \checkmark\checkmark  \\
#       %       pldi09\_fig4\_1  & 3,3,1  &   3 & 37.86  & \checkmark            \\
#       %       pldi09\_fig4\_2  & 5,4,2  &   5 & 40.13  & \checkmark            \\
#       %       pldi09\_fig4\_3  & 3,3,2  &   3 & 39.67  & \checkmark            \\
#       %       pldi09\_fig4\_4  & 4,4,2  &   1 & 110.40 & \checkmark            \\%todo: bug, crash;
#       %       pldi09\_fig4\_5$^*$  & 3,4,2  &   3 & 159.94 & \checkmark        \\
#       %       popl09\_fig2\_1  & 5,12,3 &   2 & 46.85  & \checkmark            \\
#       %       popl09\_fig2\_2  & 4,9,3  &   2 & 43.22  & \checkmark\checkmark  \\
#       %       popl09\_fig3\_4  & 3,4,3  &   4 & 186.38 & \checkmark            \\
#       %       popl09\_fig4\_1$^*$  & 3,3,2  &   5 & 209.70 & \checkmark        \\
#       %       popl09\_fig4\_2  & 5,12,3 &   2 & 56.55  & \checkmark            \\
#       %       popl09\_fig4\_3  & 5,3,2  &   5 & 25.20  & \checkmark            \\
#       %       popl09\_fig4\_4  & 3,3,2  &   3 & 44.86  & \checkmark            \\
#       %       \bottomrule
#       %     \end{tabular}
#       %       \label{tab:complexity}
#       %       \end{table}


# As shown in \S\ref{sec:applications}, nonlinear invariants can represent precise program runtime complexity.
# More specifically, the roots of nonlinear relationships yield obtain disjunctive information that capture precise program complexity bounds.

# To further evaluate {{{tool}}} for discovering program complexity, we collect 19 programs, adapted\footnote{We remove nondeterministic features in these programs because {{{tool}}} assumes deterministic behavior.} from existing static analysis techniques specifically designed to find runtime complexity~\cite{gulwani2009control,gulwani2009speed,gulwani2009speed-cav}.
# These programs, shown in Table~\ref{tab:complexity}, are small, but contain nontrivial structures and represent examples from Microsoft's production code~\cite{gulwani2009control}.
# For this experiment, we instrument each program with a fresh variable $t$ representing the number of loop iterations and generate postconditions over $t$ and input variables (e.g., see Figure~\ref{fig:complexity}).
# %We then compare our complexity results with those reported from the respective work.

# \subsubsection*{Results}

# Table~\ref{tab:complexity}, which has a similar format as Table~\ref{tab:nla}, shows the results of {{{tool}}}. % from 11 runs.
# Column~\textbf{Correct} contains a \checkmark if {{{tool}}} generates invariants matching the bounds reported in the respective work\footnote{In Table~\ref{tab:complexity}, results for programs prefixed with \texttt{pldi09},  \texttt{popl09}, and \texttt{cav09} are from~\cite{gulwani2009control},~\cite{gulwani2009speed},~\cite{gulwani2009speed-cav}, respectively.}, and \checkmark \checkmark if the discovered invariants represent more precise bounds than the reported ones.%\footnote{Invariants are more precise when they characterize program behavior more accurately.}
# % Program marked with $^*$ were modified slightly to help our analysis---described below.

# For 18/19 programs, {{{tool}}} discovered runtime complexity characterizations
# that match or improve on reported results.
# For \texttt{cav09\_fig1a}, we found the invariant $mt - t^2 - 100m + 200t = 10000$, which indicates the correct bound $t = m + 100 \lor t = 100$.
# For these complexity analyses, we also see the important role of combining both inequality and equality relations to produce informative bounds.
# For \texttt{popl09\_fig3\_4}, {{{tool}}} inferred a disjunctive equality showing that $t=n\lor t=m$ and inequalities asserting that $t\ge n \land t \ge m$, together indicating that $t=\max(n,m)$, which is the correct bound for this program.
# For \texttt{pldi09\_fig4\_5}, we obtained nonlinear results giving two bounds $t=n-m$ and $t=m$, which establish the reported upper bound $t=\max(n-m,m)$.
# In two programs, {{{tool}}} obtained better bounds than reported results (marked with $\checkmark\checkmark$).
# The \texttt{tripple} program shown in Figure~\ref{fig:complexity} (\texttt{pldi\_fig2} in Table~\ref{tab:complexity}) is a concrete example where the three inferred bounds are strictly less than the previously best known bound.

# For \texttt{popl09\_fig2\_1} and \texttt{popl09\_fig4\_2} (marked with $\checkmark^*$), we obtained similar complexity bounds as the reported results.
# However, the reported results also give the preconditions leading to the bounds (thus more informative than ours), but also have some \emph{incorrect} bounds (our complexity results are correct).
# For example, in \texttt{popl09\_fig2\_1}, we got 3 bounds $t=m + n - a - b$, $t = n - a$, and $t=0$ (when we do not enter the loop).
# The reported results give 4 bounds, three of which are similar to ours and also include preconditions (e.g., $n>a \land m >b \implies t=n+m-a-b$  indicates that $t=n+m-a-b$ occurs when $n>a \land m >b$).
# However, one of these 4 reported bounds, $(a \ge n \land b < m)  \implies t = m-b$, is incorrect because under this condition the program does not enter the loop and thus has $t=0$ instead of $t = m-b$ ($m-b$ is positive because of the condition $b<m$).


# We were not able to obtain sufficiently strong invariants to show the reported bound for  \texttt{pldi09\_fig4\_4}.
# However, if we create a new term representing the quotient of an integer division of two other variables in the program, and obtain invariants over that term, we obtain more precise bounds than those reported in~\cite{gulwani2009control}.

#       %       \subsubsection{Results for C programs}
#       %       Benefits: can be used for multiple languages (just need a symbolic execution for programs in that lang).  For C, use CIVL.  The only diff is obtaining symbolic states using CIVL, after that use the exact same algos.  Some main diff:  CIVL doesn't have default depths (just use depth 20).  Takes a bit long to run (give concrete example, but not too much diff), both cannot handle certain kind of 3rd lib (sqrt).

# \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
#   \textbf{RQ2:} We demonstrate a rather surprising application of {{{tool}}}'s invariants.
#   {{{tool}}} was able to discover unexpected and difficult invariants capturing the precise complexity bounds of 18/19 programs.
#   In some cases, these results help reveal unknown or more informative complexity bounds.
# \end{tcolorbox}

# \subsection{RQ3: Disjunctive Invariant Results}\label{sec:rq3}

# In this experiment, we evaluate {{{tool}}}'s max/min invariants on benchmark programs used in existing disjunctive invariant analysis work~\cite{allamigeon2008inferring,nguyen2014using}.
# These programs, listed in Table~\ref{tab:minmax}, typically have many execution paths, e.g., the sorting method \texttt{oddeven5} contains 12 serial ``if'' blocks and thus $2^{12}$ paths.

# The documented correctness assertions for these programs require reasoning about disjunctive invariants,
# but do not involve higher-order logic. 
# For example, the sorting procedures are asserted to produce a sorted output, but are not asserted to produce a permutation of the input.
# Surprisingly, {{{tool}}} discovers undocumented nonlinear relations that represent such permutation properties.  

# \begin{table}
#   \caption{Disjunctive Invariant results. \checkmark: produce results that match or imply documented invariants. $^*$: require minor modifications.}  
#   \centering
#   \begin{tabular}{lcc|ccc|r|c}
#     \textbf{Prog}& \textbf{L} &\textbf{V} &\textbf{T} & \textbf{E,I,M} & \textbf{NL(D)} & \textbf{Time(s)} & \textbf{Correct}\\
#     \midrule
#     strncpy  & 1 & 3  & 4  & 0,2,2  & 0(1) &   6.6 & \checkmark \\
#     oddeven2 & 1 & 4  & 5  & 2,1,2  & 1(2) &   4.5 & \checkmark \\
#     oddeven3 & 1 & 6  & 7  & 3,2,2  & 2(3) &  10.3 & \checkmark \\
#     % oddeven4 & 1 & 8  & 9  & 3,3,3  & 2(3) &  77.7 & \checkmark \\
#     oddeven4$*$ & 1 & 8 & 10 & 4,3,3 & 3(4) & 92.4 & \checkmark  \\
#     oddeven5 & 1 & 10 & 39 & 2,2,35 & 1(2) & 207.0 & \checkmark \\
#     partd1   & 2 & 3  & 5  & 1,2,2  & 1(2) &  10.3 & \checkmark \\
#     partd2   & 2 & 4  & 5  & 1,2,2  & 1(3) &  58.1 & \checkmark \\
#     partd3   & 4 & 5  & 12 & 1,7,4  & 1(4) & 151.9 & \checkmark \\
#     partd4   & 5 & 6  & 16 & 0,11,5 & 0(1) & 158.5 & \checkmark \\
#     partd5   & 6 & 7  & 22 & 0,16,6 & 0(1) & 192.9 & \checkmark \\
#     parti1   & 2 & 3  & 5  & 1,2,2  & 1(2) &  10.2 & \checkmark \\
#     parti2   & 3 & 4  & 8  & 1,4,3  & 1(3) &  66.0 & \checkmark \\
#     parti3   & 4 & 5  & 12 & 1,7,4  & 1(4) & 182.0 & \checkmark \\
#     parti4   & 5 & 6  & 16 & 0,11,5 & 0(1) & 185.0 & \checkmark \\
#     parti5   & 6 & 7  & 22 & 0,16,6 & 0(1) & 218.0 & \checkmark \\
#     \bottomrule
#   \end{tabular}
#   \label{tab:minmax}
# \end{table}

# \subsubsection*{Results}
# Table~\ref{tab:minmax}, which has similar format as Table~\ref{tab:nla}, shows the experimental results.
# For all 15 programs, the discovered invariants are sufficiently strong to prove the correctness of these programs (i.e., they match or imply the documented assertions).


# For \texttt{strncpy}, which simulates the C \texttt{strncpy} function to copy the first
# $n$ characters from a (null-terminated) source $s$ to a
# destination $d$, {{{tool}}} inferred two min-plus invariants
# \[
#   \min(|s|, n) - |d| \le 0 ~,~ \min(|d|, n) - |s| \le 0,
# \]
# which represent the relation
# \[
#   (n \ge |s| ~\land~ |d|=|s|) ~\lor~ (n < |s| ~\land~ |d| \ge n)
# \]
# This captures the desired semantics of \texttt{strncpy}: if $n \ge |s|$, then the copy stops at the null terminator of $s$, which is also copied to $d$, so $d$ ends up with the same length as $s$.
# However, if $n < |s|$,  then the terminator is not copied to $d$, so $|d| \ge n$. 

# % OddEven2
# % x0 + x1 - y0 - y1 == 0
# % x1^2 - x1*y0 - x1*y1 + y0*y1 == 0
# % y0 - min(x0, x1) == 0
# % y1 - max(x0, x1) == 0

# % OddEven3
# % x0 + x1 + x2 - y0 - y1 - y2 == 0
# % x2^3 - x2^2*y0 - x2^2*y1 + x2*y0*y1 - x2^2*y2 + x2*y0*y2 + x2*y1*y2 - y0*y1*y2 == 0
# % x1^2 + x1*x2 + x2^2 - x1*y0 - x2*y0 - x1*y1 - x2*y1 + y0*y1 - x1*y2 - x2*y2 + y0*y2 + y1*y2 == 0
# % y2 - max(x0, x1, x2) == 0
# % min(x0, x1, x2) - y0 == 0

# % oddeven4
# % x0 + x1 + x2 + x3 - y0 - y1 - y2 - y3 == 0
# % 2. x1^2 + x1*x2 + x2^2 + x1*x3 + x2*x3 + x3^2 - x1*y0 - x2*y0 - x3*y0 - x1*y1 - x2*y1 - x3*y1 + y0*y1 - x1*y2 - x2*y2 - x3*y2 + y0*y2 + y1*y2 - x1*y3 - x2*y3 - x3*y3 + y0*y3 + y1*y3 + y2*y3 == 0
# % x2^3 + x2^2*x3 + x2*x3^2 + x3^3 - x2^2*y0 - x2*x3*y0 - x3^2*y0 - x2^2*y1 - x2*x3*y1 - x3^2*y1 + x2*y0*y1 + x3*y0*y1 - x2^2*y2 - x2*x3*y2 - x3^2*y2 + x2*y0*y2 + x3*y0*y2 + x2*y1*y2 + x3*y1*y2 - y0*y1*y2 - x2^2*y3 - x2*x3*y3 - x3^2*y3 + x2*y0*y3 + x3*y0*y3 + x2*y1*y3 + x3*y1*y3 - y0*y1*y3 + x2*y2*y3 + x3*y2*y3 - y0*y2*y3 - y1*y2*y3 == 0
# % y0 - min(x0, x1, x2, x3) == 0
# % max(x0, x1, x2, x3) - y3 == 0

# As a second example, for \texttt{oddeven}$_N$, which sorts the
# input elements $x_0,\dots,x_N$ and stores the results in $y_0,\dots,y_N$, {{{tool}}}'s inferred max/min invariants prove the outputs $y_0$ and $y_N$ hold the smallest and largest elements of
# the input, i.e., $y_0 = \texttt{min}(x_i)$ and $y_N = \texttt{max}(x_i)$.
# {{{tool}}}'s octagonal inequalities also show that the results are sorted, i.e., $y_0 \le y_1 \le \dots \le y_N$.
# These results are equivalent to the documented invariants and similar to those obtained using purely static analyses~\cite{allamigeon2008inferring}.


# As shown in Table~\ref{tab:minmax} {{{tool}}} also found nonlinear relations, even though the documented invariants do not contain any such properties.
# Similarly to the complexity example mentioned in \S\ref{sec:applications}, these nonlinear properties are rather unexpected and complicated, but capture surprisingly useful and interesting program information.
# For example, for \texttt{oddeven2}, {{{tool}}} found two equalities:
# \begin{align}
#   x_0 + x_1 - y_0 - y_1 = 0\label{oddeven2_a}\\
#   x_1^2 - x_1y_0 - x_1y_1 + y_0y_1 = 0\label{oddeven2_b}
# \end{align}
# The first linear equality shows that the sum of the inputs are the same as the outputs, which is true for sorting numbers.
# The second nonlinear inequality is undocumented, yet when combined with the first equation, yields useful information stating that the outputs $y's$ are permutations of the inputs $x's$.  To see this, first notice that the second inequality contains the disjunctive information that $x_1$ is either $y_1$ or 
# $y_0$:
# \[
#   (x_1^2 - x_1y_0 - x_1y_1 + y_0y_1 = 0)
#   ~\implies~ (x_1 - y_0) (x_1 - y_1) = 0 
# \]

# Next, combining these two cases $x_1 = y_1 \lor y_0$ with Eq~\ref{oddeven2_a} shows that the outputs $y_0,y_1$ are permutations of the inputs $x_0,x_1$, i.e., $x_1 = y_0  \implies x_0 = y_1$ and $x_1 = y_1 \implies x_0 = y_0$:
# \begin{gather*}
#   (x_1 = y_0) \land (y_0 + y_1 - x_0 - x_1 = 0) \implies(x_0 = y_1)\\
#   \text{and}\\
#   (x_1 = y_1) \land (y_0 + y_1 - x_0 - x_1 = 0) \implies(x_0 = y_0)\\
# \end{gather*}


# For other programs, we also derive permutation properties from the obtained invariants through the same reasoning. For example, for \texttt{OddEven3}, {{{tool}}} discovered three nonlinear equalities:
# \begin{align}
#   x_0 + x_1 + x_2 - y_0 - y_1 - y_2 = 0\label{oddeven3_a}\\
#   x_1^2 + x_1x_2 + x_2^2 - x_1y_0 - x_2y_0 - x_1y_1 - x_2y_1 + \label{oddeven3_b} \\
#   y_0y_1 - x_1y_2 - x_2y_2 + y_0y_2 + y_1y_2 = 0 \nonumber\\
#   x_2^3 - x_2^2y_0 - x_2^2y_1 + x_2y_0y_1 - x_2^2y_2 + x_2y_0y_2 +\label{oddeven3_c} \\
#   x_2y_1y_2 - y_0y_1y_2 = 0\nonumber\
# \end{align}

# As before, we first factor the highest-degree equality (Eq~\ref{oddeven3_c}) to obtain $(x_2 - y_0) (x_2 - y_1) (x_2 - y_2) = 0$, i.e., $x_2 = y_0 \lor y_1 \lor y_2$).  Then for each case we combine with the other equations to obtain all possible permutations (for 3 variables).  We illustrate the case when $x_2=y_0$, whose combination with Eq~\ref{oddeven3_b} shows that $x_1 = y_1 \lor x_1 = y_2$:

# \begin{align*}
#   (x_1^2 + x_1x_2 + x_2^2 - x_1y_0 - x_2y_0 - x_1y_1 - x_2y_1 + y_0y_1 \\
#   - x_1y_2 - x_2y_2 + y_0y_2 + y_1y_2 = 0) \land (x_2 = y_0) \\
#   \implies (x_1 - y_1) (x_1 - y_2) = 0
# \end{align*}

# These results are then combined with Eq~\ref{oddeven3_a} (e.g., when $x_2=y_0 \land x_1=y_2$, we have $x_0=y_1$) to derive permutation properties (e.g., $x_2=y_0 \land x_1 = y_2 \land x_0 = y_1$ is a permutation of three numbers):
# \begin{align*}
#   (x_0 + x_1 + x_2 - y_0 - y_1 - y_2 = 0) ~\land~ (x_2 = y_0 \land x_1=y_2)\\
#   \implies x_0 - y_1 = 0
# \end{align*}
# Thus by doing this for all cases, these nonlinear results reveal that the resulting outputs form the six permutations of the three inputs for \texttt{OddEven3}.
# % TODO: \khn{The eqt invs in partd123 and parti123 represent the post condition: the variable $i$ must equal one of the input variables.}

# While we obtained the documented invariants for all these benchmark programs with default settings in {{{tool}}},  we had to increase the number of terms (parameterized in {{{tool}}}) to find the undocumented nonlinear relations for \texttt{oddeven4} (indicated with $^*$) because these relations have degree 4, which would require using more {terms} as the program involves 8 variables (by default {{{tool}}} uses at most 200 terms).  

# {{{tool}}} failed to obtain the undocumented nonlinear invariants for some challenging
# problems due to equation solving timeout.
# For example, the permutation of \texttt{oddeven5} would require a nonlinear equation of degree 5, which would require 3003 terms over the 10 variables in the program.  Sage was not able to solve equations involving this many unknowns and thus {{{tool}}} cannot infer the nonlinear equations to represent permutations over 5 numbers.
# Despite this {{{tool}}} was able to infer other invariants about sortedness, largest, and smallest values, which match the documented invariants. A better, potentially external, equation solver might improve the scalability of {{{tool}}}.

# \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
#   \textbf{RQ3:} {{{tool}}} found sufficiently strong max/min and nonlinear invariants to establish the correctness of 15/15 programs requiring disjunctive invariants.
# We also discovered expressive undocumented nonlinear invariants that capture the higher-order permutation property of sorting algorithms.
# \end{tcolorbox}

# \subsection{RQ4: Checking Assertions}


#       %       We compare {{{tool}}} to CEGIR-based invariant tool PIE~\cite{padhi2016data}.
#       %       PIE aims to verify annotated relations by generating invariants based on the given assertions.
#       %       In contrast, {{{tool}}} generates invariants at given locations without given assertions or postconditions.


#       %       We use the HOLA benchmarks~\cite{dillig:oopsla13:abductiveinference}, adapted by the PIE developers.
#       %       These programs are annotated with various assertions representing loop invariants and postconditions.
#       % %       This benchmark consists of 49 programs, shown in Table~\ref{tab:hola},
#       %       This benchmark consists of 49 small programs, but contain nontrivial structures including nested loops or multiple sequential loops.
#       %       These programs,  shown in Table~\ref{tab:hola}, have been used as benchmarks for other static analysis techniques~\cite{beyer2007software,Gupta:2009:IEI:1575060.1575112,jeannet2010interproc}.
#       %       Note that HOLA programs only requires linear invariants and in general PIE does not work with program with nonlinear properties (in fact, as reported in ~\cite{}, PIE fails on every single NLA programs, i.e., either timeout or cannot generate any required nonlinear invariants).

#       %       For this experiment, we first run PIE and record its run time on proving the annotated assertions.
#       %       Next, we removed the assertions in the programs and asked {{{tool}}} to generate invariants at those locations.

# Several existing works generate invariants to verify given assertions or specifications.
# For example, to prove an assertion \lt{assert(p)} in a program, PIE~\cite{padhi2016data} computes an invariant \lt{p'} that is sufficiently strong to prove \lt{p}, i.e., $p' \implies p$. 
# In contrast, {{{tool}}} does not require given assertions to generate invariants (i.e., its goal is invariant discovery instead of finding invariants to prove specific goals).
# Nonetheless, we still can use {{{tool}}} to discover invariants and compare them to the given assertions (e.g., using the Z3 solver).

# %Unlike PIE and other static analysis works 
# %in PIE and other
# In this experiment, we evaluate {{{tool}}} on the 46 HOLA benchmark programs  used in several static analyses (e.g.,~\cite{beyer2007software,gupta2009invgen,jeannet2010interproc}). 
# Similarly to the NLA programs, these programs are small (less than 50 LoC), but contain nontrivial structures including nested loops or multiple sequential loops and are part of the program synthesis competition SyGuS~\cite{sygus}.
# These programs are annotated with various assertions representing loop invariants and postconditions.
# These assertions do not involve nonlinear properties, but involve various non-trivial relations such as inequalities that are not expressible using octagonal relations and disjunctions that are not expressible using max/min invariants.

# \subsubsection*{Results}
# \begin{table}
#   \caption{{{{tool}}}'s runs on HOLA benchmarks. \checkmark: produce sufficiently strong invariants to prove assertions. $\circ$: fail to make sufficiently strong invariants. $^*$: require minor modifications.}\label{tab:hola}
#   \centering
#   \begin{tabular}{lcc|ccc|r|c}
#     \textbf{Prog}& \textbf{L} &\textbf{V} &\textbf{T} & \textbf{E,I,M} & \textbf{NL(D)} & \textbf{Time(s)} & \textbf{Correct}\\
#     \midrule
#     H01     & 1 & 2 & 4  & 1,3,0  & 0(1) &   2.8 & \checkmark \\
#     H02     & 1 & 2 & 4  & 1,3,0  & 0(1) &   2.7 & \checkmark \\
#     H03     & 1 & 1 & 1  & 0,1,0  & 0(1) &  48.2 & \checkmark \\
#     H04     & 1 & 1 & 1  & 0,1,0  & 0(1) &   7.9 & \checkmark \\
#     H05     & 1 & 2 & 3  & 1,2,0  & 1(3) &   3.7 & \checkmark \\
#     H06     & 1 & 2 & 4  & 1,3,0  & 0(1) &  10.1 & \checkmark \\
#     H07     & 1 & 3 & 4  & 1,3,0  & 0(1) &   6.9 & \checkmark \\
#     H08     & 1 & 2 & 2  & 0,2,0  & 0(1) &  13.2 & \checkmark \\
#     H09     & 2 & 1 & 2  & 0,2,0  & 0(1) & 127.5 & \checkmark \\
#     H10     & 1 & 2 & 3  & 0,3,0  & 0(1) &   3.4 & \checkmark \\
#     H11     & 1 & 2 & 2  & 2,0,0  & 0(1) &  15.6 & \checkmark \\
#     H12     & 1 & 1 & 2  & 0,2,0  & 0(1) &   5.3 & \checkmark \\
#     H13     & 1 & 2 & 2  & 1,1,0  & 0(1) &   2.7 & \checkmark \\
#     H14     & 1 & 2 & 4  & 1,3,0  & 1(2) &   4.6 & \checkmark \\
#     H15     & 1 & 1 & 1  & 0,1,0  & 0(1) &   1.9 & \checkmark \\
#     H16     & 1 & 3 & 4  & 0,1,3  & 0(1) &   3.5 & \checkmark \\
#     H17     & 1 & 2 & 3  & 1,2,0  & 1(3) &   2.9 & \checkmark \\
#     H18     & 1 & 2 & 4  & 2,1,1  & 2(2) &   3.1 & \checkmark \\
#     H19     & 1 & 2 & 5  & 1,2,2  & 0(1) &   8.8 & \checkmark \\
#     H20     & 1 & 5 & 3  & 2,1,0  & 1(2) & 102.6 & \checkmark \\
#     H21     & 1 & 2 & 2  & 1,1,0  & 1(2) &   5.1 & \checkmark \\
#     H22     & 1 & 3 & 6  & 2,4,0  & 0(1) &   3.5 & \checkmark \\
#     H23     & 1 & 1 & 1  & 0,1,0  & 0(1) &   1.7 & \checkmark \\
#     H24     & 1 & 2 & 2  & 0,2,0  & 0(1) & 245.0 & \checkmark \\
#     H25     & 1 & 2 & 4  & 1,3,0  & 0(1) &  10.0 & \checkmark \\
#     H26     & 1 & 2 & 4  & 1,3,0  & 0(1) &  42.8 & \checkmark \\
#     H27     & 1 & 1 & 1  & 0,1,0  & 0(1) &  43.5 & \checkmark \\
#     H28     & 1 & 2 & 3  & 0,3,0  & 0(1) &   3.0 & \checkmark \\
#     H29     & 1 & 4 & 5  & 2,3,0  & 0(1) &  12.3 & \checkmark \\
#     H30     & 1 & 2 & 2  & 2,0,0  & 0(1) &  15.7 & \checkmark \\
#     H31     & 2 & 3 & 6  & 0,6,0  & 0(1) &  63.1 & \checkmark \\
#     H32$^*$ & 1 & 2 & 3  & 1,2,0  & 0(1) &   2.9 & \checkmark \\
#     H33     & 1 & 2 & 3  & 1,2,0  & 0(1) &  41.5 & \checkmark \\
#     H34     & 1 & 5 & 16 & 5,1,10 & 3(2) &  20.4 & \checkmark \\
#     H35     & 1 & 2 & 5  & 1,2,2  & 0(1) &   2.8 & \checkmark \\
#     H36     & 1 & 4 & 8  & 2,6,0  & 0(1) &  64.2 & \checkmark \\
#     H37     & 1 & 2 & 4  & 1,1,2  & 0(1) &   3.7 & \checkmark \\
#     H38     & 1 & 2 & 3  & 1,2,0  & 0(1) &   2.8 & \checkmark \\
#     H39     & 1 & 2 & 2  & 0,2,0  & 0(1) &  17.7 & \checkmark \\
#     H40     & 1 & 2 & 4  & 1,3,0  & 0(1) &   8.1 & \checkmark \\
#     H41     & 1 & 4 & 11 & 2,2,7  & 1(2) &   6.5 & \checkmark \\
#     H42     & 1 & 3 & 5  & 2,3,0  & 1(2) &   8.9 &  $\circ$   \\
#     H43     & 1 & 3 & 2  & 0,2,0  & 0(1) &   4.2 & \checkmark \\
#     H44     & 1 & 3 & 6  & 0,3,3  & 0(1) &   4.3 & \checkmark \\
#     H45     & 1 & 2 & 4  & 1,3,0  & 0(1) &  53.4 & \checkmark \\
#     H46     & 1 & 1 & 2  & 0,2,0  & 0(1) &   2.7 & \checkmark \\
#     \bottomrule
#   \end{tabular}
# \end{table}

# Table~\ref{tab:hola} shows the results.
# Column \textbf{Correct} shows whether {{{tool}}}'s generated invariants match or imply the annotated assertions. % and therefore prove these assertions. avoid using prove
#       %       The ``-'' symbol indicates that {{{tool}}} fails to generate invariants and a \checkmark indicates that the generated invariants match or imply the assertions.
#       %       A $\circ$ indicates that the generated invariants are not sufficiently strong to prove the assertions.

# For 45/46 programs, {{{tool}}} discovered invariants are sufficiently strong to show the assertions.
# In most of these cases, we obtained correct and stronger invariants than the given assertions.
# For example, for H23, {{{tool}}} inferred the invariants $i = n, n^2 - n - 2s = 0,$ and $ -i \le n$, which imply the postcondition $s \ge 0$.
# For H29, we obtained the invariants $b + 1 = c, a + 1 = d, a + b \le 2,$ and $2 \le a$, which imply the given postcondition $a+c = b+d$.


# On one hand, this is expected because these assertions just involve linear properties and {{{tool}}} has been shown to work with programs with much harder invariants.
# On the other hand, {{{tool}}} was able to find undocumented nonlinear invariants, whose combinations with other invariants allow Z3 to establish assertions under forms that are \emph{not} supported by {{{tool}}}.
#       %       also found invariants that are precise enough to establish assertions under forms that are \emph{not} supported by {{{tool}}}.
# For example, H08 contains a postcondition $x<4\ \lor\ y>2$, which has a disjunctive form of strict inequalities.
# {{{tool}}} did not produce this invariant, but instead produced a correct and stronger relation $x \le y$, which implies this condition.
# Nonlinear invariants also allow us to check the assertions involving conditional information such as \lt{if(c) assert (p);} where the property $p$ only holds when the condition $c$ holds.
# For example, for H18, we obtained  the nonlinear relations $j^2 - 100j = 0$ and $fj = 100f$, which imply $j=0 \lor j=100$ and thus the annotated conditional assertion $f \ne 0 \implies j=100$.


# %Many HOLA programs contain disjunctive (or conditional) properties,


# We were not able to generate sufficiently strong invariants to establish the assertion $a \equiv 1 \mod 2$ in H42 because this property cannot be expressed using {{{tool}}}'s supported invariants or combination with nonlinear invariants.
# Note that for H32, the path condition returned by SPF has a strange form (many nested parentheses) that crashes the Python AST parser, and thus we manually remove some parentheses from this condition. 

# \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
#   \textbf{RQ4:} {{{tool}}} was able to generate invariants that together establish the assertions in 45/46 HOLA programs. In many cases, {{{tool}}} inferred correct and stronger invariants that prove asserted properties that are expressed in a form that is not directly supported by {{{tool}}} (e.g., strict inequalities).
# \end{tcolorbox}

# \subsection{RQ5: Using Symbolic States}\label{sec:rq5}

# \begin{table}
#   \centering
#   \caption{Symbolic states at different depths. \textbf{check}: checking candidate invariants (invalid (\textbf{S}AT), valid (\textbf{U}NSAT), unknown (\textbf{?})) and \textbf{max}: optaining the upper bound values of terms (found (\textbf{S}AT), unknown (\textbf{?})).} 
#   \label{tab:symstates}
#   \begin{tabular}{lr|cc}
#     &&\multicolumn{2}{c}{\textbf{solver}}\\
#     \textbf{Prog}&\textbf{T(s)} &\textbf{check S/U(?)}& \textbf{max S(?)} \\
#     \midrule    
#     Bresenham   & 64.5 & 242,192         & 170        \\
#     CohenCu     &  1.1 & 228,230 (1)     & 182        \\
#     CohenDiv    & 15.8 & 572,1771 (10)   & 1893       \\
#     Dijkstra    &  1.0 & 194,140 (21)    & 117        \\
#     DivBin      & 32.9 & 385,804         & 788        \\
#     Egcd        & 67.6 & 1053,3102       & 2705       \\
#     Egcd2       & 38.5 & 176,511 (1)     & 123        \\
#     Egcd3       & 38.7 & 192,368 (5)     & 56         \\
#     Fermat1     & 44.1 & 567,1153 (59)   & 629 (75)   \\
#     Fermat2     & 48.3 & 200,425 (23)    & 213 (27)   \\
#     Freire1     &  1.3 & 48,55           & 45         \\
#     Freire2     &  0.9 & 0,968           & 0          \\
#     Geo1        &  0.9 & 85,210          & 187 (2)    \\
#     Geo2        &  0.9 & 85,201          & 184 (3)    \\
#     Geo3        &  0.9 & 192,157 (1)     & 150        \\
#     Hard        & 39.5 & 701,1253 (3)    & 1436       \\
#     Knuth       & 36.3 & 1053,2922 (375) & 1218 (135) \\
#     Lcm1        & 44.2 & 1050,2326 (2)   & 2410       \\
#     Lcm2        & 73.3 & 330,788         & 852        \\
#     MannaDiv    &  4.1 & 192,514 (20)    & 354        \\
#     Prod4br     & 31.0 & 377,540 (5)     & 598        \\
#     ProdBin     & 35.4 & 196,398 (5)     & 399 (1)    \\
#     Ps2         &  1.0 & 35,131          & 110        \\
#     Ps3         &  1.0 & 35,120          & 110        \\
#     Ps4         &  1.0 & 35,120          & 110        \\
#     Ps5         &  1.0 & 35,120          & 110        \\
#     Ps6         &  1.0 & 35,120          & 110        \\
#     Sqrt1       &  0.9 & 77,307          & 285        \\
#     \bottomrule
#   \end{tabular}
# \end{table}

# \begin{figure*}[h!]
#   \begin{tikzpicture}[scale=1.0]
#     \begin{axis}[   name=ax1,
#       ylabel={\small \# kept invs (check)},
#       xlabel={\small depth},
#       ymode=log,
#       legend style={font=\footnotesize, at={(-0.0,-0.20)},anchor=north west,legend columns=9},
#       ]

#       % Bresenham
#       \addplot[solid,color=darkgray,mark=*,mark options={solid,scale=0.75}] coordinates {(0,279) (7,39) (10,38) (11,37) };
#       % CohenCu
#       \addplot[solid,color=blue,mark=square,mark options={solid,scale=0.75}] coordinates {(0,274) (7,47) (8,46) };
#       % CohenDiv
#       \addplot[solid,color=blue,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,896) (7,412) (8,378) (9,361) (10,352) (11,339) (13,154) };
#       % Dijkstra
#       \addplot[solid,color=red,mark=triangle*,mark options={solid,scale=0.75}] coordinates {(0,186) (7,74) (8,64) (9,53) };
#       % DivBin
#       \addplot[densely dashed,color=red,mark=*,mark options={solid,scale=0.75}] coordinates {(0,537) (7,177) (8,167) (9,166) (10,164) (11,158) };
#       % Egcd
#       \addplot[solid,color=olive,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,1605) (7,720) (9,663) (11,641) (13,563) (15,559) };
#       % Egcd2
#       \addplot[solid,color=red,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,237) (7,127) (8,124) (10,122) (11,114) };
#       % Egcd3
#       \addplot[densely dashed,color=violet,mark=*,mark options={solid,scale=0.75}] coordinates {(0,237) (7,114) (8,100) (9,90) (10,54) (11,53) };
#       % Fermat1
#       \addplot[solid,color=violet,mark=square*,mark options={solid,scale=0.75}] coordinates {(0,774) (7,238) (8,232) (9,229) (10,209) (11,207) };
#       % Fermat2
#       \addplot[densely dashed,color=darkgray,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,278) (7,86) (8,83) (9,80) (11,78) };
#       % Freire1
#       \addplot[densely dashed,color=red,mark=triangle*,mark options={solid,scale=0.75}] coordinates {(0,59) (7,11) };
#       % Geo1
#       \addplot[densely dashed,color=olive,mark=square,mark options={solid,scale=0.75}] coordinates {(0,126) (7,46) (8,41) };
#       % Geo2
#       \addplot[densely dashed,color=red,mark=square,mark options={solid,scale=0.75}] coordinates {(0,125) (7,41) (8,40) };
#       % Geo3
#       \addplot[solid,color=olive,mark=*,mark options={solid,scale=0.75}] coordinates {(0,224) (7,31) };
#       % Hard
#       \addplot[solid,color=darkgray,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,946) (7,268) (8,261) (9,259) (10,257) (11,250) };
#       % Knuth
#       \addplot[densely dashed,color=olive,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,1659) (8,813) (11,684) (12,601) (23,599) };
#       % Lcm1
#       \addplot[densely dashed,color=olive,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,1500) (7,540) (8,487) (9,459) (10,456) (11,40) };
#       % Lcm2
#       \addplot[densely dashed,color=red,mark=square*,mark options={solid,scale=0.75}] coordinates {(0,490) (7,163) };
#       % MannaDiv
#       \addplot[densely dashed,color=blue,mark=triangle*,mark options={solid,scale=0.75}] coordinates {(0,288) (7,130) (8,122) (9,111) (10,105) (11,100) (12,97) (13,96) };
#       % Prod4br
#       \addplot[densely dashed,color=darkgray,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,481) (7,123) (8,120) (9,114) };
#       % ProdBin
#       \addplot[solid,color=violet,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,277) (7,81) };
#       % Ps2
#       \addplot[solid,color=red,mark=square*,mark options={solid,scale=0.75}] coordinates {(0,59) (7,28) (9,26) (10,25) (11,24) };
#       % Ps3
#       \addplot[solid,color=violet,mark=square,mark options={solid,scale=0.75}] coordinates {(0,59) (7,24) };
#       % Ps4
#       \addplot[densely dashed,color=red,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,59) (7,24) };
#       % Ps5
#       \addplot[solid,color=darkgray,mark=triangle*,mark options={solid,scale=0.75}] coordinates {(0,59) (7,24) };
#       % Ps6
#       \addplot[solid,color=violet,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,59) (7,24) };
#       % Sqrt1
#       \addplot[solid,color=darkgray,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,138) (7,62) (9,61) };

#       \legend{Bresenham, CohenCu, CohenDiv, Dijkstra, DivBin, Egcd, Egcd2, Egcd3, Fermat1, Fermat2, Freire1, Geo1, Geo2, Geo3, Hard, Knuth, Lcm1, Lcm2, MannaDiv, Prod4br, ProdBin, Ps2, Ps3, Ps4, Ps5, Ps6, Sqrt1}


#     \end{axis}
#     \begin{axis}[   name=ax2,
#       at={(ax1.south east)},
#       xshift=2.5cm,
#       ylabel={\small \# kept invs (max)},
#       xlabel={\small depth},
#       ymode=log,
#       ]


#       % CohenCu
#       \addplot[solid,color=blue,mark=square,mark options={solid,scale=0.75}] coordinates {(0,42) (7,35) };
#       % Fermat1
#       \addplot[solid,color=violet,mark=square*,mark options={solid,scale=0.75}] coordinates {(0,218) (7,141) (8,119) (9,100) (10,87) (11,77) (12,73) (13,68) };
#       % Fermat2
#       \addplot[densely dashed,color=darkgray,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,76) (7,48) (8,36) (9,32) (10,30) (11,28) };
#       % Geo1
#       \addplot[densely dashed,color=olive,mark=square,mark options={solid,scale=0.75}] coordinates {(0,40) (7,39) (8,37) (10,36) (11,35) };
#       % Geo2
#       \addplot[densely dashed,color=red,mark=square,mark options={solid,scale=0.75}] coordinates {(0,39) (7,38) (8,37) (10,35) (11,34) };
#       % Knuth
#       \addplot[densely dashed,color=olive,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,409) (8,281) (11,248) (12,227) (16,194) (17,183) (22,180) (23,155) };
#       % ProdBin
#       \addplot[solid,color=violet,mark=triangle,mark options={solid,scale=0.75}] coordinates {(0,81) (10,80) };
#       % Ps3
#       \addplot[solid,color=violet,mark=square,mark options={solid,scale=0.75}] coordinates {(0,22) (11,21) };
#       % Ps6
#       \addplot[solid,color=violet,mark=oplus,mark options={solid,scale=0.75}] coordinates {(0,22) (11,21) };
#     \end{axis}
#   \end{tikzpicture}
#   \caption{Candidate invariants removed over incremental symbolic depths. Depth 0 indicates purely dynamic, i.e., results have not been checked with symbolic states at any depth.  Depth 7 is the default and first depth that we check invariants.}
#   \label{fig:symbolic_depths}
# \end{figure*}

# A main novelty of {{{tool}}} is that it exploits the symbolic states computed by symbolic execution to improve invariant inference.
# Table~\ref{tab:symstates} reports the uses of symbolic states in {{{tool}}} from the NLA runs shown in Table~\ref{tab:nla}.
# Column {\textbf{T(s)}} shows the total time of executing symbolic execution over multiple depths to obtain symbolic states.
# The next two columns show the numbers of calls to Z3 to check invariants (\textbf{check}) and compute the upperbounds of terms (\textbf{max}).
# Column \textbf{check} reports the number of times Z3 disproves (\textbf{S}) or proves (\textbf{U}) candidate invariants, or returns unknowns (\textbf{?}).
# Column \textbf{max} reports the number of times Z3 returns upper bound values (\textbf{S}) or unknown (\textbf{?}) (Z3's optimization technique returns $\infty$ for terms having no bounds).

# \subsubsection*{Results}
# From these results, we see that {{{tool}}} invokes Z3 many times.
# This is due to two factors: (i) {{{tool}}} produces many octagonal and max/min-plus candidate invariants (e.g., every pair of terms produces eight candidate octagonal inequalities), and (ii) we analyze each candidate using symbolic states obtained at \emph{multiple} depths as described in \S\ref{sec:using_symbolic_states} (i.e., after proving a candidate using symbolic states at depth $k$, we check it again using symbolic states at depth $k+1$ and only stop when the candidate is either disproved or remains unchanged for 3 consecutive depths).

# We also see that Z3 returns more unknowns when computing upper bounds than checking candidate invariants (though for \texttt{knuth} the percentages of unknowns are approximately the same, 8\% for checking and 10\% for upper bound finding).
# This might be because Z3 is more optimized for finding satisfiability assignments than optimal assignments (especially for complex max/min-plus terms).

# Note that while being used frequently, constraint solving tasks do not take up too much time as shown in Table~\ref{tab:nla}.
# This is because Z3 is generally efficient for numerical reasoning, and {{{tool}}} exploits parallelism and performs these tasks simultaneously.


# Figure~\ref{fig:symbolic_depths} shows the effect of varying symbolic depth in {{{tool}}} from the above NLA runs. 
# For the  graph \textbf{check} on the left, the \texttt{y-axis} lists the number of the invariants remaining after being analyzed using the symbolic states at the depths given in the \texttt{x-axis}.  
# The invariants shown at depth ``zero'' are newly-generated invariants that have not been analyzed at any depth.
# {{{tool}}} analyzes an invariant starting at depth 7 (default) and increments the depth until it either disproves that invariant or makes no progress in 3 consecutive depths (\S\ref{sec:verify}).  

# These results show that for each program we generate a large number of invariants (i.e., depth 0) and disprove (and remove) many of them at the default depth 7.  
# Additional depths help remove a modest number of additional invalid results.
# Moreover, most programs do not require depths beyond 7 (\texttt{egcd} is an exception that requires up to depth 24 to stabilize its results).

# For the graph \textbf{max}  on the right, the invariants at depth ``zero'' represent newly-generated terms that we need to find upper bounds for and at depth $k$ are the number of remaining terms after obtaining the upper bound values using symbolic states at depth $k$.  
# As described in \S\ref{sec:optimize} {{{tool}}} drops a candidate term if we found that it has no upper bound or is larger than a parameterized threshold value (by default set to 20).
# Similar to \textbf{check}, {{{tool}}} increases the symbolic depths to find upper bounds for a term until the term is dropped or its value remains unchanged for 3 consecutive depths.
# Note that this graph does not show programs that have no changes (e.g., they start with $n$ terms and never drop any of them).


# Similarly to the \textbf{check} graph, we see that with additional depth inference converges quickly for most programs; the outlier \texttt{egcd} uses up to depth 24.
# However, different than the \textbf{check} graph, the upper bounds are relatively stable and do not change for most programs (those that are not shown).

# \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
# \textbf{RQ5:} This experiment shows the effectiveness of using depth-adaptive symbolic execution.
# First, symbolic states are important and effective in detecting spurious invariants, even at the shallow depth of 7.
# Second, additional depths further help remove invalid results in more difficult programs.
# Third,  {{{tool}}} automatically increments depth based on the structure of the program and the state of the invariant inference algorithm, and thus can accommodate programs requiring a range of symbolic depths.
# \end{tcolorbox}

# \subsection{RQ6: Comparing Invariant Inference Approaches}\label{sec:rq6}

# \paragraph*{\textbf{{\tool}'s Performance on Java and C programs}} While the previous experiments report {\tool}'s results on Java programs, {\tool} also supports C programs.
# {\tool} automatically invokes the CIVL symbolic execution frontend tool on C programs to obtain symbolic states and applies the same backend algorithms for invariant inference and checking.

# {{{tool}}}'s results are similar for C or Java programs.
# By default, CIVL appears to run faster than SPF on more complex programs and slower than SPF on easier ones.
# For example, for \texttt{Lcm2} CIVL only took 27.3s while SPF took 73.3s, and for \texttt{Ps2\text{--}6} CIVL took 3 seconds while SPF only took a second.
# Thus, for complex programs whose runtimes are dominated by symbolic execution (e.g., as shown in Figure~\ref{fig:symbolic_depths}), {\tool} runs faster on the C versions. For the mentioned \texttt{Lcm2} program, the analysis of the took 48.5s for the C version and 96.8s for the Java version, but both analyses yield the exact 9 resulting invariants.

# Thus, while different symbolic engine frontends produce different symbolic states and runtimes, the resulting invariant qualities are similar, showing the generality of using symbolic states.
# The modular design of {\tool} also makes it easy to add new front ends (e.g., to support another language or symbolic execution engine, we just override a few functions in {\tool} to invoke the new symbolic execution tool and parse its results).

# \paragraph*{\textbf{{\tool} Compared to Other Invariant Approaches}} We compare {{{tool}}} to two other invariant generation tools NumInv and G-CLN, the verification tool PIE, and the CHC solvers Eldarica and GSPACER.
# %Note that three tools work differently than SymInfer in that they infer invariants specifically to prove a given property (thus the quality of the generated invariants depend on the given properties).
# Table~\ref{tab:compare} summarizes the results on the benchmarks used in our evaluation (- indicates that we were not able to run the tool on this benchmark as discussed below).

# \begin{table}
#   \centering
#   \caption{Comparing {{{tool}}} to other tools. Note that G-CLN comes with 27 NLA programs.}\label{tab:compare}% and we failed get the tool to run on the missing one.
#   \begin{tabular}{lcccc}
#     \toprule
#     &NLA&COMPLE&DISJ&HOLA\\
#     \midrule
#     {{{tool}}} &\textbf{28/28}  &\textbf{18/19} &15/15 & \textbf{45/46}\\
#     NumInv  &26/28   &\textbf{18/19} &0/15  & \textbf{45/46}\\
#     G-CLN (w/cust.)  &26/27 &-  &-  & -\\
#     G-CLN (w/default)&5/27  &-  &-  & -\\
#     \midrule
#     PIE              &-     &-  &-   &38/46\\        
#     GSPACER         &-     &-  &-   &41/46\\  
#     Eldarica        &-     &-  &-   &46/46\\
#     \bottomrule
#   \end{tabular}
# \end{table}

# %TODO cannot run G-CLN on other benchmarks because it requires user given traces. The work does not mention anything about these traces (or any tweaks), e.g., how they were chosen.  And from our observation, is extremely sensitive to these traces and other parameters.

# \paragraph*{\emph{NumInv}} Our previous invariant work NumInv~\cite{nguyen2017counterexample} also relies on DIG's algorithms to infer numerical invariants, but calls the  KLEE symbolic execution tool~\cite{cadar2008klee} as a blackbox to check invariants.
# NumInv works with C programs and supports equalities of the form in Eq~\ref{eq:eqt} and octagonal inequalities of the form in Eq~\ref{eq:oct}.
# Thus, as shown in Table~\ref{tab:compare}, NumInv achieved similar results as {{{tool}}} for the NLA, COMPLE(XITY), and HOLA programs, whose correctness only rely on nonlinear equalities and linear inequalities.
# Note that {{{tool}}} was able to show the correctness of 2 more NLA programs than NumInv (28 vs. 26) because {{{tool}}} uses a better equation solver than the one used in NumInv, which timed out when solving large equations appeared in the two programs \texttt{Edgcd2} and \texttt{Egcd3}.

# NumInv does not support min and max invariants.
# Thus, while it was able to generate similar equality and inequality invariants as {{{tool}}} for the DISJ programs (e.g., nonlinear equations describing the permutation property of sorting algorithms shown in~\S\ref{sec:rq3}), it cannot generate any of the required max/min inequalities to capture the semantics of these programs (0/15 in Table~\ref{tab:compare}).  For example, NumInv cannot discover the min-plus invariants capturing the correctness property of \texttt{strncpy} and the max/min relations showing that  the first and the last output elements represent the smallest and largest elements of the inputs for the \texttt{oddeven}$_{N}$ sorting programs.

# It is difficult to directly compare the efficiency of  {{{tool}}} and NumInv. 
# On one hand, using symbolic states allows {{{tool}}} to reuse the results and directly compute inequalities as described in \S\ref{sec:rq5}.
# On the other hand, the LLVM-based KLEE symbolic engine used in NumInv runs much faster than the Java SPF tool used in {{{tool}}}.
# For example, for \texttt{Divbin}, {{{tool}}} took 32.9s just to run SPF to obtain symbolic states but took 47.3s in total (thus only 14.4s for equality and inequality invariants (including max/min relations that NumInv does not consider) inference due to the use of symbolic states).
# For this program, NumInv took 50.51s in total, but it repeatedly invoked KLEE as a black box to check invariants.  The algorithmic advantage of using symbolic states allows {{{tool}}} to run faster, despite using a slower symbolic execution tool -- {{{tool}}} using KLEE to generate symbolic states would run substantially faster.

# \paragraph*{\emph{G-CLN}} The recent tool G-CLN~\cite{yao2020learning} uses a gated continuous neural network to learn candidate invariants from program traces and relies on user-supplied specifications (e.g., postconditions) to check the invariants.
# G-CLN focuses on nonlinear invariants (and also was evaluated on the NLA benchmark). %, but does not support max/min invariants.
# The experimental data of G-CLN consists of pre-supplied concrete program traces for 27 NLA programs (it does not have \texttt{Bresenham}) and Z3 formulae representing the semantics and specifications (e.g., loop invariants or post-conditions) for each program.

# We ran the provided runscript, which invokes G-CLN to learn invariants from given traces and checks candidate invariants with provided specifications.
# We confirmed that the generated invariants match or imply the correctness of 26/27 programs\footnote{For \texttt{knuth}, G-CLN only infers linear equalities $a = d$ and $t = 0$.}  as shown in Table~\ref{tab:compare}.
# The runtime\footnote{The G-CLN paper runs its experiments on a GPU, which our machine does not have, and thus we only run G-CLN on CPU.} of G-CLN ranges from 8.9s for \texttt{fermat2} to 78.7s for \texttt{lcm1} (with median around 26.5s).
# % .\khn{note: we only use CPU for PyTorch (and not GPU).\tvn{why mention CPU and GPU?}}
# % \khn{I think we might need to mention that, because the original paper run experiments on GTX 1080Ti GPU.}

# We found a couple of limitations in the implementation of G-CLN.
# First, the tool requires user-supplied loop invariants and post-conditions to check its results.
# This guarantees sound results, but needs the user to provide this information (in most programs, the given specifications are either the exact documented invariants or something similarly informative)%, e.g., for \texttt{hard}, the given post-conditions are the documented invariants $q = 0, r = A, d = B*p$ and for \texttt{ps2}, a given postcondition is $2x - k^2 - k = 0$, which is the documented loop invariants $2x-y^2-y=0$ and the loop exit condition $c=y=k$).
# % <<<<<<< HEAD
# % \tvn{KimHao, given a couple of examples})
# % \khn{For \texttt{ps2}, the given post condition is $2x - k^2 - k = 0$ and documented invariants are $2x - y^2 - y = 0$, $c = y$, and $c \le k$ (note that $c$ and $y$ will have values from $1$ to $k$ in the while loop, and by the end of the loop, $c = y = k$)\khn{todo 1 more}\tvn{do you have something where the given postconditions are exacted the doumented nonlinear invairants}.}.
# % =======
# % This guarantees sound results, but needs the user to provide this information (in most programs, the given specifications are documented nonlinear invariants, e.g., \tvn{KimHao, given a couple of examples})
# % \khn{For \texttt{ps2}, the given post condition is $2x - k^2 - k = 0$ and documented invariants are $2x - y^2 - y = 0$, $c = y$, and $c \le k$ (note that $c$ and $y$ will have values from $1$ to $k$ in the while loop, and by the end of the loop, $c = y = k$)\khn{For the first loop of \texttt{hard}, the post conditions are $A >= 0, B >= 1, r == A, d == B * p, q == 0, r < d$, and the documented invariants are $A >= 0, B >= 1, q == 0, r == A, d == B*p$}.}.
# % >>>>>>> 9314b9c6fde95e253bec3e0b6143d030eac795e6
# Second, G-CLN relies on the given traces and does not create new inputs or traces.
# G-CLN is very sensitive to both of these factors.
# We reran it eliminating the user-supplied specifications and using 90\% of the traces and found that  
# G-CLN only obtained sufficiently strong invariants for 20/27 programs.
# In addition to \texttt{knuth}, G-CLN failed 5 new programs: \texttt{egcd2}, \texttt{freire2}, \texttt{lcm1}, \texttt{lcm2}, and \texttt{prod4br}.
# For example, G-CLN was not able to infer the documented invariants $qx + sy = b$, $px + ry = a$ in \texttt{egcd2} and $4r^3 - 6r^2 + 3r + 4x - 4a = 1$ in \texttt{freire2}.
# Note that these issues can be mitigated by using a CEGIR approach like the one used in {{{tool}}}, e.g., using symbolic states to check invariants and generate counterexample inputs and traces to improve inferred results.

# % >>>>>>> ef711053581c05274fbe74382b101ae8365d9e70

# %\texttt{freire2} has three documented invariants, $4r^3 - 6r^2 + 3r + 4x - 4a = 1$, $x \ge 0$, and $-12r^2 + 4s = 1$; G-CLN fails to find the first equation.
#       %       This is equivalent to our use of symbolic states to check the results.

# Moreover, while learning invariants using gated neural networks can be effective, we found that G-CLN requires many specific settings from the users for \emph{each program}.
# For example, the parameter \texttt{limit\_poly\_terms\_to\_unique\_vars} is only used in \texttt{geo3}, and \texttt{drop\_high\_order\_consts} is only used in \texttt{prod4br}. 
# The \texttt{dropout} parameter is configured differently for different programs: for \texttt{fermat1}, \texttt{fermat2}, and \texttt{ps2}--\texttt{ps6} it is 0; for \texttt{mannadiv} is 0.1; for \texttt{freire2}, and \texttt{cohencu} it is 0.2; for \texttt{sqrt1} it is 0.5; and for the rest it is 0.3.
# The specifics of how inference is performed are explicitly controlled by configuration parameters.
# For some programs inequalities  are disabled (e.g., \texttt{egcd2, egcd3, knuth, lcm1} have \texttt{ineq=-1}), while some use different inequality inference methods (e.g.,  \texttt{cohencu}, \texttt{cohendiv/2}, \texttt{divbin/2}, \texttt{hard/2}, \texttt{mannadiv}, \texttt{ps2}--\texttt{ps6}, \texttt{sqrt1} use \texttt{ieq=1} and others use \texttt{ieq=0}).
# Note here that \texttt{program/2} means the second loop of \texttt{cohendiv}) (thus these parameter settings are not for each program, but also for each program location in some cases). 
# Moreover, for 12 programs, the runscript consists of degree information for \emph{individual variables} to control the generation of terms.
# For example, \texttt{prod4br} has $\texttt{max\_deg=3},  \texttt{var\_deg}=\{q:3, p:3, a:0, b:0, x:1, y:1\}$ to specify that terms such as $xq$ and $pq$ are not considered because their degrees exceed 3.

# In short, there are many parameters in G-CLN, and their uses and values depend on different programs.
# When we run without these customizations, G-CLN fails to discover the expected invariants and in many cases produced run-time errors.
# For example, with just the default settings (e.g., \texttt{max\_deg=2}), G-CLN obtained sufficiently strong invariants for only 5/27 programs and also gave runtime errors for 6 programs.
# Surprisingly, when using \texttt{max\_deg=4} (which technically would help generate more terms and thus invariants), G-CLN produced runtime errors for 5 programs and was not able to find sufficiently strong invariants for any programs (e.g., even missing \emph{linear} equalities such as $z=6n+6$ in \texttt{cohencu}).


# We were not able to run G-CLN on new programs because we do not know what parameters, settings, traces, and user-supplied specifications should be provided.
# Given that G-CLN requires extensive customization even on its own benchmark programs, we believe that it is difficult to make the tool work with new programs.
# Note that G-CLN does not support max/min invariants so it will likely fail to find those invariants, which are required in the DISJ benchmark programs.

# %Without other tweaks but the default $\max\_deg=2$ setting and the individual degree settings, {{{tool}}} 


# %0/27 programs (5 of these programs give runtime error and the other 22 programs failed to find documented invariants, e.g., $t=2a+1$ and $s=(a+1)^2$ for \texttt{sqrt1}).


# %Running the experiments without these customizations produces 

# %(e.g., 6 programs give runtime error, some produce no results or )



# %e.g., the user provide \texttt{var\_deg} option that give the maximum degree specfically for each variable.
# %the programs  \texttt{dijkstra}, \texttt{egcd2}, \texttt{egcd3}, \texttt{fermat1}, \texttt{fermat2}, \texttt{geo1}, \texttt{geo2}, \texttt{geo3}, \texttt{prod4br}, \texttt{ps2}, \texttt{ps3}, and \texttt{ps4}




# %(e.g., the tool has different values for parameters such as neural network traning parameters, max degree, and in some cases the max degree value for \emph{each} variable).
# %For example, for \texttt{geo3}, t


# %While most program analysis tools can be customized to increase flexiblity, they often have default values that work well in most cases. 

# %In short, we were not able to determine  Given that every program has a different set of parameters and each paramemter has a different value, 

# %Despite the novelty and potential usefulness of using  gated continous neural network, this 

# %for the neural network to work effectively G-CLN involves many parameters to tweak, e.g.,
# %The median number of custom configurations is 3.
# %\texttt{geo3} has the most number of custom configurations (8):

# % For example, without the given postconditions, G-CLN fails to find correct invariants \texttt{ps5} : it misses the inequality $c\le k$
# % which has three documented invariants $6y^5 + 15y^4 + 10y^3 - 30x - y = 0$, $c = y$, and $c \le k$. G-CLN misses the last inequality $c \le k$.


# %as if we randomly reduce the number of provide traces by 10\%.


# % The recent work G-CLN~\cite{yao2020learning} uses gated continuous neural networks to learn nonlinear SMT formulae to represent loop invariants from program execution traces and uses traditional Hoare logic to check inductive loop invariants.  This approach requires that sufficiently strong postconditions are given in order to prove the invariants and generate effective counterexamples to remove spurious results.
# % Thus, without sufficient strong goal (e.g., assertions or postconditions), this technique, similarly to PIE and ICE, cannot generate strong invariants  and thus, is not directly comparable with {{{tool}}}.
# % \tvn{Provide stats (1) showing G-CLN needs assertions and postconditions to work and (2) that the ML requires lots of manual tweakings}.

# % \khn{NEW}
# % G-CLN requires expert knowledge to infer correct invariants (invariants that imply the documented invariants).
# % Using the provided dataset and configurations, G-CLN inferred correct invariants for 25/27 programs.
# % \tvn{why can't it infer all?  what 2 programs can't it do and why?}
# % .

# % Without the provided post condition, G-CLN inffered correct invariants for 24/27 programs
# % \tvn{which ones, and why?}.
# % Without postconditions, G-CLN fails to find correct invariants for one more program, \texttt{ps5}, which has three documented invariants
# % $6y^5 + 15y^4 + 10y^3 - 30x - y = 0$, $c = y$, and $c \le k$.
# % G-CLN misses the last inequality $c \le k$.

# % G-CLN implementation does not have the facilities to collect new traces, so the counterexamples were already incoperated into the provided traces.



# % Without both post condition and custom configurations, G-CLN solved 5/27 programs.
# % \tvn{give some concrete examples}
# % 6 benchmarks give runtime error.
# % For \texttt{geo3}, G-CLN found no equations, hence misses the documented invariant $xz - x + a - azy = 0$.
# % The same happens for \texttt{prod4br}, which G-CLN misses the equation $q + abp == xy$.
# % For \texttt{fermat2}, G-CLN found two simple equations $u\%2 = 1$ and $v\%2 = 1$, but miss a more complex equation $4(A + r) = u^2 - v^2 - 2u + 2v$.
# % For \texttt{cohencu} first loop, G-CLN misses all invariants $z = 6n + 6$, $y = 3n^2 + 3n + 1$, $x = n^3$, and $n \le a + 1$.
# % For the second loop of \texttt{divbin}, G-CLN misses the inequality $r < b$.

# % By default, the \texttt{max\_deg} option is set to 2.
# % Without both post condition and custom configurations, and max degree set to 4, G-CLN solved 0/27 programs.
# % \tvn{give some concrete examples}
# % 5 programs give runtime error.
# % \texttt{ps2} has three documented invariants $2x - y^2 - y = 0$, $c = y$, and $c \le k$; but G-CLN only found the second equation.
# % For \texttt{sqrt1}, G-CLN misses all invariants $a^2 \le n$, $t = 2a + 1$, and $s = (a + 1)^2$.

# %We also tried to run G-CLN on new programs but were not successful because the implmentation has many hard-coded data for each test program.
# %\tvn{and also the format has to be a formulae?  give some concrete examples}
# %The input has two parts: $.csv$ traces, and Z3 formulae for loop condition, pre condition, post condition, and loop body.


# % \tvn{Can we say that G-CLN does not support max/min invs?  Just based on what they inferred for the NLA ones?}
# % \khn{I'm not sure about this one, but AFAIK, G-CLN won't support max/min invs out-of-the-box.}


# \paragraph*{\emph{PIE}} PIE~\cite{padhi2016data} uses a CEGIR and decision learning approach to infer invariants to prove given specifications, e.g., assertions or pre and postconditions.
# Thus, PIE aims to find sufficiently strong invariants to prove given specifications.

# We were not able to directly run PIE because the original PIE tool that works with C is no longer available and the current version instead requires program models, which we find difficult to obtain from high-level languages such as C or Java.
# %models in the SyGuS specification format~\cite{sygus}, which is difficult to convert from high-level imperative language programs.
# Nonetheless, in the NumInv work~\cite{nguyen2017counterexample}, we were able to run the original PIE and found that it failed to prove the annotated properties in 8 HOLA programs (e.g., it generates invariants that are too weak to establish them).
# For example, for H37,  PIE failed to prove the postcondition \lt{if (n > 0) assert(0 <= m && m < n)} which involves both conditional assertions and strict inequalities.
# For this program, {{{tool}}} inferred 2 nonlinear equations and 3 inequalities\footnote{$m^2 = nx - m - x, mn = x^2 - x,  -m  \le x, x \le m + 1, n \le x$}, which are correct and together show the assertion.
# PIE also failed to find any of the high-degree nonlinear invariants found by {{{tool}}} (e.g., in NLA), even when we ask it to find invariants to prove those nonlinear invariants.

# \paragraph*{\emph{CHC} Solvers}
# Several verification works encode verification tasks (program semantics and desired property) as Horn clauses, and then use a CHC solver to find invariants to prove the given property.
# Thus, similar to PIE, these works generate invariants to prove specific goals.

# We evaluated two popular CHC solvers Eldarica~\cite{rummer2013disjunctive} and GSPACER~\cite{krishnan2020global} to prove the properties in the HOLA programs.
# These CHC solvers work on formulae encoded in the SMT-LIB format, and we directly use the SMT-LIB files provided for the 46 HOLA programs available at~\cite{smthola}.

# Eldarica solved 46/46 HOLA programs, most of them in under 3 seconds (except H34 took 11 seconds, and H32 took 18 minutes).
# GSPACER solved 41/46 programs, most of them in under 1 second (except H18 took 3.8 minutes).
# Thus, these tools are comparable to {{{tool}}} and better than PIE.

# However, just like PIE, these techniques focus on generating invariants to verify given goals (assertions or postconditions in the HOLA programs). Specifically, they cannot infer invariants in the absence of a given property whereas this is precisely what {{{tool}}} is designed to do.
# For example, when we change the postcondition of H19 to \texttt{True}, Eldarica and GSPACER simply generate \texttt{True} as the invariants.
# These solvers also appear sensitive to the given conditions.
# For H30, Eldarica was able to prove the annotated postcondition $c\ge 0$, but fails to terminate when given something else, e.g., $c=100$ or $c=499500$.
# For H30, GSPACER got timeouts in all cases, even with the annotated postcondition.
# Eldarica and GSPACER also did not terminate after 30 minutes when we attempt to prove incorrect properties  (e.g., the incorrect postcondition $y \ne 100$ in H19).

#       %       {{{tool}}} also improves on CEGIR technique PIE by computing invariants for more programs (43/46 vs. 38/46 HOLA programs); we note that in combination {{{tool}}} and PIE can compute invariants for all 46 HOLA programs.


# %\paragraph*{\textbf{Others}}: There were also other tools but they do not focus on high-degree nonlinear invariants or disjunctive invariants, e.g., PIE, ICE, and iDiscovery (another CEGIR-based tools reviewed in \S\ref{sec:related}) cannot find any of the high-degree nonlinear invariants found by {{{tool}}}. \tvn{may be move this to related}

# %\tvn{Rewrite this section so that it's not too much about comparing to PIE, but to show an application of that {{{tool}}}'s invs. There are many other static analysis works, e.g., PIE, that can verify these HOLA programs (without explicitly infer invariants). Here we show that SymInfer's inferred invs can also do that.}



# \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
#   \textbf{RQ6:} {{{tool}}} was able to infer more numerical invariants than NumInv, G-CLN, PIE, Eldarica, and GSPACER.
#   The ability to exploit and reuse symbolic states allows {{{tool}}} to strike a balance between expressive power and computational cost, while guaranteeing correctness, to establish state-of-the-art performance in numerical invariant inference.
# \end{tcolorbox}

# \subsection{Threats to Validity}
# The chief threat to external validity lies in the generalizability
# of the benchmarks used in our evaluation.
# Our evaluation uses 4 different benchmarks developed by
# other research groups and we use all of each of the benchmarks--we do not select subsets of benchmarks.
# The benchmarks are admittedly small programs and they
# clearly do not capture many aspects of complexity present
# in large software projects.
# However, they do include complex computational kernels that are
# characteristic of realistic programs, e.g.,~\cite{gulwani2009control}.
# Moreover, invariant inference techniques can be applied modularly
# to individual functions, so the complexity of the enclosing software
# system is less relevant to assessing the cost-effectiveness of such
# techniques.
# Finally, we aim to promote comparative evaluation and
# reproducibility of our results which is achieved by using
# standard benchmarks and releasing our implementation\footnote{{{{tool}}} and all experimental data are available at \url{https://github.com/unsat/dig/}.
# }.

# {{{tool}}} makes use of multiple underlying analysis tools, e.g.,
# SPF, CIVL, SAGE, Z3, and DIG.  These are widely used and robust systems
# which provides a degree of confidence that they are correct.
# That said, our primary means of addressing the internal validity
# of our findings was to perform manual and automated checking of
# all experimental results.  For example, we ran independent checks,
# using an SMT solver to discharge validity claims for
# implication or equality formulae,
# to confirm that invariants computed by {{{tool}}} were valid invariants.
# We then manually checked all of those results.

# \section{Related Work}\label{sec:related}

# \paragraph*{\emph{Daikon-based Dynamic Analyses}} Ernst et al.'s pioneering work on Daikon~\cite{ernst2000dynamically} demonstrated
# that specifications of program behavior can be inferred by observing concrete
# program states.  Daikon used a template-based approach to define candidate invariants
# and, to mitigate cost, a rather modest set of templates is used that do not
# capture nonlinear or disjunctive properties.
# Many researchers have built on the foundation of Daikon by adopting its template-based
# approach.
# For example, iDiscovery~\cite{zhang2014feedback} uses Daikon templates for inference and then attempts to verify or refute
# candidate invariants by running the symbolic execution tool SPF.
# However, neither Daikon nor iDiscovery is capable of inferring the expressive nonlinear or
# disjunctive invariants that {{{tool}}} can infer
# for the programs in Figures~\ref{fig:cohen} and~\ref{fig:complexity}.

# \paragraph*{\emph{DIG}} The DIG~\cite{nguyen2014dig,nguyen2012using,nguyen2014using} dynamic invariant generation approach focuses on numerical invariants and supports more expressive families of templates,
# such as nonlinear equations and octagonal inequalities, and therefore can compute more expressive numerical relations than those supported by Daikon.
# However, DIG's results are only correct with respect to given program execution traces and might not generalize (i.e., they can be spurious).

# In \cite{nguyen2014using}, DIG is extended with max/min invariants to infer disjunctive information and integrated with a theorem prover using k-induction to prove valid invariants and remove spurious ones. This work shows that many loop invariants, especially those in complex nonlinear programs, cannot be proved using standard induction (i.e., when $k=1$) and requires k-induction where $k>0$.
# However, the requirement that invariants being formally proved using $k$-induction makes this work very expensive, e.g., the sorting program \texttt{oddeven5} shown in~\S\ref{sec:eval} takes over half an hour to be proved.
# Due to this inefficiency, after disproving an invariant, this work does not use counterexample inputs to refine that invariant or to find new ones. % (which would invoke the expensive proving process for each new candidate invariant).%  , after disproving that $x\le 5$ is invalid, it does not find new inputs to refine the upper bound of $x$.
# NumInv~\cite{nguyen2017counterexample} combines DIG's algorithms to infer nonlinear equations and octagonal invariants and the symbolic execution tool KLEE~\cite{cadar2008klee} to check and generate counterexamples to refine those invariants.
# We compared NumInv to {\tool} in~\S\ref{sec:rq6}.
# %This work uses KLEE as a blackbox and invokes the tool for every invariant check.
# %While KLEE is efficient, having to invoke the tool on every candidate invariant is time-consuming.  Thus NumInv does not support disjunctive relations, such as max/min, as these would generate more candidate invariants to check.
# %One of the novelties of {{{tool}}} lies in how it exploits and resues symbolic states computed by symbolic execution to improve invariant inference.

# %explores trade-offs between correctness and efficiency by using symbolic execution instead of a formal prover.
# %NumInv uses a CEGIR iterative algorithm that combines DIG's algorithms to infer nonlinear equations and octagonal invariants and the symbolic execution tool KLEE~\cite{cadar2008klee} to check those invariants for C programs.
# %If the invariants are incorrect KLEE returns counterexample traces, which help dynamic inference obtain better results.


# \paragraph*{\emph{Static Analyses and Goal-Oriented Invariant Generation}}

# Static analyses based on the classical abstract interpretation framework~\cite{cousot1992abstract,cousot1996abstract,mine2006octagon} generate sound invariants under abstract domains (e.g., interval, octagonal, and polyhedra domains) to overapproximate program behaviors to prove the absence of errors~\cite{cousot2005astree}.
# Trade-offs occur between the efficiency and expressiveness of the considered domains.
# The work in~\cite{rodriguez2004automatic} uses the domain of nonlinear polynomial equalities and Gr\"obner basis to generate equality invariants of the form in Eq~\ref{eq:eqt}.
# This approach is limited to programs with assignments and loop guards expressible as polynomial equalities and requires user-supplied bounds on the degrees of the polynomials to ensure termination.
# The work in~\cite{rodriguez2007generating} does not require upperbounds on polynomial degrees but is restricted to non-nested loops.
# {\tool} also generates invariants under various domains, but it integrates learning and checking candidate invariants using symbolic states, and does not have limitations on program constructs or require a priori degree knowledge.

# Many static analyses generate invariants to prove specifications, e.g., assertions and pre and postconditions for a function or program, and thus can exploit the given specifications to guide the invariant inference process.
# PIE~\cite{padhi2016data} and ICE~\cite{garg2016learning} use CEGIR approach to learn invariants to prove given assertions.
# To prove a property, PIE iteratively infers and refines invariants by
# constructing necessary predicates to separate (good) states satisfying the
# property and (bad) states violating that property.
# ICE uses a decision learning algorithm to guess inductive invariants over
# predicates separating good and bad states
# and generates ``implication'' counterexamples to
# learn more precise invariants.
# For efficiency, they focus on octagonal predicates and only search for invariants that are boolean combinations of octagonal relations (thus do not infer nonlinear and disjunctive invariants such as those shown in Figures~\ref{fig:cohen} and~\ref{fig:complexity}).
# The data-driven approach G-CLN~\cite{yao2020learning} uses gated continuous neural networks to learn numerical loop invariants from program execution traces and uses traditional Hoare logic and Z3 to check inductive loop invariants.  
# %In addition to relying on substantial problem-specific customization, this approach requires that sufficiently strong postconditions are given to prove the invariants.
# We compared PIE and G-CLN to {\tool} in~\S\ref{sec:rq6}, and found that without sufficiently strong goals (e.g., given postconditions), these approaches (PIE, ICE, G-CLN) cannot generate strong invariants like those discovered by {{{tool}}}.
# G-CLN also relies on substantial problem-specific customizations to generate invariants.


# \paragraph*{\emph{CHC Solvers}}
# Several verification works use constrained Horn clauses (CHC) to synthesize invariants to prove safety properties~\cite{gurfinkel2015seahorn,fedyukovich2017sampling,fedyukovich2018solving,fedyukovich2018accelerating,prabhu2018efficiently}.
# These works encode verification conditions, which consist of program semantics (e.g., initial states, infeasible post states, state transitions) and predicates with unknowns representing inductive and safe invariants (satisfying the initial states but avoiding the bad states) as Horn clauses, and then use a CHC solver to find satisfying assignments for the unknowns to generate the invariants.
# Thus, the problem of generating program invariants to prove programs is reduced to the problem of CHC satisfiability solving, which can be efficiently solved due to advancements in CHC solving technologies. %and advancements in CHC solving techniques directly help invariant generation.

# Examples of CHC solvers include Eldarica~\cite{rummer2013disjunctive}, which checks the satisfiability of Horn clauses over Presburger arithmetic by combining Predicate Abstraction~\cite{graf1997construction} and CEGAR~\cite{clarke2003counterexample}, and the solver works in\cite{hojjat2018eldarica,hojjat2017deciding}, which extend Eldarica to support formulae over the theories of integers, algebraic data-type, and bit vectors.
# SPACER~\cite{komuravelli2016smt}, a popular SMT-based model checking Horn solver, is used in Z3 and CHC-based program analysis tools such as SeaHorn~\cite{gurfinkel2015seahorn}, and has been built upon by other CHC solving techniques such as GSPACER~\cite{krishnan2020global}.
# FreqHorn~\cite{fedyukovich2017sampling} learns candidate invariants by analyzing samplings representing frequency distributions of features found in the program (e.g, formulae involving variables, constants, arithmetic, and comparison operators in code).
# Other works, e.g.,~\cite{prabhu2018efficiently,fedyukovich2019quantified}, extend FreqHorn to use execution traces in addition to program features to learn invariants.
# In particular,~\cite{prabhu2018efficiently} uses equation solving to infer candidate invariants and generates counterexamples to check if the invariants can be represented using purely polynomial equations or they would need conditional invariants.
# %FreqHorn has also been extended to infer invariants about array ranges to reason about safety properties of programs involving arrays~\cite{fedyukovich2019quantified}.



# % In addition to CHC Solvers, syntax-guided synthesis (SYGUS) solvers used in program synthesis can also be used to generate invariants.
# % These solvers aim to find satisfying assignments to a logical formula encoding a program (or a fragment of a program represented as unknown holes in the formula) under certain a template or grammar to satisfy a given specification.
# % Since invariants can be represented as code, CVC4sy\tvn{brain stops working ... will have to finish this tomorrow.}



# % The SMT-based model checking SPACER solver uses compositional analysis to iteratively create and check local reachability queries for unknown predicates in large Horn clauses.
# % SPACER is bundled in Z3 to solve Horn clauses, and also 
# %\tvn{need to rewrite FreqHorn, it's a CHC solver, not a program anlysis tool, also talk about CVC4SY solver, which is a related work mentioned by R3}
# % The FreqHorn~\cite{fedyukovich2017sampling} learns candidate invariants from program syntax encoding and sampling frequencies.%, represents these information as linear CHC's, and solves them using the Z3 SMT solver.
# % %\cite{fedyukovich2018accelerating} improves FreqHorn speed by combining enumerative learning with inductive-subset extraction and leveraging counterexamples-to-induction (CTIs) and interpolation-based bounded proofs.

# %include Spacer, an SMT-based model checking technique 

# %Among well-known CHC solvers, the SMT-based model checking SPACER solver~\cite{komuravelli2016smt} uses compositional analysis to iteratively create and check local reachability queries for unknown predicates in large Horn clauses.
# %SPACER is bundled in Z3 to solve Horn clauses, and also has been employed by CHC-based program anslysis tools such as SeaHorn~\cite{gurfinkel2015seahorn} and extended by other CHC solving techniques such as GSPACER~\cite{krishnan2020global}.
# %Other CHC solvers (e.g., GSPACER~\cite{krishnan2020global}) are also built directly on top of SPACER.
# %The Eldarca solver~\cite{rummer2013disjunctive} that.

# %These CHC-based works rely on the power of backend Horn solvers.

# Similar to the goal-oriented invariant generation techniques, CHC solvers synthesize invariants to solve verification conditions encoding specific program properties. % (e.g., implicit safety properties such as no null pointer dereferencing or explicitly given assertions).
# Thus, the generated invariants largely depend on the verification goal. %(e.g., the assertions representing the ``bad'' or unsafe states).
# As a concrete example, for \texttt{ps2}, when given the documented postcondition $y^2 - 2x + y = 0$, FreqHorn quickly (within a second) found 3 invariants,  $y^2 - 2x + y = 0$, $y^2 - 2x + y \ge 0$, and $y \ge 0$, to prove the given postcondition. %.in around 1.3s.
# When given a less precise post condition $y - x \le 0$, FreqHorn finds 3 invariants: $y \ge 0$, $x - y \ge 0$, and $x - 2y \ge -1$ (and no equality invariants).
# Interestingly, when given something that is \emph{not} an invariant, e.g., the wrong postcondition $y^2-2x=0$, FreqHorn does not appear to terminate (we manually kill its process after 15 minutes).
# For a more complex example such as \texttt{ps6}, even when given the documented postconditions, FreqHorn cannot generate any invariants to prove the postconditions and also does not appear to terminate.
# These tools (FreqHorn and the CHC solvers Eldarica and GSPACER evaluated in~\S\ref{sec:rq6}) do not generate invariants if the goal is not specified.


# %In contrast, {\tool} is an invariant discovery tool, which generates invariants under specific templates regardless of any specific goals.
# %\khn{FreqHorn does not terminate after 15 minutes for \texttt{ps6} (with post condition same as invariant $2y^6 + 6y^5 + 5y^4 - y^2 - 12x = 0$)}

# %In contrast, {\tool} is an invariant discovery tools that generate invariants regardless 
# %The Z3 SMT solver employs the SPACER solver traditionally used in SMT-based model checking techniques~\cite{}.
# %To scale to large formulae, SPACER uses compositionality that
#                                                                                              %                                                                                              The CHC-based program analysis tool Seahorn also employ SPACER 

# %The SPACER CHC solver bundled with Z3 uses the IC3/PDR (Property-Directed Reachability) approach in model checking

# % Development in Horn solvers directly affects the performance of these works, as they often invoke off-the-shelf SMT solver such as Z3 that integrates Horn solver.


# % The Z3 SMT solver and well-known CHC-based program anlintegrates the SPACER technique~\cite{komuravelli2016smt} to solve CHC's.
# % The classical SPACER tool uses a model checking algorithm compute over and under-approximation 

# % The Spacer solver~\cite{komuravelli2016smt} extends the IC3/PDR engine in Z3 



#                                                                                              %                                                                                              by combining Predicate Abstraction~\cite{graf1997construction} with CEGARclarke2003counterexample



# %the problem of generating program invariants is reduced to the problem of CHC satisfiability solving and improvements to CHC solvers directly help invariant generation.

# %As these techniques rely on solving CHC clauses, 

# %


# % A Constrained Horn Clause (CHC) is a logical implication involving unknown predicates.
# % Systems of CHCs are often used to verify programs with arbitrary loop structures: interpretations of unknown predicates, which make every CHC in the system true, represent the inductive invariants~\cite{fedyukovich2018solving}.
# % \khn{near-verbatim copy from \cite{fedyukovich2018solving} abstract. Do we need to rephrase this?}
# % Let $V$ be a set of variables and $V'$ be a primed copy of $V$ (i.e., $V' = \{x' \mid x \in V\}$). 
# % CHC solvers take a system of CHCs, which is a set of SMT formulae: (i) pre condition $Init(V)$, (ii) loop transition $Tr(V, V')$, and (iii) bad states to avoid $Bad(V)$.
# % The solvers find a \emph{safe} inductive invariant $Inv(V)$ that do not intersect with the bad states~\cite{fedyukovich2018accelerating}.
# % \begin{align*}
# %     Init(V) &\Longrightarrow Inv(V) && \text{(i)} \\
# %     Inv(V) \land Tr(V, V') &\Longrightarrow Inv(V') && \text{(ii)} \\
# %     Inv(V) \land Bad(V) &\Longrightarrow \bot && \text{(iii)} \\
# % \end{align*}
# % A system of CHCs can also represent a program with multiple loops by making the invariant and the negations of loop condition of the previous loop as the precondition for the next loop~\cite{fedyukovich2018solving}.

# % \cite{fedyukovich2017sampling} introduce FreqHorn, an SMT-based, probabilistic, and syntax-guided method to discover inductive numerical invariants.
# % FreqHorn uses an \emph{``enumerate-and-check''} approach, which generates invariant candidates from program's source code and then checks using an SMT solver.
# % FreqHorn stops when it all invariants needed to verify safety, or or until the search space is exhausted.
# % \cite{fedyukovich2018accelerating} improves FreqHorn speed by combining enumerative learning with inductive-subset extraction and leveraging counterexamples-to-induction (CTIs) and interpolation-based bounded proofs.
# % \cite{prabhu2018efficiently} and \cite{fedyukovich2018solving} extend FreqHorn to use execution traces in addition to the syntax to generate invariant candidates.
# % \cite{prabhu2018efficiently} uses equation solving like {\tool} to generate candidate equality invariants, and uses an SMT solver to check if they satisfy the CHC system or find a counterexample for the candidate invariant.
# % \cite{prabhu2018efficiently} uses the counterexamples to check if the invariants can be represented using purely polynomal equations, or they would need conditional invariants.
# % \cite{fedyukovich2019quantified} extends FreqHorn to infer inductive invariants for array-handling programs (universally quantified invariants).

# % \khn{I'm not sure about this part}
# % \cite{komuravelli2016smt} introduce RecMC, an SMT-based algorithm for model checking safety of recursive programs, implemented in Spacer.
# % Spacer extends the IC3/PDR engine in Z3~\cite{demoura2008z3} by maintaining both under- and over-approximations during analysis.
# % \cite{krishnan2020global} presents GSPACER, which extends Spacer with explicit \emph{global guidance} into the local reasoning performed by IC3-style algorithms.

# % \cite{rummer2013disjunctive} presents the Eldarica solver for horn clauses over Presburger arithmetic by combining Predicate Abstraction~\cite{graf1997construction} with Counterexample-Guided Abstraction Refinement (CEGAR)~\cite{clarke2003counterexample} to automatically check whether a given set of Horn clauses is satisfiable.
# % \cite{hojjat2018eldarica} extends Eldarica to support problems over the theories of integers, algebraic data-types~\cite{hojjat2017deciding}, and bitvectors.
# % \cite{reynolds2019cvc} (todo)

# % CHC solvers can guarantee the correctness of the generated invariants, and hence can be used to verify safety properties of a program.
# % However, the quality of the results largely depend on the post conditions provided by the users.
# % For example, if $Bad(V) = \bot$, then $Inv(V) = \top$ is enough to satisfy the system of CHCs.
# % CHC solvers also require the program to be model in a specific way (SMT formulae), hence cannot be used directly on program source code.

# % For \texttt{ps2}, when given the precise post condition $y^2 - 2x + y = 0$, FreqHorn found 3 invariants:  $y^2 - 2x + y = 0$, $y^2 - 2x + y \ge 0$, and $y \ge 0$ in around 1.3s.
# % When given a less precise post condition $y - x \le 0$, FreqHorn finds three invariants: $y \ge 0$, $x - y \ge 0$, and $x - 2y \ge -1$ (and no equality invariants).
# % When given the weakest post condition ($\top$), FreqHorn finds the same three invariants as the previous case.
# % However, when given the wrong post condition ($y^2 - 2x = 0$), FreqHorn do not terminate after 15 minutes.

# \section{Conclusion}\label{sec:conclusion}
# We introduce the concept of symbolic states as an intermediate
# representation that can be leveraged to support the automated
# generation of
# useful and complex invariants for software systems.
# We propose a CEGIR approach that exploits symbolic states
# to generate candidate invariants and also to check or refute,
# and iteratively refine, those candidates.
# A key to the success of these methods is the ability to directly manipulate
# and reuse rich encodings of large sets of concrete program states.

# We present {{{tool}}} which implements CEGIR using symbolic states to efficiently discover rich invariants over numerical variables at arbitrary program locations.
# Evaluation on a set of 108 programs comprising 4 different benchmarks demonstrates that {{{tool}}} is cost-effective in discovering useful invariants to describe precise program semantics, characterize the runtime complexity of programs, and check nontrivial correctness properties.
# This offers compelling evidence of the benefits of symbolic states in invariant inference.

# Moreover, continuing advances in symbolic reasoning systems
# suggest that symbolic state representations are positioned to
# become increasingly attractive for invariant inference.
# For example, generating symbolic states can be sped up for invariant inference by combining directed symbolic execution~\cite{ma2011directed} to target locations of interest, memoized symbolic execution~\cite{yang2012memoized} to store symbolic execution trees for future extension, and parallel symbolic execution~\cite{staats2010parallel} to accelerate the incremental generation of the tree.  Moreover, we can apply techniques for manipulating symbolic states in symbolic execution~\cite{cadar2008klee,visser2012green} to significantly reduce the complexity of the verification conditions sent to the solver.
# In future work, we plan to explore how to extend and adapt such optimizations 
# from the general problem of symbolic execution to the problem of invariant inference.



# \ifCLASSOPTIONcompsoc
#   % The Computer Society usually uses the plural form
#   \section*{Acknowledgments}
# \else
#   % regular IEEE prefers the singular form
#   \section*{Acknowledgment}
# \fi

# We thank the anonymous reviewers for helpful comments.
# This material is based in part upon work supported by the UNL Faculty Award, by the
# National Science Foundation under grant numbers 1948536, 1617916, and 1901769,
# by the U.S. Army Research Office under grant number W911NF-19-1-0054,
# and by the DARPA ARCOS program under contract FA8750-20-C-0507.

# \bibliographystyle{IEEEtran}

# \debugite{
#   \bibliography{/users/tnguyen/git/www/nguyenthanhvuh/bibs/main.bib}
# }
# {
#   \bibliography{paper}
# }



# % biography section
# % 
# % If you have an EPS/PDF photo (graphicx package needed) extra braces are
# % needed around the contents of the optional argument to biography to prevent
# % the LaTeX parser from getting confused when it sees the complicated
# % \includegraphics command within an optional argument. (You could create
# % your own custom macro containing the \includegraphics command to make things
# % simpler here.)
# %\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
# % or if you just want to reserve a space for a photo:


# \begin{IEEEbiography}[{\includegraphics[width=1.3in,height=1.2in,clip,keepaspectratio]{me2.pdf}}]{ThanhVu Nguyen}
#   is an assistant professor in the Department of Computer Science at George Mason University, United States.
#   His research interests include dynamic invariant inference, automatic program repair, and
#   configurable systems analysis.
#   His work in these areas has been recognized with several of impact and distinguished paper awards.
# \end{IEEEbiography}

# % if you will not have a photo at all:
# \begin{IEEEbiography}[{\includegraphics[width=1in,height=1in,clip,keepaspectratio]{khn.pdf}}]{Kim Hao} is a sophomore  pursuing a bachelor's degree in Computer Science and Mathematics from the University of Nebraska-Lincoln, United States. He is a member of the UNSAT research group and advised by ThanhVu Nguyen. His research interests are in software analysis, testing, and verification. He has published in top software engineering conferences, including ICSE, OOPSLA, and ASE.
# \end{IEEEbiography}

# % insert where needed to balance the two columns on the last page with
# % biographies
# %\newpage

# \begin{IEEEbiography}[{\includegraphics[width=1.2in,height=1.5in,clip,keepaspectratio]{mbdwyer.pdf}}]{Matthew B. Dwyer}
# is the Robert Thomson Distinguished Professor in the Department of Computer Science at
# the University of Virginia, United States.
# His research interests include software analysis,
# verification and testing and his work in these areas has
# been recognized over the years with several test-of-time
# and distinguished paper awards.
# He is a Fellow of the IEEE and of the ACM.
# \end{IEEEbiography}


# % You can push biographies down or up by placing
# % a \vfill before or after them. The appropriate
# % use of \vfill depends on what kind of text is
# % on the last page and whether or not the columns
# % are being equalized.

# \vfill

# % Can be used to pull up biographies so that the bottom of the last one
# % is flush with the other column.
# \enlargethispage{-5in}


# \debug{
#   \newpage
#   \clearpage
#   \appendices
#   \input{appendix}
#   }


# \end{document}




