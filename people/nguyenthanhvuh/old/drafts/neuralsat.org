#+TITLE: NeuralSAT: A CDCL-based constraint solving approach to DNN Verification
#+date: <2021-06-13 08:02>
#+description: The Neuralsat DNN verification technique and tool
#+filetags: blog dnn ai ml verification
#+HTML_HEAD: <link rel="stylesheet" href="https://dynaroars.github.io/files/org.css">

#+MACRO: tool NeuralSAT

#+begin_quote
NeuralSAT: A CDCL-based constraint solving approach to DNN Verification
#+end_quote


* Introduction
Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. Among many others, they have been used for image recognition, autonomous driving, airplane collision control, power grid control, fake news detection, drug synthesis and discovery, and not surprisingly, COVID-19 detection and diagnosis.

However, just like traditional software, DNNs can have “bugs”, e.g., producing unexpected results on inputs that are different from those in training data, and be attacked, e.g., small perturbations to the inputs by a malicious adversary or even sensorial imperfections result in misclassification. These issues, which have been observed in many DNNs and demonstrated in the real world, naturally raise the question of how DNNs should be tested, validated, and ultimately *verified* to meet the requirements of relevant robustness or safety standards.

To address this question, researchers have developed powerful techniques and tools to verify DNNs. Constraint-based approaches aim to both correctly prove and disprove properties but do not scale to large networks. In contrast, abstraction-based approaches scale much better, and modern abstraction verification tools can refine their abstractions to avoid returning spurious counterexamples. In general, abstraction approaches work better, and the top performers in the annual Neural Network Verification competitions (VNN-COMPs), such as \alpha\beta-CROWN and ~nnenum~, all use abstractions.

Surprisingly, the problem of DNN verification, even with networks with nonlinear activation functions such as “ReLU”, has been shown in the seminal Reluplex work to be reducible from the classical satisfiability (SAT) problem. However, while modern SAT and SMT constraint solvers scale well to large formulae and help make the theoretically intractable SAT problem practically tractable, constraint-based DNN verifiers do not seem to scale and be competitive with the best DNN verifiers. For example, Planet, which adapts the MiniSAT solver for DNN verification, has been far surpassed by the state of the art (e.g., those competing in VNN-COMPs) and has not been improved upon or extended
in any new work. Other constraint approaches also have been shifting away from improving their core constraint-based search algorithms and focused on parallel optimizations and integration with abstractions, e.g., Marabou, the successor of the well-known Reluplex solver, integrates multithreading and abstractions to its core simplex-based solver.

Inspired by Planet and Marabou, but with the belief that constraint-based DNN verification can do better, we propose NeuralSAT, an
SAT solving approach that integrates abstraction with the DPLL(T) algorithm used in modern SAT/SMT constraint solvers. Our insight is to integrate clause learning (to improve performance by addressing SAT challenges such as avoiding past mistakes and improve backtracking) in modern SAT solving with an abstraction-based theory solver (to quickly check for infeasibility) in SMT solving and abstraction-based DNN verification.

In this work, we describe a NeuralSAT prototype that supports feedforward neural networks (FNNs), a major deep learning model, with the ReLU activation function. Preliminary results on the standard benchmarks used in VNN-COMPs show that the prototype, though in its early development stage and relatively unoptimized, is very competitive with the best DNN verifiers that are much more mature and optimized (e.g., leveraging multiprocessing heavily). We believe that the synergistic combination of clause learning and the-
ory abstraction is the “secret sauce” distinguishing NeuralSAT and helping it avoid the scalability problem faced by other constraint-based approaches.

Our experiments using real-world neural network benchmarks in VNN-COMPs consisting of billion of parameters show that NeuralSAT is highly competitive with other top performers in VNN-COMPs. It ranks second in the number of problems it can solve (just behind αβ-CROWN, the winner of VNN-COMP’21 and ’22) and is the fastest in the problems it can solve.

What is even more exciting is that NeuralSAT is still a relatively unoptimized prototype (e.g., its search algorithm is single-thread) while all modern DNN verification tools are heavily optimized and parallelized. We hope that NeuralSAT will renew interest in constraint-based approaches and lead to more scalable solutions for DNN verification, in the same way that clause learning and DPLL(T) algorithms allow modern SAT/SMT solvers to effectively handle the SAT/SMT problems.

* Background
We summarize DPLL and CDCL, satisfiability checking techniques that inspire the development of {{{tool}}}. We also review the DNN verification problem and abstraction techniques often used to solve them and adopted in {{{tool}}}.
Readers familiar with these techniques can skip this section.

** Constraint Solving


The classical satisfiability (SAT) problem asks if a given Boolean formula can be satisfied.  Given a formula $f$, a SAT solver returns /sat/ if it can find a satisfying assignment that maps truth values to variables of $f$ that makes $f$ evaluate to true, and /unsat/ if it cannot find any satisfying assignments. The problem is NP-Complete and being efficient is crucial for SAT solving techniques.


\begin{wrapfigure}{r}{0.228\textwidth}
  \centering
   \vspace{-0.2in}
   \hspace{-0.4in}
   \includegraphics[width=1.15\linewidth]{figure/dpll.pdf}
   \vspace{-0.1in}  
   \caption{\label{fig:dpll} DPLL Algorithm.}
   \vspace{-0.2in}
 \end{wrapfigure}
Fig.~\ref{fig:dpll} gives an overview of *DPLL*, a SAT solving technique introduced in 1961 by Davis, Putnam, Logemann, and Loveland. DPLL is an iterative algorithm that (i) decides an unassigned variable and assigns it a truth value, (ii) performs Boolean constraint propagation (BCP),  which detects \emph{unit clauses} containing single literals that must have certain values and either infers such values to them  or determines that would cause a conflict; (iii) analyzes the conflict to backtrack to a previous decision level \texttt{dl}; and (iv) erases assignments at levels larger than \texttt{dl} to try new  assignments. These steps repeat until DPLL finds  a satisfying assignment and returns \texttt{sat}, or decides that it cannot backtrack (\texttt{dl}=-1) and returns \texttt{unsat}.


*CDCL* (Conflict-Driven Clause Learning), introduced in 1996, is the standard algorithm in modern SAT solving.
CDCL implements DPLL, but also \emph{learns new clauses} to avoid past mistakes and backtrack non-chronologically.
%Similar to DPLL, CDCL is sound and complete, but also incurs an exponential complexity.  However, 
Due to its ability to learn conflicting clauses, CDCL can significantly  reduce the search space and allow SAT solvers to scale to large problems.

\textbf{DPLL(T)} extends CDCL to support checking SMT formulae involving nonBoolean variables, e.g., real numbers and data structures such as strings, arrays, lists.
DPLL(T) combines DPLL and CDCL with dedicated \emph{theory solvers} to analyze formulae in those theories\footnote{SMT is Satisfiability Modulo Theories and the T in DPLL(T) stands for Theories.}.  %Examples of SMT formulae include those involving numerical variables with linear or nonlinear arithmetics (formale involving nonlinear operations such as multiplication and division among variables),
For example, to check a formula involving linear arithmetics over the reals, DPLL(T) can invoke a linear programming solver to check the linear constraints in the formula.
Modern DPLL(T)-based SMT solvers such as Z3 and CVC4
include solvers supporting a wide range of theories including linear arithmetics, nonlinear arithmetics, string, arrays.

%Fig.~\ref{fig:dpllt} gives an overview of DPLL(T), which takes as input an SMT formula and returns \texttt{sat} or  \texttt{unsat}. The algorithm first  abstracts the SMT formula into a SAT formula by replacing atoms with Boolean variables (e.g., if $b$ represents $x\ge 5$, then $b=T$ means $x\ge 5$, and $b=F$ means $x < 5$). DPLL(T) then repeatedly uses the steps in DPLL to find an assignment for the SAT formula, e.g., analyzing  conflicts and returning \texttt{unsat} if backtracking is not possible (the Boolean/SAT abstraction guarantees that the unsatisfiability of the SAT abstraction implies that the input SMT problem is \texttt{unsat}). However, if DPLL(T) finds an assignment for the SAT formula, then that does not necessarily mean the original SMT problem is also satisfiable. DPLL(T) would invoke the dedicated theory solver to check if the assignment actually satisfies the SMT formula. If the theory solver validates the assignment, DPLL(T) returns \texttt{sat}; otherwise, it refines the SAT formula with the information given by the theory solver and repeats the process on the new SAT formula.


** The DNN verification problem

A /feed-forward neural network/ (*FNN*), the quintessential model in deep learning, consists of an input layer, multiple hidden layers, and an output layer. Each layer has a number of neurons, each connected to neurons from the previous layer through a predefined set of weights (derived by training the network with data). A \textbf{DNN} is an FNN with at least two hidden layers. 



The output of a DNN is obtained by iteratively computing  the  values  of  neurons  in  each  layer.
The value of a neuron in the input layer is the input data. The value of a neuron in the hidden layers is computed by applying an \emph{affine transformation} to values of neurons in the previous layers, then followed by an \emph{activation function} such as the popular Rectified Linear Unit (ReLU) activation.

Specifically, the value of a hidden neuron \(y\) is 
$ReLU(w_1v_1 + \dots{} + w_nv_n + b)$, where \(b\) is the bias parameter of \(y\), \(w_i, \dots, w_n\) are the weights of \(y\), \(v_1,\dots,v_n\) are the neuron values of the  previous  layer of \(y\), \(w_1v_1 + \dots + w_nv_n+b\) is the affine transformation, and \(ReLU(x) = \max(x,0)\) is the ReLU activation. The values of a neuron in the output layer is evaluated similarly but can be without applying the activation function.




*Verification.* Given a DNN \(N\) and a property $\phi$, the \emph{DNN verification problem} asks if $\phi$ is a valid property of $N$.
Typically, $\phi$ is a formula of the form $\phi_{in} \Rightarrow \phi_{out}$, where $\phi_{in}$ is a property over the inputs of $N$ and $\phi_{out}$ is a property over the outputs of $N$.
%This form of properties has been used to encode safety and security requirements of DNNs, e.g., safety specifications to avoid collision in unmanned aircraft~\cite{kochenderfer2012next} and \emph{adversarial robustness}~\cite{katz2017towards} properties desired by all DNNs, in which a small input perturbation does not cause major spikes in the DNN's outputs.
A DNN verifier attempts to find a \emph{counterexample} input to $N$ that satisfies $\phi_{in}$ but violates $\phi_{out}$.  If no such counterexample exists, $\phi$ is a valid property of $N$. Otherwise, $\phi$ is not valid and the counterexample can be used to retrain or debug the DNN~\cite{huang2017safety}.




% Verification tool such as Marabou and nnenum are then applied to the network to prove that the network is safe or identifier counterexample representing small input differences causing large output changes.


% \footnote{This is encoded as the differences of the inputs being within a certain small range  ($\phi_{in}$) implies the differences of the outputs still fall within a certain range in $\phi_{out}$)}.

\begin{figure*}
  \begin{minipage}[c]{0.23\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{figure/dnn.pdf}
    \caption{\label{fig:dnn} An FNN with ReLU.}
  \end{minipage}
  \hfill
  \begin{minipage}[c]{0.76\textwidth}
  \centering
  \footnotesize
    \begin{tabular}{c|c|cc|c|cc}
    \toprule
    Iter & \textbf{BCP} & \multicolumn{2}{c|}{\textbf{DEDUCTION}}& \textbf{DECIDE} & \multicolumn{2}{c}{\textbf{ANALYZE-CONFLICT}} \\
      &&Constraints&Bounds&&Backtrack&Learned Clauses\\
      \midrule
      Init &-& $I = -1 \le x_1 \le 1; -2 \le x_2 \le 2$ & $-1 \le x_1 \le 1; -2 \le x_2\le 2$ & - &-&$C = \{v_3 \lor \overline{v_3};\; v_4 \lor \overline{v_4}\}$\\
      
      1 &-&$I$ & $ x_5 \le 1 $& $\overline{v_4}@1$&-&-\\
      
      2 &-&$I; x_4=\texttt{off}$&$ x_5 \le -1$& - & 0 &  $C = C \cup \{v_4\}$\\
      
      3 &$v_4@0$&$I; x_4=\texttt{on} $&$ x_3 \ge 0.5; x_5 \le 0.5$ & $v_3@0$&-&-\\
      
      4 &-&$I; x_3=\texttt{on}; x_4=\texttt{on}$&-&- & \bf{-1} & $C = C\cup \{\overline{v_4}\}$\\
      
      % 5 &$v_4@0$&$I; x_3=\texttt{on}; x_4=\texttt{on}$&-&- & \bf{-1}& $C = C \cup \overline{v_3}\lor \overline{v_4}$\\
      
       \bottomrule
    \end{tabular}
    \caption{{{{tool}}}'s run producing \texttt{unsat}.}\label{tab:valid}    
  \end{minipage}
\end{figure*}


\paragraph{Example} Fig.~\ref{fig:dnn} shows a simple DNN with two inputs $x_1,x_2$, two hidden neurons $x_3,x_4$, and one output $x_5$. The weights of a neuron are shown on the edges connecting to it, and the bias is shown above or below each neuron. The outputs of the hidden neurons  are computed using affine transformation and ReLU, e.g., $x_3 = ReLU(-0.5x_1+0.5x_2+1.0)$. The output neuron is computed with just the affine transformation, i.e., $x_5=-x_3+x_4-1$.

A valid property for this DNN is that the output is $x_5 \le 0$ for any inputs $x_1 \in [-1,1], x_2\in[-2,2]$. An invalid property for this network is that $x_5 > 0$ for those similar inputs.
A counterexample showing this property violation is $\{x_1=-1, x_2=2\}$, from which the network evaluates to $x_5=-3.5$. Such properties can capture \emph{safety requirements} (e.g., one rule in the aircraft collision system in~\cite{kochenderfer2012next,katz2017reluplex} is ``if the intruder is distant and significantly slower than us, then we stay below a certain threshold'') or \emph{local robustness}~\cite{katz2017towards} conditions (a form of adversarial robustness stating that if individual inputs are close within certain regions, then the output remains within some bound).

%Fig.~\ref{fig:dnn}b shows the same network but with each hidden neuron $x$ split into two neurons $x'$ and $x''$ representing the result of affine transformation on $x$ and ReLU activation on $x'$, respectively, e.g.,  $x_3'=-x_1-0.5x_2-1.0$ and  $x_3'' = ReLU(x_3')$. This ReLU-slitting representation is adopted by {{{tool}}} and other DNN analyses (e.g., ~\cite{katz2017reluplex,wang2018efficient,henriksen2020efficient}) 
% including Reluplex~\cite{katz2017reluplex}, Neurify~\cite{wang2018efficient}, and VeriNet~\cite{henriksen2020efficient},
%because it does not change the semantics or complexity of the problem and is easier to reason about as we will show in \S\ref{sec:overview}.

\textbf{Abstraction.}
Relu-based DNN verification can be formulated as an SMT checking problem and in fact is NP-Complete~\cite{katz2017reluplex}.
%Specifically, we encode $N$ using a formula $\alpha$, which consists of linear constraints representing affine transformations and if-then-else \texttt{ite} conditions capturing ReLU.
%For example, the output of node $x_3$ of the DNN in Fig.~\ref{fig:dnn} is  $\texttt{ite}(-0.5x_1+0.5x_2+1.0 > 0, x_3=-0.5x_1+0.5x_2+1.0, x_3=0)$.
%Next, we conjoin $\alpha$  with the \emph{negation} of $\phi$ to create the SMT formula $\alpha \land \neg{\phi}$.
%Finally, we check this formula and obtain either \texttt{unsat} or \texttt{sat} indicating that $\phi$ is or is not a valid property of $N$, respectively. % \hd{Reversed order? SAT: not a valid prop, UNSAT: a valid prop}
%For example, to prove the property given in Eq.~\ref{eq:valid_prop} for a DNN $N$ encoded by a formula $F$, we create the formula $F \land  -1\le x_1 \le 1 \land -2 \le x_2 \le 2 \land x_5 > 0$ and obtain \texttt{unsat} when checking its satisfiability.
%The SMT formulation of FNN verification can be solved using an SMT solver. 
%In fact, ReLU-based DNN verification
%and then solved using an SMT solver (conjunction of linear constraints representing the network and the negation of the property to be solved for).
%Verifying DNN's with ReLU is NP-Complete (reduced from 3SAT) and thus theoretically can be solved using a SAT or SMT solver.
However, while the problem is theoretically reducible to other NP-Complete problems, including satisfiability, general SAT and SMT solvers do not scale for large and complex formulae encoding real-world complex and large DNNs.
%effective DNN verification techniques typically exploit the structure of the DNN (e.g., ReLU activation) and 
Thus, for scalability, modern DNN verification approaches exploit the behaviors of the DNNs (e.g., ReLU) and employ techniques such as abstraction that sacrifice precision to achieve efficiency.
%using various abstractions to approximate ReLU computations. % and they exploit the structure of networks (e.g., ReLU activation) approximations, which sacrifice precision to achieve scalability~\cite{singh2018fast,singh2019abstract,wang2018formal}.

Abstraction ~\cite{wang2018formal,singh2018fast,singh2019abstract} overapproximates nonlinear computations (e.g., ReLU) of the network using linear abstract domains such as interval~\cite{wang2018formal}, zonotope~\cite{singh2018fast}, polytope~\cite{singh2019abstract,xu2020fast}.
%This is similar to abstract interpretation~\cite{cousot1977abstract} in program analysis, in which nonconvex regions representing program states are overapproximated into convex regions.
A DNN verification technique using an approximation, e.g., the polytope abstract domain, %such as DeepPono~\cite{fillin} that uses the polytope abstraction,
works by (i) representing the input ranges of the DNN as polytopes, (ii) applying transformation rules to the affine and ReLU computations of the network to compute polytope regions representing values of neurons, and (iii) finally, converting the polytope results into output bounds.
The resulting outputs are an overapproximation of the actual outputs. % because these results were computed using the polytope abstraction.

%\textbf{Abstraction-based Reachability Analysis} At a high level, DNN verification can be viewed as a reachability problem where we over-approximate the computation of the DNN (or program) and check if that overapproximation overlaps with region representing the negation of a safety property of interest. If overlapping does not occurs (i.e., the unsafe region is not reachable), the property is safe. However, if the region is reachable, we cannot conclude that the property is violated due to overapproximation. Abstraction-based DNN analyses employ various abstraction domains to overapproximate DNN computation (in particular, to approximate activation functions such as ReLU). %While different abstract domains have different computational complexity, in general DNN abstractions are efficient -- the issue occurs when 





%FNN verification can be formulated as an SMT checking problem, in which the input SMT formula encodes both the DNN $N$ and property $P$.


% Given  an  assignment  of  values  to inputs, the output of the FNN, i.e., values of of the neurons in the output layer, is computed by iteratively computing  the  values  of  neurons  in  each  layer.
% The value of a neuron in the input layer is the input data. The value of a neuron in the hidden layers is computed by applying a linear \emph{affine transformation} on neuron values of the previous layers, then followed by a non-linear \emph{activation function} such as the popular Rectified Linear Unit (ReLU) activation. Specifically, the value of a hidden neuron $y$ is 
% \begin{equation}
% ReLU(w_1v_1 + \dots + w_nv_n+b),\label{eq:affine_relu}
% \end{equation}
% where $b$ is the bias parameter of $y$, $w_i, \dots, w_n$ are the weights of $y$, $v_1,\dots,v_n$ are the neuron values of the  previous  layer of $y$, $w_1v_1 + \dots + w_nv_n+b$ is the affine transformation, and $ReLU(x) = \max(x,0)$ is the RELU activation. The value of a neuron in the output layer is evaluated similarly but without the activation function.  

%As with many other neural network verification approaches, we focus on FNN with ReLU, though the technique described in the paper is generalized to other type of neural networks.


%The value of the neuron in the output layer is evaluated similarly as that of the hidden neuron, but without activation function. 

%Each layer consists of a number of neurons; a \emph{fully-connected feed-forward} neural network connects every neuron in one layer to the neuron in the next layer.  
%The values of the neurons in the input layer are simply input data. Each neuron in the hidden layer is associated with a weight and bias parameter and its value is derived from derived from neurons correcin the previous layers.
%An output layer (with units uLi
%) whose values are derived from the last hidden layer.

%connected layers that connect every neuron in one layer to every neuron in the other laye
%Each hidden layer consists of a number of neurons, each of which is associated with a weight and bias parameter. % and typically connects to neurons in a different hidden layer.




%Each hidden neuron is computed by first applying a linear \emph{affine} transformation to the inputs (if the neuron is in the first hidden layer) or the values of the outputs of the neurons in the other layers connecting to $i$ (if the neuron is not in the first hidden layer), and then followed by a non-linear \emph{activation} function.

%For example, the output of a hidden neuron $y$ using  the Rectified Linear Unit (ReLU) activation is 
%\begin{equation}
%  ReLU(w_1v_1 + \dots + w_nv_n+b),\label{eq:affine_relu}
%\end{equation}
%where $w_i, \dots, and w_n$ are the weights and $v_1,\dots,v_n$ are the values of  neurons in the  previous  layer of neuron $y$, and  $b$ is the bias parameter of $y$, and $w_1v_1 + \dots + w_nv_n+b$ is the affine transformation, and the activation function ReLU defined as  $ReLU(x) = \max(x,0)$. The value of each neuron in the output layer is evaluated similarly but without the activation function applied.



%Each non-input layer $i$ consists of a number $s_i$ of neurons, and the neuron $v_{i,j}$ denotes the $j^{th}$ neuron in the $i^{th}$ layer.
%Each neuron  is connected to another neuron $v_{i,k}$ via a direct edge $e_{i,j}$ 

%Each hidden layer is associated with a weight matrix $W$ and a bias vector $b$.
%The DNN input vector is denoted as $v_1$, and the output vector of each hidden layer $i$ is
%$v_i=f(W_{i-1}v_{i-1}+b_i)$, where $W_{i-1}v_{i-1}+b_j$ is a linear affine transformation and $f$ is a nonlinear activation function such as Rectified Linear Unit (ReLU), defined as $ReLU(x) = \max(0,x)$.
%The output layer is evaluated similarly, but only using affine transformation and without the activation function $v_n=W_{n-1}v_{n-1}+b_n$. Given an input vector $v_1$, the DNN is evaluated by sequentially calculating the $v_i$ for $i=2,\dots n$, and returning $v_n$ as the network's output.


%Each non-input layer consists of multiple neurons, each of which consists of a weight and a bias parameter. 
%Given  an  assignment  of  values  to inputs, the output of the DNN is computed by iteratively computing  the  values  of  neurons  in  each  layer.
%Typically, the value of a neuron in a hidden layer is computed by applying a linear \emph{affine transformation}, which is a linear combination of the outputs of the neurons in the previous layers, followed by a non-linear \emph{activation} function such as Rectified Linear Unit (ReLU) activation defined as $ReLU(x) = \max(x,0)$.
%More specifically, the output of a neuron $n$ is 
%\begin{equation}
%  ReLU(w_1v_1 + \dots + w_nv_n+b),\label{eq:affine_relu}
%\end{equation}
%where $w_i, \dots, w_n$ be weights and $v_1,\dots,v_n$ the values of  neurons in the  previous  layer of neuron $n$, and $b$ is the bias parameter of $n$.\tvn{the output neuron uses ReLU?}

%$where $v_1,\dots,v_n$ are the values of  the  previous  layer’s  neurons, $w_1,\dots,w_n$ are  the  weight parameters and $b$  is a  bias  parameter  associated  with  the neuron. 
%Moreover, we call a neuron \emph{active} if its output in Eq.~\ref{eq:affine_relu} is positive and \emph{inactive} otherwise.

% \begin{figure}
%   \begin{minipage}[t]{0.48\linewidth}
%   \centering
%   \includegraphics[width=0.8\linewidth]{figure/example-a.pdf}
%   \caption*{a}
% \end{minipage}
%   \begin{minipage}[t]{0.50\linewidth}
%   \centering
%   \includegraphics[width=\linewidth]{figure/example-b.pdf}
%   \caption*{b}
% \end{minipage}
% \caption{(a) A simple FNN with ReLU activation. (b) The same FNN with seperated neurons for ReLU activations.}\label{fig:dnn}
% \end{figure}



%{{{tool}}} uses this RELU-splitting representation because it allows for more straight-forward reasoning and does not change the semantics or complexity of the problem.  This representation is also adopted by other FNN analyses including Reluplex~\cite{katz2017reluplex}, Neurify~\cite{wang2018efficient}, and VeriNet~\cite{henriksen2020efficient}.

% \subsubsection{Verifying FNN's}\label{sec:nnverif}

% Given as input an FNN $N$ and a property $P$, the FNN verification problem asks if $P$ is a valid property of $N$~\cite{katz2017reluplex}. Typically, $P$ has the form $P_{in} \Rightarrow P_{out}$, where $P_{in}$ is a formula over the inputs of $N$ and $P_{out}$ is a formula over the outputs of $N$. A FNN verification tool then attempts to find a counterexample input that satisfies $P_{in}$ but causes $N$ to produce an output that does not satisfies $P_{out}$.  If no counterexample exists, then $P$ is a valid property of $N$; otherwise, $P$ is not a valid property of $N$. % and the counterexample is a witness of its invalidity (i.e., running $N$ on the counterexample input results in $P_{out}$ being false).



%\textbf{SMT formulation} FNN verification can be formulated as an SMT checking problem, in which the input SMT formula encodes both the DNN $N$ and property $P$.
%Specifically, we encode $N$ using a formula $F$, which is a conjunction of linear constraints representing the values of the neurons\footnote{ReLU functions (i.e., $\max$) can be represented as disjunctions or if then else (\texttt{ite}) conditions and affine transformations can be represented as  linear constraints over the reals.}.  For example, the output of $x_3$ is  $ite(-x_1-0.5x_2-1.0 > 0, x_3=-1x_1-0.5x_2-1.0, x_3=0)$.
%Next, we conjoin $F$  with the \emph{negation} of $P$ to create the input SMT formula $F \land \neg{P}$.
%Finally, we check this formula and obtain \texttt{sat} or \texttt{unsat} indicating that $P$ is or is not a valid property of $N$, respectively. 
%For example, to prove the property given in Eq.~\ref{eq:valid_prop} for a DNN $N$ encoded by a formula $F$, we create the formula $F \land  -1\le x_1 \le 1 \land -2 \le x_2 \le 2 \land x_5 > 0$ and obtain \texttt{unsat} when checking its satisfiability.


%The problem of verifying DNN's with ReLU is NP-Complete  and thus belongs to the same complexity class as SAT solving. 
%Moreover, this would allow us to apply a SAT solver to verify DNNs.

%The SMT formulation of FNN verification can be solved using an SMT solver. 
%In fact, verifying DNN's with ReLU is NP-Complete~\cite{katz2017reluplex} (reduced from 3SAT) and thus theoretically can be solved using a SAT solver.
%However, general SAT and SMT solvers do not scale for large and complex SAT or SMT formulae encoding real-world neural networks containing many layers and neurons.
%Thus, effective DNN verification techniques typically exploit the structure of the DNN (e.g., ReLU activation) and employ approximations that sacrifice precision  to achieve scalability.

%This is mainly because each ReLU computation results in two possible values and thus causes an exponential complexity in the number of possible.

%Just as with SAT solving, scalability is a challenge to design a practical DNN verification tool.


%We can check the resulting SMT formula using an off-the-shelf SMT solver such as Z3. 

%Thus, many techniques have been designed specifically for the verification of DNN's, e.g., approximation of the computation the DNN. %\S\ref{sec:related} summarizes the state of the art of DNN verification techniques.

%\tvn{example}
%The value $v_{i,j}$ of a neuron $j$ of a layer $i$ is computed as:
%\begin{equation}
%ReLU(w_1v_1 + \dots + w_nv_n+b_{i,j}),\label{eq:dnn2}
%\end{equation}




%A feed-forward neural network consists of a finite set of nodes (neurons) arranged in finite layers.
%The activation $a_j$ of a neuron in a layer is computed sequentially in two steps: The first step consists of computing the weighted sum $z_j$ of the activation received from the neurons in the preceding layer together with addition of a bias $b_j$ associated with the neuron, i.e., $z_j=b_j+\sum_{i=1}^{p}{w_{i,j}*a_i}$, where $w_{i,j}$ is the weight of the link connecting the neuron $i$ of the preceding layer and $p$ denotes the number of neurons in that layer.
%Second, a non-linear function $\mathcal{F} : \mathcal{R} \rightarrow \mathcal{R}$ is applied on the weighted sum to get the activation $a_j = \mathcal{F}(z_j)$.

%DeepZono~\cite{singh2018fast}
%Abstraction-based DNN verification ~\cite{wang2018formal,singh2018fast,singh2019abstract} overapproximate the computations of the network using abstraction domains such as interval~\cite{}, zonotope~\cite{}, polytope~\cite{}.
%This is similar to abstract interpretation~\cite{cousot1977abstract} in program analysis, in which nonconvex regions representing program states are overapproximated into convex regions.
%A DNN verification technique using approximation, such as DeepPoly~\cite{TODO} that uses the polytope abstraction, works by (i) representing the input ranges of the DNN as polytopes, (ii) applying transformation rules to affine functions and ReLU activations of the DNN to compute polytope regions, and (iii) finally, converting the polytope results into output ranges. 
%The resulting outputs would be abstraction or overapproximation of the actual outputs because these results were computed using the polytope abstraction.
%Note that because the outputs were computed using the zonotope abstraction, they would be overapproximations of the actual ones. %, i.e., the actual output region is contained in the approximated one.

%Abstraction allows us to treat DNN verification as a reachability problem. If we cannot reach or obtain any output values violating a property $P$ from the abstracted outputs, then we also cannot get  undesirable values from the actual outputs (which are contained in the abstracted ones), and thus $P$ is proved. However, if the abstract outputs contain some undesirable value violating $P$, then we cannot conclude that the actual outputs would also contain that value (i.e., a spurious counterexample). 
  
%  show that the abstraction do not contain values violating the desired property, then the actual regions, which are contained within the abstraction, certainly do not contain any undesriable values, and thus the property is proved.
 



%Verification (for both software and DNNs) can be represented as a reachability problem in which the property is proved if we cannot reach any undesirable program states or values violating the desired property. Overapproximation solves this by showing that if the abstract region do not contain any undesirable values, then the actual result region also do not contain undesirable values.




% Thus, if values that are not in the output 
%outputs in the property to be proved are contained within the (approximated) results, then the property is proved. %Otherwise, however, we cannot 

%{{{tool}}} uses ponotope~\cite{fillin} in its DPLL framework to decide \texttt{unsat} results. This helps proving properties. 

% \subsubsection{Random-based Falsification}

% \tvn{Hai: talk about the random heuristics that {{{tool}}} uses to get counterexample}

% The falsification algorithm uses a derivative free sampling based optimization method to direct the search for a counter example based on the safety property.
% The core of the falsification procedure is a derivative-free sampling-based optimization method \cite{yu2016derivative} and being adopted in \cite{das2021fast}.

% For an illustration of the heuristic, consider the simple case when the safety property is only a term with $y_i \triangleleft y_j$. 
% Based on the type of the relation, the decision to either construct a maximization or a minimization problem is made. 
% If the relation is $\le$ or $<$, the main idea is to search for samples in the domain of the neural network that maximizes the variable $y_i$ (or minimizes the variable $y_j$) so that we could find a sample for which the relation  $y_i \triangleleft y_j$ evaluates to false.

% In this way, the algorithm intends to direct the search of inputs in the domain that drives the output of the network towards the boundary separating the unsafe and safe region and thereafter, looking for inputs for which the network’s output crosses over from the safe to the unsafe region.
% % We adopt the original falsification to our {{{tool}}} with few improvements, such as, 
% \tvn{I don't understand this technique.  We never mentioned anything about it during the illustration (just use LP to check for infeasibility).}



% %Several recent DNN's  abstraction techniques include ReluVal (intervals)~\cite{wang2018formal}, and DeepZono (zonotopes)~\cite{deepzono}, and NNV (starsets).

  

% %\subsection{DeepPoly - Polyhedral Abstraction}

% %\subsection{Random-based Falsification}

\section{Overview of {{{tool}}}}\label{sec:overview}

\begin{wrapfigure}{r}{0.17\textwidth}
  \centering
  %\vspace{-0.1in}
  \hspace{-0.3in}
  \includegraphics[width=1.1\linewidth]{figure/arch.pdf}
  \vspace{-0.1in}
  \caption{\label{fig:overview} {{{tool}}}.} 
  \vspace{-0.15in}
\end{wrapfigure}
Fig.~\ref{fig:overview} gives an overview of {{{tool}}}, which follows the DPLL(T) framework (\S\ref{sec:background}) and consists of DPLL/CDCL components (light shades) and the theory solver (dark shade).
{{{tool}}} first abstracts DNN verification into a SAT problem, consisting of only clauses over Boolean variables (\emph{Boolean Abstraction}).  Here, {{{tool}}} creates Boolean variables to represent the \emph{activation} status of neurons, e.g., with ReLU a (hidden) neuron is \emph{active} if the input value to ReLU is positive and \emph{inactive} otherwise. Next, {{{tool}}} creates  clauses asserting each neuron is either active/true or inactive/false.
This abstraction allows {{{tool}}} to  use (i) DPLL/CDCL to search for truth values satisfying these clauses and (ii) the theory solver to check the feasibility of truth assignments with respect to the constraints encoding the DNN and the property of interest.


{{{tool}}} now enters an iterative process to find Boolean assignments satisfying the activation clauses.
First, {{{tool}}} assigns a truth value to an unassigned variable (\emph{Decide}), detects unit clauses caused by this assignment, and infers additional assignments (\emph{Boolean Constraint Propagation}).
Next, {{{tool}}} invokes the theory solver (\emph{DEDUCTION}), which (i) tightens the bounds of the network inputs using the current assignments and an LP solver and (ii) abstracts (approximates) the bounds of the network outputs using the new input bounds, (iii) and checks if these bounds are feasible with the property of interest.

If the theory solver determines feasibility, {{{tool}}} continues with new assignments (\emph{Decide}). Otherwise, {{{tool}}} analyzes the infeasibility (\emph{AnalyzeConflict}) and learns clauses to avoid such infeasibility and backtrack to a previous iteration (\emph{Backtrack}).
This process repeats until {{{tool}}} no longer can backtrack (returns \texttt{unsat}, indicating the DNN has the property) or finds a complete assignment for all boolean variables (returns \texttt{sat}, and the user can query the solver for a counterexample).



%We provide a completely walkthrough of {{{tool}}} using the network and property examples in \S\ref{sec:nnverif}.  Our aim is that the readers would  understand the intuition and main ideas in {{{tool}}} after reading the section.
% \begin{figure}
%   \centering
%   \includegraphics[width=0.3\linewidth]{figure/arch.pdf}
%   \caption{{{{tool}}} Overview.}\label{fig:overview}
% \end{figure}


% \textbf{Boolean Abstraction and Status Variables} {{{tool}}} encodes network verification as an SMT checking problem described in \S\ref{sec:nnverif} and uses a  DPLL(T)-based algorithm to solve it. 
% The DPLL(T) algorithm uses the core design of DPLL, which assigns values to \emph{Boolean} variables and backtracks assignment decisions when conflicts arise.
% However, the variables in the SMT formula representing the FNN and property are real-valued, thus we exploit the structure of network to create a Boolean abstraction for DPLL.

% Observe that when evaluating the network on any concrete input, the status of each hidden neuron before ReLU is either \texttt{on} ($>0$) or \texttt{off} ($\le 0$). This allows us to assign truth values representing the status of hidden neurons.
% Thus, from the given network, {{{tool}}} first creates Boolean variables representing the status of hidden pre-ReLU neurons (i.e., a status variable for every "primed" variable in the network). Next, {{{tool}}} forms a set of initial clauses ensuring that each status variable is either \texttt{T} (on) or \texttt{F} (off). 
% %Thus, during preprocessing, {{{tool}}} also creates Boolean variables representing the status values of hidden pre-ReLU neurons and an initial set of clause enforcing that each variable needs to be either $T$ (\texttt{on}) or $F$ (\texttt{off}).
% For example, for the network in Fig.~\ref{fig:dnn}b, {{{tool}}} creates  status variables $v_3,v_4$ for neurons $x_3',x_4'$, respectively, and two initial clauses $v_3\lor \overline{v_3}$ and $v_4 \lor \overline{v_4}$.

% %of {{{tool}}} is to convert DNN verification into an SMT checking problem, 

% %{{{tool}}} uses the core ideas in DPLL, which assigns values to \emph{Boolean} variables and backtracks assignment decisions when conflicts arise, to check the satisfiability of an SMT formula capturing the FNN verification problem.



% \begin{figure}
%   \centering
%   \includegraphics[width=\linewidth]{figure/overview.pdf}
%   \caption{{{{tool}}} Overview.  \tvn{Hai: replace Encoder with Boolean Abstraction} \hd{Done.}}\label{fig:overview}
% \end{figure}




% \begin{itemize} %[leftmargin=*]
% \item \textbf{DECIDE} assigns a truth value to an unassigned status variable (if all variables are assigned {{{tool}}} returns \texttt{sat}).
% DECIDE also increments the decision level by 1 and associates the assignment with the new decision level.  
%     %  Each DECIDE assignment is associated with a decision level that is 1 more than the current decision level.
% %  \tvn{when does the decision level incremented? - The decision level increases by 1 when the DECIDE has to choose a value of T/F for an unassigned variable, not in the case of forcing in BCP or implying in DEDUCTION}
% %  and a truth value for it.\tvn{do you use any heuristics to decide variable and truth value? -- Answer: Currently, {{{tool}}} uses VSIDS, decision that bases on the number of occurrences of each literal. Need to develop a more meaningful heuristic.}. Each assignment is associated with a decision level that is 1 more than the current decision level\tvn{correct? Yes}.\tvn{what's the output of Decide?  what is a full assignment and just an assignment? -- Answer: If the input assignment of DECIDE is NOT full, which means that DECIDE has to decide, then output goes to BCP. Otherwise, if input assignment of DECIDE is full, no more variable needs to be decided, then return SAT.}

% \item \textbf{BCP} (Boolean Constraint Propagation) analyzes the current assignment and  clauses to find \emph{unit clauses} to infer values to status variables (e.g., if we have the assignment $a=F$ and a unit clause $a\vee b$, then $b$ must be $T$ to satisfy the clause).
%   % It does this by finding inferred by resolution.
%   Each BCP assignment is associated with the current decision level because the assignment is automatically inferred. %, i.e., a variable must have a certain value to satisfy the clause.
%   %The decision level associated with each BCP assignment is the current decision level. % these variables must have the assigned values (i.e., not possible to backtrack and change).  

% %“conflict” if and only if a conflict is encountered.DescriptionRepeated application of the unit clause rule until either a conflictis encountered or there are no more implications.CommentsThis repeated process is called Boolean constraint propagation(BCP). BCP is applied in line 2 because unary clauses at thisstage are unit clauses.



% %False if and only if there are no more variables to assign. 
% %CommentsThere are numerous heuristics for making these decisions, someof which are described later in Sect. 2.2.5. Each such decision isassociated with a decision level, which can be thought of as thedepth in the search tree.
  
% \item \textbf{DEDUCTION} is the dedicate ``theory solver'' in DPLL(T) and checks if the current truth assignment for status variables actually satisfies the  constraints encoding  the network and the desired property. To do this, {{{tool}}} invokes subcomponents including an LP solver to check satisfiability of linear constraints and a reachability heuristics that uses (polytope) abstractions to approximate bounds over output. %nonlinear (e.g., ReLU) constraints. %, and an implication heuristics that \tvn{what does this do}?
%   If DEDUCTION determines the current assignment has a conflict, {{{tool}}} invokes ANALYZE-CONFLICT to backtrack to reverse conflicting assignments; otherwise, {{{tool}}} goes to DECIDE to make new assignments.  
  

% \item \textbf{ANALYZE-CONFLICT} 
%   analyzes the conflict found by DEDUCTION and adds a ``conflict'' clause to the current set of clauses to avoid this conflict in subsequent iterations.
%   It also computes a decision level (\texttt{dl}) to backtrack to. If the conflict is determined to be at decision level 0, {{{tool}}} obtains -1 as the backtrack decision level and returns \texttt{unsat}.

% \item 

% \end{itemize}

% %\tvn{Hai: write this part. Refer to the Dynaplex paper for example on how to write the overview description.}
% %Deduce consists of three subtasks: (i) determining where the current constraints is feasible (if no, goes to conflict analysis); (ii) tightening variable bounds;

% % Next, {{{tool}}} generates a set of clauses over these Boolean variables so that it can find values for the variables to satisfy the clauses. For example, a simple set of clause for the two variables $v_3,v_4$ is:
% % \begin{equation}
% %     \begin{aligned}
% %         c_1 &: v_3 \vee \neg v_3 \\
% %         c_2 &: v_4 \vee \neg v_4 
% %     \end{aligned}
% % \end{equation}


% %that the algorithm could only stop when all variables are decided (sat) or could not find a valid set of values for these variables (unsat).


% %next we create the clause ..  and the goal of DPLL is to assign status value for the variables to satisfies these clauses (as well as the constraints representing the semantics of the neuron status).

% %Now, given the input SMT formula representing, and the generated set of clauses representing constraints over variables corresponding to status values of hidden neurons, \tool, whose overview is given in Fig.~\ref{} works as follows:  

% %each hidden neuron before ReLU, e.g., $x_3, x_4$, could be one in two possible status on/off.

% %we could see that when execute the DNN with a concrete example, each hidden neuron before ReLU, e.g., $x_3, x_4$, could be one in two possible status on/off. 

% %Indeed, we create Boolean variables to capture the \emph{status} of hidden neurons and assign values to those variables. 


% %At is core, {{{tool}}} attempts to assign the status of the hidden neurons 

% %\textbf{Neuron status} With ReLU activation, we call a neuron is \textit{on} if its value before feeding into ReLU is greater than 0, then output of ReLU on that neuron equals to itself, e.g. $x_3$ is on if $x_3 > 0$ then $\hat{x}_3 = ReLU(x_3) = x_3$, and a neuron is \textit{off} if its value less than 0, then output of ReLU equals to 0, e.g. $x_3$ is off if $x_3 \le 0$ then $\hat{x}_3 = ReLU(x_3) = 0$.

% %With the definition of the neuron status, we could see that when execute the DNN with a concrete example, each hidden neuron before ReLU, e.g., $x_3, x_4$, could be one in two possible status on/off. 
% %These values are identical to True/False in SAT problem, so that we can use DPLL algorithm to find a valid set of status for these neurons.

% %To start DPLL algorithm, set of variables is $\{v_3, v_4\}$ corresponding to status of $x_3, x_4$ respectively, and set of clauses needs to be defined such that the algorithm could only stop when all variables are decided (sat) or could not find a valid set of values for these variables (unsat).
% %A simple starting CNF clauses satisfying this requirement could be:
% % \begin{equation}
% %     \begin{aligned}
% %         c_1 &: v_3 \vee \neg v_3 \\
% %         c_2 &: v_4 \vee \neg v_4 
% %     \end{aligned}
% % \end{equation}

% %Specifically, 


% %Fig.~\ref{fig:overview} gives an overview of {{{tool}}}, which takes as input an SMT formula encoding the neural network and the property of interest, and returns \texttt{unsat} if it is not possible to satisfies the formula (i.e., the property is valid for the network) and \texttt{sat} if is is possible to satisfy the formula (i.e., the property is invalid for the network). {{{tool}}} first analyzes sthe input formula to create Boolean variables representing the status of hidden pre-ReLU neurons (i.e., a status variable for every "primed" variable in the formula) and initial clauses indicating that each status varibale is either \texttt{T} or \texttt{F}. 

\subsection{Illustration}\label{sec:unsat}



We use {{{tool}}} to prove that for inputs $x_1 \in [-1, 1], x_2 \in [-2,2]$ the DNN in Fig.~\ref{fig:dnn} produces the output $x_5 \le 0$.
{{{tool}}} takes as input the formula $\alpha$ representing the DNN:
\begin{equation}\label{eq:ex}
\begin{aligned}
  x_3 = ReLU(-0.5x_1 + 0.5x_2 + 1)  \;\land\;  \\
  x_4 = ReLU(x_1 + x_2 - 1) \;\land\; \\
  x_5 = -x_3 + x_4 -1
\end{aligned}
\end{equation}
and the formula $\phi$ representing the property:
\begin{equation}\label{eq:valid_prop}
    \phi : -1\le x_1 \le 1 \land -2 \le x_2 \le 2 \quad\Rightarrow\quad x_5 \le 0.
  \end{equation}
To prove that  $\alpha \Rightarrow \phi$, {{{tool}}} shows  that \emph{no} value assignments to $x_1,x_2$ satisfying the input properties but resulting in $x_5 > 0$, i.e., we show the unsatisfiability of $\overline{\alpha \Rightarrow \phi}$:
\begin{equation}\label{eq:negprop}
  \alpha\; \land\; -1 \le x_1 \le 1     \;\land\; -2 \le x_2 \le 2   \;\land\; x_5 > 0.
\end{equation} 

In the following, we write $x \mapsto v$ to denote that the variable $x$ is assigned with a truth value $v \in \{T,F\}$. This assignment can be either decided by \texttt{Decide} or inferred by \texttt{BCP}. We also write $x@dl$ and  $\overline{x}@dl$ to indicate the respective assignments $x \mapsto T$ and $x \mapsto F$  at decision level $dl$.

\paragraph{Boolean Abstraction} First, {{{tool}}} creates two Boolean variables $v_3$ and $v_4$ to represent the
%(pre-ReLU)
activation status of the hidden neurons $x_3$ and $x_4$, respectively. For example, $v_3=T$ means $x_3$ is \texttt{active} and thus implies the constraint $-0.5x_1 + 0.5x_2 + 1 > 0$. Similarly, $v_3=F$ means $x_3$ is \texttt{inactive} and implies $-0.5x_1 + 0.5x_2 + 1\le 0$. Next, {{{tool}}} forms two clauses  $\{v_3 \lor \overline{v_3} \;;\; v_4 \lor \overline{v_4}\}$ indicating these variables are either \texttt{active} or \texttt{inactive}.

%Now, {{{tool}}} searches for truth assignments for activation variables to satisfy the clauses. %(and later check that they also satisfy the constraints of DNN implied by these variables and the properties to be proved).
%We summarize the five iterations {{{tool}}} uses to determine that no such assignment exists (i.e., \texttt{unsat}).

%We show how {{{tool}}} proves that network in Fig.~\ref{fig:dnn} has property in Eq.~\ref{eq:valid_prop}, i.e., for any inputs $x_1 \in [-1, 1], x_2 \in [-2,2]$, the network produces the output $x_5 \le 0$. 

% \textbf{SAT formulation} {{{tool}}} first encodes the verification task into an SMT checking problem as described in \S\ref{sec:nnverif} by representing the network in Fig.~\ref{fig:dnn}b as the formula:

% and negating of the property in Eq~\ref{eq:valid_prop}:
% \begin{equation}\label{eq:negprop}
%         -1 \le x_1 \le 1     \;\land\; -2 \le x_2 \le 2   
%         \;\land\; x_5 > 0.
% \end{equation} 

%\hd{I might use different weight for DNN to trigger some technique in the tool.}\tvn{yes, that's fine, make it as easy to illustrate as possible, but should also be challenging enough to require at least 2-3 iterations.  Also good to demonstrate 2 cases:  sat and unsat. For example, focus on say unsat,  and then after illustrating that,  make a new subsection and change something so that it would be sat and briefly mentions how {{{tool}}} would work to return sat}\tvn{\tool is complex, so instead of using the Dynaplex example, which is a bit too easy,  maybe look at the illustrative example for the GenTree's paper---it has more details.  Or even look at the Reluplex's illustrative example in the Survey paper}.

%We use an example\tvn{Hai: create an example, may be use the DNN from PA4 (or subset of that DNN)} to demonstrate how {{{tool}}} works.  Fig.~\ref{fig:ex} shows a small DNN ....  


% \begin{figure}
%   \centering
%   \includegraphics[width=0.7\linewidth]{figure/example-b.pdf}
%   \caption{Simple FNN with ReLU activation.} \label{fig:ex}
% \end{figure}

%\hd{Step 0: About the network - copy somewhere, need to rewrite}
%\textbf{Running example}

%We use an example of simple fully-connected feed-forward neural network with %ReLU activation shown in Fig.~\ref{fig:dnn} to demonstrate how {{{tool}}} works. 
%This network has already been trained and we have the learned weights and bias shown in the figure. 
%The network consists of three layers: an input layer, a hidden layer, and an output layer with two neurons each. 
%The weights on the edges represent the learned coefficients of the weight matrix used by the affine transformations done at each layer. The learned bias for each neuron is shown above or below it. 


% {{{tool}}} aims to find an assignment satisfying the conjunction of the formulae in Eq.~\ref{eq:ex} and~\ref{eq:negprop}. Such an assignment represents a counterexample violating the property, i.e., within the given ranges but does not satisfy the output requirement.
% %If such an input does not exist, {{{tool}}} returns \texttt{unsat}, indicating the property is valid. Otherwise it returns \texttt{sat} and the input, which represents a counterexample violating the property.

% Next, from the formula in Eq.~\ref{eq:ex}, {{{tool}}} creates two Boolean variables $v_3$ and $v_4$ to represent the status of the hidden pre-ReLU neurons $x_3'$ and $x_4'$. For example, $v_3=T$ means the status of $x_3$ is \texttt{on}, i.e., $-x_1-0.5x_2-1>0$ and $v_3=F$ means $x_4=\texttt{off}$, i.e.,  $-x_1-0.5x_2-1>0$. {{{tool}}} also forms two initial clauses indicating the status variables must be either \texttt{on} or \texttt{off}: $v_3 \lor \overline{v_3} \;;\; v_4 \lor \overline{v_4}$.



% \begin{equation}\label{eq:newvars}
%     \begin{aligned}
%         v_3 \vee \overline{v_3} \\
%         v_4 \vee \overline{v_4} 
%     \end{aligned}
% \end{equation}




% \begin{table*}
%   \caption{{{{tool}}}'s run producing \texttt{unsat}. The notation $x@dl$ and $\overline{x}@dl$ mean the assignments $x \mapsto T$ and $x \mapsto F$ at decision level $dl$, respectively.}\label{tab:valid}
%   \centering
%   \footnotesize
%     \begin{tabular}{c|c|cc|c|cc}
%     \toprule
%     Iter & \textbf{BCP} & \multicolumn{2}{c|}{\textbf{DEDUCTION}}& \textbf{DECIDE} & \multicolumn{2}{c}{\textbf{ANALYZE-CONFLICT}} \\
%       &&Constraints&Bounds&&Backtrack&Learned Clauses\\
%       \midrule
%       Init &-& $I = -1 \le x_1 \le 1; -2 \le x_2 \le 2$ & $-1 \le x_1 \le 1; -2 \le x_2\le 2$ & - &-&$C = \{v_3 \lor \overline{v_3};\; v_4 \lor \overline{v_4}\}$\\
      
%       1 &-&$I$ & $ x_5 \le 1 $& $\overline{v_4}@1$&-&-\\
      
%       2 &-&$I; x_4=\texttt{off}$&$ x_5 \le -1$& - & 0 &  $C = C \cup \{v_4\}$\\
      
%       3 &$v_4@0$&$I; x_4=\texttt{on} $&$ x_3 \ge 0.5; x_5 \le 0.5$ & $v_3@0$&-&-\\
      
%       4 &-&$I; x_3=\texttt{on}; x_4=\texttt{on}$&-&- & \bf{-1} & $C = C\cup \{\overline{v_4}\}$\\
      
%       % 5 &$v_4@0$&$I; x_3=\texttt{on}; x_4=\texttt{on}$&-&- & \bf{-1}& $C = C \cup \overline{v_3}\lor \overline{v_4}$\\
      
%        \bottomrule
%     \end{tabular}
% \end{table*}

\paragraph{DPLL(T) Iterations} {{{tool}}} searches for an assignment to satisfy the clauses and the constraints they imply.
In this example, {{{tool}}} uses four iterations, summarized in Tab.~\ref{tab:valid}, to determine that no such assignment exists and the problem is thus \texttt{unsat}.

%\emph{Initially}, the set of constraints  consists of the given bounds over the inputs, and the set of the clauses contains the two clauses in Eq.~\ref{eq:newvars}.

In \emph{iteration 1}, as shown in Fig.~\ref{fig:overview}, {{{tool}}} starts with BCP, which has no effects because the current clauses and (empty) assignment produce no unit clauses.
In DEDUCTION, {{{tool}}} uses an LP solver to determine that the current set of constraints, which currently contains just the initial input bounds, is feasible. {{{tool}}} then uses the polytope abstraction to obtain an output upper bound $x_5 \le 1$ and thus deduces that satisfying the output $x_5 >0$ might be feasible. {{{tool}}} continues with DECIDE, which uses a heuristic to select the unassigned variable $v_4$ and randomly sets $v_4=F$.  {{{tool}}} also increments the decision level ($dl$) to 1 and associates $dl=1$ to the assignment, i.e., $\overline{v_4}@1$.

In \emph{iteration 2}, BCP again has no effects because it does not detect any unit clauses. In DEDUCTION, {{{tool}}} determines that current set of constraints, which contains $x_1 + x_2 - 1 \le 0$ due to the assignment $v_4\mapsto F$ (i.e., $x_4=\texttt{off}$), is feasible. Also, with this new constraint, {{{tool}}} approximates new bounds for hidden neurons and uses those to approximate a new output upper bound $x_5\le -1$.  However, because overapproximation gives $x_5 \le -1$, satisfying $x_5 > 0$ is \emph{infeasible}.

{{{tool}}} now enters ANALYZE-CONFLICT and determines that $v_4$ causes the conflict ($v_4$ is the only variable assigned so far).  From the assignment $\overline{v_4}@1$, {{{tool}}} computes a new clause $v_4$, indicating that $v_4$ must be $T$, and backtracks to $dl$ $0$, which erases all assignments decided \emph{after} this level. Thus, $v_4$ is now unassigned and the constraint  $x_1 + x_2 - 1 \le 0$ is also removed.

In \emph{iteration 3}, BCP detects the unit clause $v_4$ and infers $v_4@0$. In DEDUCTION, we now have the new constraint $x_1 + x_2 - 1 > 0$ due to $v_4 \mapsto T$ (i.e., $x_4=\texttt{on}$).  With the new constraint, {{{tool}}} approximates the output upper bound $x_5 \le  0.5$, which means $x_5>0$ might be feasible.
Also, {{{tool}}} computes new bounds $0.5 \le x_3 \le 2.5$ and $0 < x_4 \le 2.0$, and deduces that $x_3$ must be positive (because $x_3 \ge 0.5$).  Thus, {{{tool}}} has a new assignment $v_3@0$ ($dl$ stays unchanged due to the implication).

In \emph{iteration 4}, BCP has no effects because we have no new unit clauses (the existing one $v_4$ is already satisfied with the assignment $v_4@0$ in iteration 3).  In DEDUCTION, {{{tool}}} determines that the current set of constraints, which contains the new constraint $-0.5x_1+0.5x_2+1 > 0$ (due to $v_3 \mapsto T$), is \emph{infeasible}. Thus, {{{tool}}} enters ANALYZE-CONFLICT and determines that $v_4$, which was set at $dl=0$ (by BCP in iteration 3), causes the conflict. 
{{{tool}}} then learns a clause $\overline{v_4}$ (the conflict occurs when we have the assignment $\{v_3 \mapsto T; v_4 \mapsto F\}$, but $v_3$ was implied and thus making $v_4$ the conflict).
However, because we already have $v_4@0$, {{{tool}}} realizes that it can no longer backtrack and thus sets $dl=-1$ and returns \texttt{unsat}.  Note that we can also see that the learned clauses $\{v_4, \overline{v_4} \}$ are not satisfiable.

% Note that because we backtrack to $dl=0$, we do not erase the assignment $v_3=T$ because it was decided at $dl=0$ (by BCP in iteration 3).

% In \emph{iteration 5}, because of the new clause $\overline{v_3} \lor v4$ and $v_3$ is already set to $T$, BCD infers $v_4=T$ at dl=0.  In DEDUCTION, {{{tool}}} determines that set of constraints, which contains $-0.5x_1+x_2+1> 0$ for $x_4'=\texttt{on}$ (because $v_4=T$), is infeasible. In ANALYZE-CONFLICT, {{{tool}}} determines $v_4$ causes the conflict  and learns the new clause $\overline{v3} \lor \overline{v4}$ (because the current assignment $v_3=1;v_4=1$ cause the conflict).
% {{{tool}}} then sets $dl=-1$ (because $v_4=T$ was decided at level 0) and realizes that it can no longer backtrack, and thus returns \texttt{unsat}.  Note that we can also see that the learned clauses $\{v_3, \overline{v_3} \lor v_4, \overline{v_3} \lor \overline{v_4}\}$ are not satisfiable.

This \texttt{unsat} result shows that the DNN has the property because we cannot find a counterexample violating it, i.e., no inputs $x_1 \in[-1,1] ,x_2\in [-2,2]$ that results in $x_5 > 0$.

%the FNN in Fig.~\ref{fig:dnn}b has the property 
%, i.e.,  inputs results in $x_5 \le 0$.


% that it is not possible to find any counterexample, which is an input $x_1 \in[-1,1] ,x_2\in [-2,2]$ that would produce the output $x_5 >0$ with the DNN in Fig.~\ref{fig:dnn}b. Thus the property in Eq.~\ref{eq:valid_prop} stating that all inputs $x_1,x_2$ in that range results in $x_5 \le 0$ is \emph{valid} for the DNN.

% \subsection{Motivating Example: Disproving Invalid Properties}\label{sec:sat}
% Here, we use {{{tool}}} to show that the DNN in Fig.~\ref{fig:dnn} \emph{does not} have the property 
% \begin{equation}\label{eq:invalid_prop}
%     \phi_2 : -1\le x_1 \le 1 \land -2 \le x_2 \le 2 \quad\Rightarrow\quad x_5 > 0.
%   \end{equation}
% As before, {{{tool}}} attempts to prove that the DNN, represented by the formula $\alpha$ shown in Eq.~\ref{eq:ex}, has the property $\phi_2$ by showing the unsatisfiability of $\overline{\alpha \Rightarrow \phi_2}$
% \begin{equation}
%   \alpha -1 \le x_1 \le 1     \;\land\; -2 \le x_2 \le 2  \;\land\; x_5 \le 0:
% \end{equation}


      

% \begin{table*}
%     \caption{{{{tool}}}'s run producing \texttt{sat}.}\label{tab:invalid}
%   \centering
%   \footnotesize
%     \begin{tabular}{ccccccc}
%     \toprule
%       Iter & \textbf{BCP} & \multicolumn{2}{c}{\textbf{DEDUCTION}}& \textbf{DECIDE} & \multicolumn{2}{c}{\textbf{ANALYZE-CONFLICT}} \\
%       &&Constraints&Bounds&&BT&Clauses\\
%       \midrule
%       Init &-& $I = -1 \le x_1 \le 1; -2 \le x_2 \le 2$ & $-1 \le x_1 \le 1, -2 \le x_2\le 2$ & - &-&$v_3 \lor \overline{v_3};\; v_4 \lor \overline{v_4}$\\
      
%       1 &-&$I$ & $x_5 \le 1$& $\overline{v_4}@ 1$&-&-\\
      
%       2 &-&$I; x_4=\texttt{off}$&$ x_5 \le -1$& $v_3@2$ & - &  -\\
      
%       3 &-&$I; x_3=\texttt{on}; x_4=\texttt{off}$& - & - & - &  -\\
      
%        \bottomrule
%     \end{tabular}
%   \end{table*}

% For this example, {{{tool}}} is unable to show unsatisfiability and instead, within three iterations (summarized in Tab.~\ref{tab:invalid}), obtains a satisfying assignment representing a counterexample disproving the property.
% In \emph{iteration 1}, {{{tool}}} behaves similarly to iteration 1 in \S\ref{sec:unsat}, which computes the upper bound $x_5 \le 1$ (thus the satisfying $x_5\le 0$ might be feasible) and makes the assignment $\neg{v_4}@1$.
% In \emph{iteration 2}, {{{tool}}} obtains the similar upper bound $x_5\le -1$ as iteration 2 in \S\ref{sec:unsat} (thus satisfying $x_5 \le 0$ is feasible) and makes another assignment $v_3@2$. In \emph{iteration 3}, {{{tool}}} deduces that $x_5 \le 0$ is satisfiable with the current assignment (the LP solver gives the model $\{x_1=-1,x_2=2,x_5=-3.5\}$)  and returns \texttt{sat} because it has achieved a full assignment (both status variables assigned with truth values).

% %solver to compute a satisfying assignment $\{x_1=0, x_2=-2\}$ (in which the network evaluates $x_5=-1$ and thus satisfying $x\le 0$).

% This \texttt{sat} result shows the property in Eq.~\ref{eq:invalid_prop} asserting $x_5 > 0$ is invalid because {{{tool}}} found a counterexample input $\{x_1=-1,x_2=2\}$, in which the network produces $x_5=-3.5$.

% Note that {{{tool}}} behaves similarly to a SAT or SMT solver and returns either the \texttt{sat} or \texttt{unsat} status; for the \texttt{sat} case, the user can query {{{tool}}} for a model representing the satisfying assignment (e.g., the LP model).

%\tvn{this goes to DEDUCTIOn too right?  and happens there?  what are the bounds ? what happens next ? gives the step until you get SAT? and what are the counterexample input? }
%\hd{In iteration 3, it goes to DEDUCTION too. In this case, all variables are assigned (full assignment case), no need to run the abstraction step to estimate the overapproximation bounds, just add the output property (output constraints) along with the existed constraints to check the feasibility of the assignment. For example, in this case, in the DEDUCTION step, the LP solver model contains the constraints as follow: $(c1):  x_1 - 0.5x_2 -1 \le 0$ for $x_3 = off$, $(c2): -0.5x_1+x_2+1 \le 0$ for $x_4 = off$, $(c3): x_5 = relu(x3) - relu(x4) - 1 = -1$ (using $relu(x3)=0$ because $x3 \le 0$), and output property $(c4): x_5 \le 0$. The LP model returns FEASIBLE and feasible solution is $x_1=0; x_2=-2$ that satisfies the output constraint $x_5 \le 0$ ($x_5=-1$ actually). The counterexample input is the feasible solution that returns from the DEDUCTION step $x_1=0; x_2=-2$.}


% We use the DNN example in Fig.~\ref{fig:dnn} and the property in Eq.~\ref{eq:invalid_prop} to illustrate {{{tool}}}'s \texttt{sat} case.
% The input formula, given in Eq.~\ref{eq:ex_invalid}, is a conjunct of the encoding of the DNN in Fig.~\ref{fig:dnn}b and the negation of the property in Eq~\ref{eq:invalid_prop}: 


% \begin{equation}\label{eq:ex_invalid}
%     \begin{aligned}
%       -1 \le x_1 \le 1 &\;\land\; -2 \le x_2 \le 2 & \land \\
%         x_3 = -x_1 - 0.5x_2 - 1 &\;\land\; \hat{x}_3 = ReLU(x_3) & \land \\
%          x_4 = -0.5x_1 + x_2 + 1&\;\land\; \hat{x}_4 = ReLU(x_4) & \land \\
%         x_5 = \hat{x}_3 - \hat{x}_4 - 1 &\;\land\; x_5 \le 0.
%     \end{aligned}
% \end{equation}

